{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T00:24:15.668129Z",
     "start_time": "2025-03-18T00:24:15.638384Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import duckdb\n",
    "import re\n",
    "import csv\n",
    "import sys\n",
    "import ahocorasick\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "import pandas as pd\n",
    "from Levenshtein import ratio\n",
    "\n",
    "\n",
    "def generate_corasick(kws: list[str]):\n",
    "\tautomation = ahocorasick.Automaton()\n",
    "\tfor idx, key in enumerate(kws):\n",
    "\t\tautomation.add_word(key, (idx, key))\n",
    "\tautomation.make_automaton()\n",
    "\treturn automation\n",
    "\n",
    "\n",
    "def extract_keywords(text: str, corasick_auto):\n",
    "\tkw = set()\n",
    "\tfor end_index, (insert_order, original_value) in corasick_auto.iter(str(text).lower()):\n",
    "\t\tkw.add(original_value)\n",
    "\treturn list(kw) if kw else None"
   ],
   "id": "9bf645964a22b698",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "con = duckdb.connect('../mining/result/models.db')\n",
    "\n",
    "tables = [\n",
    "\t\"models\",\n",
    "\t\"hf_discussions\",\n",
    "\t\"hf_discussion_events\",\n",
    "\t\"gh_repositories\",\n",
    "\t\"gh_discussions\",\n",
    "\t\"gh_comments\",\n",
    "\t\"gh_issues\",\n",
    "]\n",
    "\n",
    "for table in tables:\n",
    "\t# df = con.execute(f\"SELECT * from {table}\").df()\n",
    "\t# df.to_csv(f\"./db_dump_csv/{table}.csv\", index=False)\n",
    "\n",
    "\tcount = con.execute(f\"SELECT COUNT(1) from {table}\")\n",
    "\tprint(f\"{table=}, {count.fetchone()=}\")\n",
    "\n",
    "# con.execute(f\"COPY {table} TO './db_dump_csv/{table}.csv' (HEADER, DELIMITER ',')\")\n"
   ],
   "id": "c875f5acb48aa060"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df = pd.read_csv(\"./db_dump_csv/models.csv\")\n",
    "\n",
    "df[\"github_links_set\"] = None\n",
    "df[\"github_links_score\"] = None\n",
    "df[\"highest_score_link\"] = None\n",
    "df[\"highest_score\"] = None\n",
    "\n",
    "# file_path = \"huggingface_models_likes_all.csv\"\n",
    "# df = pd.read_csv(file_path, )\n",
    "# df.columns = [\n",
    "#     \"model_id\", \"downloads\", \"downloads_all_time\", \"likes\", \"trending_score\", \"pipeline_tags\",\n",
    "#     \"tags\", \"card_data\", \"base_model_from_card_data\", \"scan_done\", \"files_with_issues\",\n",
    "#     \"adapter_count\", \"merge_count\", \"quantized_count\", \"finetune_count\"\n",
    "# ]\n",
    "\n",
    "print(df.info())\n",
    "print(len(df))\n",
    "\n",
    "df.drop_duplicates(subset=[\"model_id\"], inplace=True)\n",
    "print(len(df))\n",
    "\n",
    "df.dropna(subset=[\"github_links\"], inplace=True)\n",
    "print(len(df))\n",
    "\n",
    "to_remove = [\n",
    "\t\")\", \"(\", \",\",\n",
    "]\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "\tprint(index, row[\"model_id\"])\n",
    "\tlink_str = row[\"github_links\"]\n",
    "\t# df[\"github_links_set\"] = df[\"github_links\"].apply(extract_github_repo_link_set)\n",
    "\t# def extract_github_repo_link_set(link_str: str):\n",
    "\tif pd.isna(link_str) or link_str == \"\" or link_str == \"[]\":\n",
    "\t\tdf.at[index, \"github_links_set\"] = None\n",
    "\t\tcontinue\n",
    "\n",
    "\tlinks = link_str.split(\",\")\n",
    "\trepo_link = None\n",
    "\tprocessed_links = set()\n",
    "\t# authors = set()\n",
    "\t# repos = set()\n",
    "\n",
    "\tfor link in links:\n",
    "\t\tif link == \"\":\n",
    "\t\t\tcontinue\n",
    "\t\tlink = link.strip()\n",
    "\t\t# remove special characters\n",
    "\t\tfor char in to_remove:\n",
    "\t\t\tlink = link.replace(char, \"\")\n",
    "\n",
    "\t\t# capture entire link\n",
    "\t\tif re.search(r\"https?:\\/\\/?github\\.com\\/[\\w-]+\\/[\\w-]+\", link):\n",
    "\t\t\trepo_link = re.search(r\"https?:\\/\\/?github\\.com\\/[\\w-]+\\/[\\w-]+\", link).group(0)\n",
    "\t\t\tprocessed_links.add(repo_link)\n",
    "\t\t\tauthor_repo = repo_link.replace(\"https://github.com/\", \"\")\n",
    "\n",
    "\t# capture each group\n",
    "\t# author, repo = re.search(r\"https?:\\/\\/?github\\.com\\/([\\w-]+)\\/([\\w-]+)\", link).groups()\n",
    "\t# authors.add(author)\n",
    "\t# repos.add(repo)\n",
    "\n",
    "\tif processed_links:\n",
    "\t\tprocessed_links = list(processed_links)\n",
    "\t\tdf.at[index, \"github_links_set\"] = processed_links\n",
    "\t\tscores = [ratio(link, row[\"model_id\"]) for link in processed_links]\n",
    "\t\tdf.at[index, \"github_links_score\"] = scores\n",
    "\t\tdf.at[index, \"highest_score_link\"] = processed_links[scores.index(max(scores))]\n",
    "\t\tdf.at[index, \"highest_score\"] = max(scores)\n",
    "# else:\n",
    "# \tdf.at[index, \"github_links_set\"] = None\n",
    "# \tdf.at[index, \"github_links_score\"] = None\n",
    "\n",
    "# print(list(processed_links))\n",
    "# row[\"github_author\"] = authors if authors else None\n",
    "# row[\"github_repos\"] = repos if repos else None\n",
    "\n",
    "df.to_csv(\"./filtered/hf_models_with_scored_link.csv\", index=False)\n",
    "\n",
    "df.dropna(subset=[\"highest_score\"], inplace=True)\n",
    "print(len(df))\n",
    "print(df.highest_score_link.nunique())\n",
    "df.to_csv(\"./filtered/hf_models.csv\", index=False)"
   ],
   "id": "32c51e12b64910af"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# print(len(df))\n",
    "# df.drop_duplicates(subset=[\"model_id\"], inplace=True)\n",
    "# print(len(df))"
   ],
   "id": "9b09c6c808737f54"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(df['highest_score'].dropna(), bins=10, color='skyblue', edgecolor='black')\n",
    "plt.title('Distribution of Highest Scores')\n",
    "plt.xlabel('Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ],
   "id": "3bf2898adc29b42a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# filter other files\n",
    "files = [\n",
    "\t\"gh_comments.csv\",\n",
    "\t\"gh_discussions.csv\",\n",
    "\t\"gh_issues.csv\",\n",
    "\t\"gh_repositories.csv\",\n",
    "\t\"hf_discussion_events.csv\",\n",
    "\t\"hf_discussions.csv\",\n",
    "]\n",
    "\n",
    "bots = [\n",
    "\t\"allcontributors[bot]\",\n",
    "\t\"allstar-app[bot]\",\n",
    "\t\"azure-pipelines[bot]\",\n",
    "\t\"codeant-ai[bot]\",\n",
    "\t\"coderabbitai[bot]\",\n",
    "\t\"copybara-service[bot]\",\n",
    "\t\"dagshub[bot]\",\n",
    "\t\"deepsource-autofix[bot]\",\n",
    "\t\"dependabot-preview[bot]\",\n",
    "\t\"dependabot[bot]\",\n",
    "\t\"devin-ai-integration[bot]\",\n",
    "\t\"ellipsis-dev[bot]\",\n",
    "\t\"github-actions[bot]\",\n",
    "\t\"google-allstar-prod[bot]\",\n",
    "\t\"greenkeeper[bot]\",\n",
    "\t\"imgbot[bot]\",\n",
    "\t\"learn-build-service-prod[bot]\",\n",
    "\t\"lgtm-com[bot]\",\n",
    "\t\"linear[bot]\",\n",
    "\t\"lumberbot-app[bot]\",\n",
    "\t\"mend-for-github-com[bot]\",\n",
    "\t\"mergify[bot]\",\n",
    "\t\"microsoft-github-operations[bot]\",\n",
    "\t\"microsoft-github-policy-service[bot]\",\n",
    "\t\"opensearch-trigger-bot[bot]\",\n",
    "\t\"pre-commit-ci[bot]\",\n",
    "\t\"pull[bot]\",\n",
    "\t\"pytorch-bot[bot]\",\n",
    "\t\"renovate[bot]\",\n",
    "\t\"restyled-io[bot]\",\n",
    "\t\"sentry-io[bot]\",\n",
    "\t\"sourcery-ai[bot]\",\n",
    "\t\"stainless-app[bot]\",\n",
    "\t\"sweep-ai[bot]\",\n",
    "\t\"sweep-nightly[bot]\",\n",
    "\t\"sync-by-unito[bot]\",\n",
    "\t\"vs-code-engineering[bot]\",\n",
    "]\n",
    "\n",
    "for file in files:\n",
    "\t# needs to remove bot issues\n",
    "\tdf = pd.read_csv(f\"./db_dump_csv/{file}\")\n",
    "\tprint(f\"{file} original count {len(df)}\")\n",
    "\n",
    "\tif \"author_login\" in list(df):\n",
    "\t\tdf = df[~df['author_login'].isin(bots)]\n",
    "\telif \"user_login\" in list(df):\n",
    "\t\tdf = df[~df['user_login'].isin(bots)]\n",
    "\n",
    "\tdf.drop_duplicates()\n",
    "\n",
    "\tprint(f\"{file} filtered count {len(df)}\")\n",
    "\tdf.to_csv(f\"./filtered/{file}\", index=False)"
   ],
   "id": "6f5433d05d63b3ba"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "kws1 = [\"vulnerability\", \"vulnerabilities\", \"cwe\", \"CWE\", \"cve\", \"CVE\", \"security\"]\n",
    "\n",
    "corasick = generate_corasick(kws1)\n",
    "\n",
    "# df = pd.read_csv(\"./result/hf/hf_models_commits.csv\")\n",
    "# df[\"keywords\"] = None\n",
    "# for index, row in df.iterrows():\n",
    "#     df.at[index, 'keywords_title'] = extract_keywords(row['title'], corasick)\n",
    "#     df.at[index, 'keywords_message'] = extract_keywords(row['message'], corasick)\n",
    "# print(f\"Record with KW founds: {len(df[df['keywords_message'].notna()])} in {len(df)}\")\n",
    "# df[(df['keywords_title'].notna()) | (df['keywords_message'].notna())].to_csv(f\"./result/hf/hf_commits_kws1.csv\",\n",
    "#                                                                              index=False)\n",
    "\n",
    "# Find kws in HF\n",
    "\n",
    "df = pd.read_csv(\"./filtered/hf_discussions.csv\")\n",
    "df[\"keywords\"] = None\n",
    "for index, row in df.iterrows():\n",
    "\tdf.at[index, 'keywords'] = extract_keywords(row['title'], corasick)\n",
    "print(f\"HF Discussions with KW founds: {len(df[df['keywords'].notna()])} in {len(df)}\")\n",
    "df[df['keywords'].notna()].to_csv(f\"./filtered_with_kw/hf_discussions.csv\", index=False)\n",
    "\n",
    "df = pd.read_csv(\"./filtered/hf_discussion_events.csv\")\n",
    "df[\"keywords\"] = None\n",
    "for index, row in df.iterrows():\n",
    "\tdf.at[index, 'keywords'] = extract_keywords(row['content'], corasick)\n",
    "print(f\"HF Discussions Events with KW founds: {len(df[df['keywords'].notna()])} in {len(df)}\")\n",
    "df[df['keywords'].notna()].to_csv(f\"./filtered_with_kw/hf_discussion_events.csv\", index=False)"
   ],
   "id": "ee51f9772c54bdf9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Find kws in GH\n",
    "\n",
    "df = pd.read_csv(\"./filtered/gh_discussions.csv\")\n",
    "df[\"keywords\"] = None\n",
    "for index, row in df.iterrows():\n",
    "\tdf.at[index, 'keywords'] = extract_keywords(\n",
    "\t\tstr(row['discussion_title']) + \"\\n\" + str(row['discussion_body']),\n",
    "\t\tcorasick\n",
    "\t)\n",
    "print(f\"GH Discussions KW founds: {len(df[df['keywords'].notna()])} in {len(df)}\")\n",
    "df[df['keywords'].notna()].to_csv(f\"./filtered_with_kw/gh_discussions.csv\", index=False)\n",
    "\n",
    "df = pd.read_csv(\"./filtered/gh_comments.csv\")\n",
    "df[\"keywords\"] = None\n",
    "for index, row in df.iterrows():\n",
    "\tdf.at[index, 'keywords'] = extract_keywords(row['comment_body'], corasick)\n",
    "print(f\"GH Comments KW founds: {len(df[df['keywords'].notna()])} in {len(df)}\")\n",
    "df[df['keywords'].notna()].to_csv(f\"./filtered_with_kw/gh_comments.csv\", index=False)\n",
    "\n",
    "# gh_issues.csv\n",
    "df = pd.read_csv(\"./filtered/gh_issues.csv\")\n",
    "df[\"keywords\"] = None\n",
    "for index, row in df.iterrows():\n",
    "\tdf.at[index, 'keywords'] = extract_keywords(\n",
    "\t\tstr(row['issue_title']) + \"\\n\" + str(row['issue_body']),\n",
    "\t\tcorasick\n",
    "\t)\n",
    "print(f\"GH Issues KW founds: {len(df[df['keywords'].notna()])} in {len(df)}\")\n",
    "df[df['keywords'].notna()].to_csv(f\"./filtered_with_kw/gh_issues.csv\", index=False)"
   ],
   "id": "19bc9ac27ea8fb4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# merge from filtered HF_DISCUSSION, GH_DISCUSSION and GH_ISSUES\n",
    "# keep all discussions even if they do not have any comments\n",
    "hf_discussions = pd.read_csv(\"./filtered/hf_discussions.csv\")\n",
    "hf_discussion_events = pd.read_csv(\"./filtered/hf_discussion_events.csv\")\n",
    "merged_hf = pd.merge(hf_discussions, hf_discussion_events, on=[\"model_id\", \"num\"], how=\"left\")\n",
    "print(\"HF merged \", len(merged_hf))\n",
    "merged_hf.drop(columns=[\"keywords_x\", \"keywords_y\"], inplace=True)\n",
    "merged_hf.to_csv(\"./merged/merged_hf_discussions.csv\", index=False)\n",
    "\n",
    "gh_discussions = pd.read_csv(\"./filtered/gh_discussions.csv\")\n",
    "gh_comments = pd.read_csv(\"./filtered/gh_comments.csv\")\n",
    "merged_gh = pd.merge(gh_discussions, gh_comments, on=[\"repo_name\", \"discussion_number\"], how=\"left\")\n",
    "print(\"GH merged\", len(merged_gh))\n",
    "merged_gh.drop(columns=[\"id_x\", \"id_y\"], inplace=True)\n",
    "merged_gh.to_csv(\"./merged/merged_gh_discussions.csv\", index=False)\n"
   ],
   "id": "4be34b370a933b90"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# merge from filtered with keywords HF_DISCUSSION, GH_DISCUSSION and GH_ISSUES\n",
    "def combine_keywords(row):\n",
    "\tkeywords = set()\n",
    "\tif not pd.isna(row[\"keywords_x\"]):\n",
    "\t\tkws = eval(row[\"keywords_x\"])\n",
    "\t\tfor kw in kws:\n",
    "\t\t\tkeywords.add(kw)\n",
    "\tif not pd.isna(row[\"keywords_y\"]):\n",
    "\t\tkws = eval(row[\"keywords_x\"])\n",
    "\t\tfor kw in kws:\n",
    "\t\t\tkeywords.add(kw)\n",
    "\tif keywords:  # Check if the list is not empty\n",
    "\t\treturn list(keywords)\n",
    "\telse:\n",
    "\t\treturn None\n",
    "\n",
    "\n",
    "hf_discussions = pd.read_csv(\"./filtered_with_kw/hf_discussions.csv\")\n",
    "hf_discussion_events = pd.read_csv(\"./filtered_with_kw/hf_discussion_events.csv\")\n",
    "merged_hf = pd.merge(hf_discussions, hf_discussion_events, on=[\"model_id\", \"num\"], how=\"left\")\n",
    "merged_hf.drop_duplicates(inplace=True)\n",
    "merged_hf[\"keywords\"] = merged_hf.apply(combine_keywords, axis=1)\n",
    "merged_hf.drop(columns=[\"keywords_x\", \"keywords_y\"], inplace=True)\n",
    "print(len(merged_hf))\n",
    "merged_hf.to_csv(\"./merged_with_kw/merged_hf_discussions.csv\", index=False)\n",
    "\n",
    "gh_discussions = pd.read_csv(\"./filtered_with_kw/gh_discussions.csv\")\n",
    "gh_comments = pd.read_csv(\"./filtered_with_kw/gh_comments.csv\")\n",
    "merged_gh = pd.merge(gh_discussions, gh_comments, on=[\"repo_name\", \"discussion_number\"], how=\"left\")\n",
    "merged_gh.drop_duplicates(inplace=True)\n",
    "merged_gh[\"keywords\"] = merged_gh.apply(combine_keywords, axis=1)\n",
    "merged_gh.drop(columns=[\"keywords_x\", \"keywords_y\", \"id_x\", \"id_y\"], inplace=True)\n",
    "print(\"GH merged\", len(merged_gh))\n",
    "merged_gh.to_csv(\"./merged_with_kw/merged_gh_discussions.csv\", index=False)"
   ],
   "id": "62505105b08a8c3d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Copy from filtered_with_kw to manual for manual labelling\n",
    "# Process:\n",
    "- manual label hf_discussion\n",
    "- manual label hf_discussion_events\n",
    "- find all discussion_event where hf_discussion is_security = 1\n",
    "- reverse find all discussion where hf_discussion_event is_security = 1\n",
    "=> manual dataset for HF discussion\n",
    "\n"
   ],
   "id": "ec8f65c616e769b1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# reconstruct the hf comments links\n",
    "# and set up label\n",
    "df = pd.read_csv(\"./filtered_with_kw/hf_discussions.csv\")\n",
    "df[\"is_security\"] = -1\n",
    "df[\"security_category\"] = None\n",
    "df[\"keyword_count\"] = df[\"keywords\"].apply(lambda x: len(eval(x)) if pd.notna(x) else 0)\n",
    "counts = {n: len(df[df[\"keyword_count\"] == n]) for n in range(0, 7)}\n",
    "df.drop_duplicates(inplace=True)\n",
    "print(\"HF Discussions with n keywords:\", counts)\n",
    "df.to_csv(\"./manual/hf_discussions_working.csv\", index=False)\n",
    "\n",
    "df = pd.read_csv(\"./filtered_with_kw/hf_discussion_events.csv\")\n",
    "df[\"url\"] = \"https://huggingface.co/\" + df[\"model_id\"] + \"/discussions/\" + df[\"num\"].astype(str) + \"#\" + df[\"event_id\"]\n",
    "df[\"is_security\"] = -1\n",
    "df[\"security_category\"] = None\n",
    "df[\"keyword_count\"] = df[\"keywords\"].apply(lambda x: len(eval(x)) if pd.notna(x) else 0)\n",
    "counts = {n: len(df[df[\"keyword_count\"] == n]) for n in range(0, 7)}\n",
    "df.drop_duplicates(inplace=True)\n",
    "print(\"HF Discussions Event with n keywords:\", counts)\n",
    "df.to_csv(\"./manual/hf_discussion_events_working.csv\", index=False)\n",
    "\n",
    "df = pd.read_csv(\"./filtered_with_kw/gh_discussions.csv\")\n",
    "df[\"url\"] = \"https://github.com/\" + df[\"repo_name\"] + \"/discussions/\" + df[\"discussion_number\"].astype(str)\n",
    "df[\"is_security\"] = -1\n",
    "df[\"security_category\"] = None\n",
    "df[\"keyword_count\"] = df[\"keywords\"].apply(lambda x: len(eval(x)) if pd.notna(x) else 0)\n",
    "counts = {n: len(df[df[\"keyword_count\"] == n]) for n in range(0, 7)}\n",
    "df = df.drop(columns=[\"id\"]).drop_duplicates(\n",
    "\tsubset=[\"repo_name\", \"discussion_number\", \"discussion_title\", \"author_login\"])\n",
    "print(\"GH Discussions with n keywords:\", counts)\n",
    "df.to_csv(\"./manual/gh_discussions_working.csv\", index=False)\n",
    "\n",
    "df = pd.read_csv(\"./filtered_with_kw/gh_comments.csv\")\n",
    "df[\"url\"] = \"https://github.com/\" + df[\"repo_name\"] + \"/discussions/\" + df[\"discussion_number\"].astype(str)\n",
    "df[\"is_security\"] = -1\n",
    "df[\"security_category\"] = None\n",
    "df[\"keyword_count\"] = df[\"keywords\"].apply(lambda x: len(eval(x)) if pd.notna(x) else 0)\n",
    "counts = {n: len(df[df[\"keyword_count\"] == n]) for n in range(0, 7)}\n",
    "df = df.drop(columns=[\"id\"]).drop_duplicates()\n",
    "print(\"GH Discussions Comment with n keywords:\", counts)\n",
    "df.to_csv(\"./manual/gh_comments_working.csv\", index=False)\n",
    "\n",
    "df = pd.read_csv(\"./filtered_with_kw/gh_issues.csv\")\n",
    "df[\"url\"] = \"https://github.com/\" + df[\"repo_name\"] + \"/issues/\" + df[\"issue_number\"].astype(str)\n",
    "df[\"is_security\"] = -1\n",
    "df[\"security_category\"] = None\n",
    "df[\"keyword_count\"] = df[\"keywords\"].apply(lambda x: len(eval(x)) if pd.notna(x) else 0)\n",
    "counts = {n: len(df[df[\"keyword_count\"] == n]) for n in [0, 1, 2, 3, 4, 5, 6]}\n",
    "df = df.drop(columns=[\"id\"]).drop_duplicates()\n",
    "print(\"GH issues with n keywords:\", counts)\n",
    "df[df[\"keyword_count\"] >= 4].to_csv(\"./manual/gh_issues_subset_working.csv\", index=False)\n",
    "df.to_csv(\"./manual/gh_issues_working.csv\", index=False)\n"
   ],
   "id": "2bbe434abbb5154d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# test get cve in gh issues\n",
    "# df = pd.read_csv(\"./filtered_with_kw/gh_issues.csv\")\n",
    "# df[\"keyword_count\"] = df[\"keywords\"].apply(lambda x: len(eval(x)) if pd.notna(x) else 0)\n",
    "# counts = {n: len(df[df[\"keyword_count\"] == n]) for n in [0, 1, 2, 3, 4, 5, 6]}\n",
    "# print(\"GH issues with n keywords:\", counts)\n",
    "# df_cve = df[df[\"keywords\"].str.contains(\"cve|CVE\", na=False)]\n",
    "# print(len(df_cve))\n",
    "# df_cve\n",
    "# NOTES: cve contains a lot of false positive\n",
    "\n",
    "df = pd.read_csv(\"./filtered_with_kw/gh_issues.csv\")\n",
    "df[\"url\"] = \"https://github.com/\" + df[\"repo_name\"] + \"/issues/\" + df[\"issue_number\"].astype(str)\n",
    "df[\"is_security\"] = -1\n",
    "df[\"security_category\"] = None\n",
    "df[\"keyword_count\"] = df[\"keywords\"].apply(lambda x: len(eval(x)) if pd.notna(x) else 0)\n",
    "counts = {n: len(df[df[\"keyword_count\"] == n]) for n in [0, 1, 2, 3, 4, 5, 6]}\n",
    "df = df.drop(columns=[\"id\"]).drop_duplicates()\n",
    "print(\"GH issues with n keywords:\", counts)\n",
    "df[df[\"keyword_count\"] >= 3].to_csv(\"./manual/gh_issues_subset_3_working.csv\", index=False)"
   ],
   "id": "3ee5f67fbf4bd40a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# HF label reverse look up\n",
    "hf_discussion = pd.read_csv(\"./manual/hf_discussions_done.csv\")\n",
    "hf_discussion_event = pd.read_csv(\"./manual/hf_discussion_events_done.csv\")\n",
    "merged_hf = pd.read_csv(\"./merged/merged_hf_discussions.csv\")\n",
    "print(f\"{len(merged_hf)=}\")\n",
    "merged_hf.drop_duplicates(inplace=True)\n",
    "# merged_hf.dropna(subset=[\"content\"], inplace=True)\n",
    "merged_hf[\"is_security\"] = 0\n",
    "\n",
    "df_non_security_disc = hf_discussion[hf_discussion[\"is_security\"] == 0][[\"model_id\", \"num\"]]\n",
    "df_non_security_event = hf_discussion_event[hf_discussion_event[\"is_security\"] == 0][[\"model_id\", \"num\"]]\n",
    "non_security_union = pd.concat([df_non_security_disc, df_non_security_event]).drop_duplicates()\n",
    "print(f\"{len(non_security_union)=}\")\n",
    "\n",
    "df_security_disc = hf_discussion[hf_discussion[\"is_security\"] == 1][[\"model_id\", \"num\"]]\n",
    "df_security_event = hf_discussion_event[hf_discussion_event[\"is_security\"] == 1][[\"model_id\", \"num\"]]\n",
    "security_union = pd.concat([df_security_disc, df_security_event]).drop_duplicates()\n",
    "print(f\"{len(security_union)=}\")\n",
    "\n",
    "# keep only the rows we labelled in manual set, either 0 or 1\n",
    "merged_hf = merged_hf.merge(\n",
    "\tpd.concat([non_security_union, security_union]).drop_duplicates(),\n",
    "\ton=[\"model_id\", \"num\"],\n",
    "\thow=\"inner\"\n",
    ")\n",
    "print(f\"{len(merged_hf)=}\")\n",
    "\n",
    "merged_hf[\"is_security\"] = merged_hf[[\"model_id\", \"num\"]].apply(tuple, axis=1).isin(\n",
    "\tpd.concat([df_security_disc, df_security_event]).apply(tuple, axis=1)\n",
    ").astype(int)\n",
    "\n",
    "merged_hf.loc[\n",
    "\tmerged_hf[[\"model_id\", \"num\"]].apply(tuple, axis=1).isin(security_union.apply(tuple, axis=1)),\n",
    "\t\"is_security\"\n",
    "] = 1\n",
    "\n",
    "merged_hf = merged_hf[merged_hf[\"event_type\"] == \"comment\"]\n",
    "\n",
    "merged_hf.to_csv(\"./merged_after_manual/merged_hf_discussions.csv\", index=False)\n",
    "\n",
    "merged_hf_sec = merged_hf[merged_hf[\"is_security\"] == 1]\n",
    "print(f\"{len(merged_hf_sec)=}\")\n",
    "merged_hf_sec.to_csv(\"./merged_after_manual/merged_hf_discussions_security.csv\", index=False)\n",
    "\n",
    "# count the distinct combination of model_id and num\n",
    "distinct_discussions = merged_hf[['model_id', 'num']].drop_duplicates().shape[0]\n",
    "print(f\"Distinct HF discussion: {distinct_discussions}\")\n",
    "\n",
    "distinct_discussions = merged_hf_sec[['model_id', 'num']].drop_duplicates().shape[0]\n",
    "print(f\"Distinct Security HF discussion: {distinct_discussions}\")\n"
   ],
   "id": "cdc82a56c3256347"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# GH label reverse look up\n",
    "gh_discussion = pd.read_csv(\"./manual/gh_discussions_done.csv\")\n",
    "gh_comments = pd.read_csv(\"./manual/gh_comments_done.csv\")\n",
    "merged_gh = pd.read_csv(\"./merged/merged_gh_discussions.csv\")\n",
    "print(f\"{len(merged_gh)=}\")\n",
    "merged_gh.drop_duplicates(inplace=True)\n",
    "merged_gh[\"is_security\"] = 0\n",
    "\n",
    "df_non_security_disc = gh_discussion[gh_discussion[\"is_security\"] == 0][[\"repo_name\", \"discussion_number\"]]\n",
    "print(f\"{len(df_non_security_disc)=}\")\n",
    "df_non_security_cmt = gh_comments[gh_comments[\"is_security\"] == 0][[\"repo_name\", \"discussion_number\"]]\n",
    "print(f\"{len(df_non_security_cmt)=}\")\n",
    "non_security_union = pd.concat([df_non_security_disc, df_non_security_cmt]).drop_duplicates()\n",
    "print(f\"{len(non_security_union)=}\")\n",
    "\n",
    "df_security_disc = gh_discussion[gh_discussion[\"is_security\"] == 1][[\"repo_name\", \"discussion_number\"]]\n",
    "print(f\"{len(df_security_disc)=}\")\n",
    "df_security_cmt = gh_comments[gh_comments[\"is_security\"] == 1][[\"repo_name\", \"discussion_number\"]]\n",
    "print(f\"{len(df_security_cmt)=}\")\n",
    "security_union = pd.concat([df_security_disc, df_security_cmt]).drop_duplicates()\n",
    "# print(security_union)\n",
    "print(f\"{len(security_union)=}\")\n",
    "\n",
    "merged_gh = merged_gh.merge(\n",
    "\tpd.concat([non_security_union, security_union]).drop_duplicates(),\n",
    "\ton=[\"repo_name\", \"discussion_number\"],\n",
    "\thow=\"inner\"\n",
    ")\n",
    "print(f\"{len(merged_gh)=}\")\n",
    "\n",
    "merged_gh.loc[\n",
    "\tmerged_gh[[\"repo_name\", \"discussion_number\"]].apply(tuple, axis=1).isin(security_union.apply(tuple, axis=1)),\n",
    "\t\"is_security\"\n",
    "] = 1\n",
    "print(f\"{len(merged_gh)=}\")\n",
    "merged_gh.to_csv(\"./merged_after_manual/merged_gh_discussions.csv\", index=False)\n",
    "\n",
    "merged_gh_sec = merged_gh[merged_gh[\"is_security\"] == 1]\n",
    "print(f\"{len(merged_gh_sec)=}\")\n",
    "merged_gh_sec.to_csv(\"./merged_after_manual/merged_gh_discussions_security.csv\", index=False)\n",
    "\n",
    "# count the distinct combination of model_id and num\n",
    "distinct_discussions = merged_gh[['repo_name', 'discussion_number']].drop_duplicates().shape[0]\n",
    "print(f\"Distinct GH discussion: {distinct_discussions}\")\n",
    "\n",
    "distinct_discussions = merged_gh_sec[['repo_name', 'discussion_number']].drop_duplicates().shape[0]\n",
    "print(f\"Distinct Security HF discussion: {distinct_discussions}\")"
   ],
   "id": "9c169a9dc7fb36c9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# More manual check here on the before the final dataset",
   "id": "be05b2fde98805f8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# manual + data from external\n",
    "merged_gh = pd.read_csv(\"./merged_after_manual/merged_gh_discussions.csv\")\n",
    "merged_hf = pd.read_csv(\"./merged_after_manual/merged_hf_discussions.csv\")\n",
    "issues = pd.read_csv(\"./manual/gh_issues_subset_3_done.csv\")\n",
    "issues_external_sec = pd.read_csv(\"./external_issues/github_sec_issues.csv\", delimiter=\";\")\n",
    "issues_external_non_sec = pd.read_csv(\"./external_issues/github_nonsec_issues.csv\", delimiter=\";\")\n",
    "\n",
    "# repo_name,discussion_number,discussion_title,discussion_body,author_login_x,author_login_y,comment_body,is_security\n",
    "merged_columns = [\n",
    "\t\"id_name\", \"id_num\", \"type\", \"content\", \"is_security\"\n",
    "]\n",
    "\n",
    "# all_df = pd.DataFrame(columns=merged_columns)\n",
    "merged_gh[\"id_name\"] = merged_gh[\"repo_name\"]\n",
    "merged_gh[\"id_num\"] = merged_gh[\"discussion_number\"]\n",
    "merged_gh[\"content\"] = (\n",
    "\tmerged_gh[\"discussion_title\"].fillna(\"\").str.strip() + \" \" +\n",
    "\tmerged_gh[\"discussion_body\"].fillna(\"\").str.strip() + \" \" +\n",
    "\tmerged_gh[\"comment_body\"].fillna(\"\").str.strip()\n",
    ").str.strip()\n",
    "merged_gh[\"type\"] = \"GH_DISCUSSIONS\"\n",
    "\n",
    "# model_id,num,title,git_ref,url,event_id,event_type,content,is_security\n",
    "merged_hf[\"id_name\"] = merged_hf[\"model_id\"]\n",
    "merged_hf[\"id_num\"] = merged_hf[\"num\"]\n",
    "merged_hf[\"content\"] = (\n",
    "\tmerged_hf[\"title\"].fillna(\"\").str.strip() + \" \" +\n",
    "\tmerged_hf[\"content\"].fillna(\"\").str.strip()\n",
    ").str.strip()\n",
    "merged_hf[\"type\"] = \"HF_DISCUSSIONS\"\n",
    "\n",
    "# repo_name,issue_url,pr_from_issue,user_login,issue_number,keywords,url,issue_title,issue_body,is_security,security_category,keyword_count\n",
    "issues[\"id_name\"] = issues[\"repo_name\"]\n",
    "issues[\"id_num\"] = issues[\"issue_number\"]\n",
    "issues[\"content\"] = (\n",
    "\tissues[\"issue_title\"].fillna(\"\").str.strip() + \" \" +\n",
    "\tissues[\"issue_body\"].fillna(\"\").str.strip()\n",
    ")\n",
    "issues[\"type\"] = \"GH_ISSUES\"\n",
    "\n",
    "all_df = pd.concat(\n",
    "\t[\n",
    "\t\tmerged_gh[merged_columns],\n",
    "\t\tmerged_hf[merged_columns],\n",
    "\t\tissues[merged_columns]\n",
    "\t]\n",
    ")\n",
    "print(\"All manual records\", len(all_df))\n",
    "# shuffle\n",
    "all_df = all_df.sample(frac=1)\n",
    "all_df.to_csv(\"./merged_after_manual/merged_all.csv\", index=False)\n",
    "\n",
    "issues_external_sec[\"id_name\"] = issues_external_sec[\"repository\"]\n",
    "issues_external_sec[\"id_num\"] = issues_external_sec.apply(lambda x: int(str(x[\"issue_api_url\"]).strip().split(\"/\")[-1]),\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t  axis=1)\n",
    "issues_external_sec[\"content\"] = (\n",
    "\tissues_external_sec[\"issue_title\"].fillna(\"\").str.strip() + \" \" +\n",
    "\tissues_external_sec[\"description\"].fillna(\"\").str.strip()\n",
    ")\n",
    "issues_external_sec[\"type\"] = \"GH_ISSUES_EXTERNAL\"\n",
    "issues_external_sec[\"is_security\"] = 1\n",
    "\n",
    "issues_external_non_sec[\"id_name\"] = issues_external_non_sec[\"repository\"]\n",
    "issues_external_non_sec[\"id_num\"] = issues_external_non_sec.apply(\n",
    "\tlambda x: int(str(x[\"issue_api_url\"]).strip().split(\"/\")[-1]), axis=1)\n",
    "issues_external_non_sec[\"content\"] = (\n",
    "\tissues_external_non_sec[\"issue_title\"].fillna(\"\").str.strip() + \" \" +\n",
    "\tissues_external_non_sec[\"description\"].fillna(\"\").str.strip()\n",
    ")\n",
    "issues_external_non_sec[\"type\"] = \"GH_ISSUES_EXTERNAL\"\n",
    "issues_external_non_sec[\"is_security\"] = 0\n",
    "\n",
    "all_df = pd.concat(\n",
    "\t[\n",
    "\t\tall_df,\n",
    "\t\tissues_external_sec[merged_columns],\n",
    "\t\tissues_external_non_sec[merged_columns]\n",
    "\t]\n",
    ")\n",
    "print(\"Manual records + external issues\", len(all_df))\n",
    "# shuffle\n",
    "all_df = all_df.sample(frac=1)\n",
    "all_df.to_csv(\"./merged_after_manual/merged_all_with_external.csv\", index=False)"
   ],
   "id": "f603f79779599816"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "all_df",
   "id": "11e587aa1ed1c056"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "\n",
    "# test stratify\n",
    "train_df, temp_df = train_test_split(\n",
    "\tall_df,\n",
    "\ttest_size=0.2,\n",
    "\trandom_state=42,\n",
    "\tstratify=all_df[[\"type\", \"is_security\"]]\n",
    ")\n",
    "print(len(train_df[train_df[\"type\"] == \"GH_DISCUSSIONS\"]))\n",
    "print(len(temp_df[temp_df[\"type\"] == \"GH_DISCUSSIONS\"]))\n",
    "print(len(train_df[train_df[\"type\"] == \"GH_ISSUES\"]))\n",
    "print(len(temp_df[temp_df[\"type\"] == \"GH_ISSUES\"]))\n",
    "\n",
    "print(len(train_df[train_df[\"is_security\"] == 1]))\n",
    "print(len(temp_df[temp_df[\"is_security\"] == 1]))\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "for i, (train_index, test_index) in enumerate(skf.split(train_df, train_df[\"is_security\"])):\n",
    "\tprint(f\"Fold {i}:\")\n",
    "\tprint(f\"  Train: index={train_index}\")\n",
    "\tprint(f\"  Test:  index={test_index}\")\n"
   ],
   "id": "e306ad56b00afa6d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "c910251ebbd89021"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Multiple HF model can link to the same GH repo\n",
    "-> We consider all of these related to 1 AI project, for example:\n",
    "- meta-llama/Llama-2-7b\n",
    "- meta-llama/Llama-2-7b-chat-hf\n",
    "- meta-llama/Llama-2-70b-chat-hf\n",
    "- meta-llama/Llama-2-7b-hf\n",
    "- meta-llama/Llama-2-13b-chat-hf\n",
    "- meta-llama/Llama-2-70b-hf\n",
    "- meta-llama/Llama-2-13b-hf\n",
    "- meta-llama/Llama-2-7b-chat\n",
    "- meta-llama/Llama-2-70b\n",
    "- meta-llama/Llama-2-70b-chat\n",
    "- meta-llama/Llama-2-13b\n",
    "- meta-llama/Llama-2-13b-chat\n",
    "\n",
    "- All of these models link to http://github.com/facebookresearch/llama: -> belong to 1 project -> facebookresearch/llama."
   ],
   "id": "53c49dfbc149e5d5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# run model in classifier",
   "id": "a8fe8177e5876b15"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "27af9740e34fc4b0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def visualize_prob_sigmoid_distribution(file_path):\n",
    "\ttry:\n",
    "\t\tdf = pd.read_csv(file_path)\n",
    "\n",
    "\t\tif 'prob_sigmoid' not in df.columns:\n",
    "\t\t\tprint(f\"Error: 'prob_sigmoid' column not found in {file_path}\")\n",
    "\t\t\treturn\n",
    "\n",
    "\t\tif not all(0 <= x <= 1 for x in df['prob_sigmoid']):\n",
    "\t\t\tprint(\"Warning: data points found out of 0-1 range. will try to filter...\")\n",
    "\t\t\tdf = df[(df['prob_sigmoid'] >= 0) & (df['prob_sigmoid'] <= 1)]\n",
    "\t\t\tprint(f\"Filtered {sum(not (0 <= x <= 1) for x in df['prob_sigmoid'])} out of range datapoints.\")\n",
    "\n",
    "\t\tplt.figure(figsize=(10, 6))  # Adjust figure size as needed\n",
    "\t\tsns.histplot(df['prob_sigmoid'], kde=True, bins=30, color='skyblue')  #Histogram\n",
    "\t\tplt.title('Distribution of prob_sigmoid')\n",
    "\t\tplt.xlabel('prob_sigmoid')\n",
    "\t\tplt.ylabel('Frequency')\n",
    "\t\tplt.grid(axis='y', alpha=0.75)\n",
    "\n",
    "\t\tmean = df['prob_sigmoid'].mean()\n",
    "\t\tmedian = df['prob_sigmoid'].median()\n",
    "\t\tstd = df['prob_sigmoid'].std()\n",
    "\t\tplt.axvline(mean, color='red', linestyle='dashed', linewidth=1, label=f'Mean: {mean:.2f}')\n",
    "\t\tplt.axvline(median, color='green', linestyle='dashed', linewidth=1, label=f'Median: {median:.2f}')\n",
    "\t\tplt.legend()\n",
    "\n",
    "\t\tplt.tight_layout()\n",
    "\t\tplt.show()\n",
    "\n",
    "\texcept FileNotFoundError:\n",
    "\t\tprint(f\"Error: File not found at {file_path}\")\n",
    "\texcept pd.errors.EmptyDataError:\n",
    "\t\tprint(f\"Error: File {file_path} is empty.\")\n",
    "\texcept pd.errors.ParserError:\n",
    "\t\tprint(f\"Error: Could not parse {file_path}. Check file format.\")\n",
    "\texcept Exception as e:\n",
    "\t\tprint(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "\n",
    "file_path = 'inference/all_gh_bert_gh_last/raw_predictions.csv'\n",
    "visualize_prob_sigmoid_distribution(file_path)"
   ],
   "id": "e6e661cdf6c7fb7a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df = pd.read_csv(\"inference/all_hf_bert_hf_best/raw_predictions.csv\")\n",
    "df_security = df[df[\"is_security_prediction\"] == 1]\n",
    "print(len(df_security))\n",
    "\n",
    "file_path = 'inference/all_hf_bert_hf_best/raw_predictions.csv'\n",
    "visualize_prob_sigmoid_distribution(file_path)\n"
   ],
   "id": "1d607629e0f4b210"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T00:55:19.523324Z",
     "start_time": "2025-03-13T00:55:19.488783Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "\n",
    "# prediction result collection\n",
    "def collect_prediction(path: str, exclude: list[str]):\n",
    "\tmodels = [\"bert_base\", \"distilbert\", \"securebert\", \"roberta_base\", \"secbert\", \"secroberta\", ]\n",
    "\tall_metrics_data = []\n",
    "\n",
    "\tsubfolders = [\n",
    "\t\tf.path\n",
    "\t\tfor f in os.scandir(path)\n",
    "\t\tif f.is_dir() and f.name not in exclude\n",
    "\t]\n",
    "\n",
    "\tfor subfolder in subfolders:\n",
    "\t\tmetrics_file_path = os.path.join(subfolder, \"metrics.csv\")\n",
    "\t\tif not os.path.exists(metrics_file_path):\n",
    "\t\t\tcontinue\n",
    "\t\tdf = pd.read_csv(metrics_file_path)\n",
    "\t\t# only get the test result\n",
    "\t\tdf = df.tail(1)\n",
    "\t\tmodel_type = None\n",
    "\t\tdata_type = None\n",
    "\n",
    "\t\tmatch = re.search(r\"_({})\".format(\"|\".join(models)), os.path.basename(subfolder))\n",
    "\t\tif match:\n",
    "\t\t\tmodel_type = str(os.path.basename(subfolder)[match.start():]).replace(\"_\", \"\", 1)\n",
    "\t\t\tdata_type = os.path.basename(subfolder)[:match.start()]\n",
    "\n",
    "\t\tdf[\"input\"] = data_type\n",
    "\t\tdf[\"model_type\"] = model_type\n",
    "\t\tdf[\"subfolder\"] = os.path.basename(subfolder)\n",
    "\t\tdf[\"folder\"] = path\n",
    "\t\tall_metrics_data.append(df)\n",
    "\n",
    "\treturn pd.concat(all_metrics_data, ignore_index=True)\n",
    "\n",
    "\n",
    "path = \"./prediction\"\n",
    "exclude = [\"backup_2402\"]\n",
    "metrics_predictions = collect_prediction(path, exclude)\n",
    "metrics_predictions"
   ],
   "id": "a3e5b17bfb68f25f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "    fold  epoch  train_loss  eval_loss  test_loss  accuracy  precision  \\\n",
       "0    NaN    NaN         NaN        NaN   0.361670  0.950911   0.960799   \n",
       "1    NaN    NaN         NaN        NaN   0.352707  0.975000   0.931818   \n",
       "2    NaN    NaN         NaN        NaN   0.180531  0.973031   0.930328   \n",
       "3    NaN    NaN         NaN        NaN   0.494877  0.948718   0.947826   \n",
       "4    NaN    NaN         NaN        NaN   0.382849  0.958688   0.948107   \n",
       "5    NaN    NaN         NaN        NaN   0.166664  0.986792   0.939759   \n",
       "6    NaN    NaN         NaN        NaN   0.937315  0.888889   0.980000   \n",
       "7    NaN    NaN         NaN        NaN   0.254390  0.967638   0.939914   \n",
       "8    NaN    NaN         NaN        NaN   0.370002  0.948238   0.910299   \n",
       "9    NaN    NaN         NaN        NaN   0.133817  0.978571   0.897959   \n",
       "10   NaN    NaN         NaN        NaN   0.208375  0.981132   0.906977   \n",
       "11   4.0    1.0    0.005047   0.000421        NaN  1.000000   1.000000   \n",
       "12   NaN    NaN         NaN        NaN   0.382160  0.984906   0.950000   \n",
       "13   NaN    NaN         NaN        NaN   0.150151  0.971429   0.893617   \n",
       "14   NaN    NaN         NaN        NaN   0.396217  0.947509   0.932584   \n",
       "15   NaN    NaN         NaN        NaN   0.383204  0.965812   0.964602   \n",
       "16   NaN    NaN         NaN        NaN   0.372478  0.948718   0.972477   \n",
       "17   NaN    NaN         NaN        NaN   0.220628  0.983019   0.938272   \n",
       "18   NaN    NaN         NaN        NaN   0.295563  0.973031   0.960526   \n",
       "19   NaN    NaN         NaN        NaN   0.151344  0.960714   0.904762   \n",
       "20   NaN    NaN         NaN        NaN   0.353284  0.965480   0.963470   \n",
       "21   NaN    NaN         NaN        NaN   0.223303  0.975000   0.931818   \n",
       "22   NaN    NaN         NaN        NaN   0.452731  0.965812   0.964602   \n",
       "23   NaN    NaN         NaN        NaN   0.317121  0.988679   0.962500   \n",
       "24   NaN    NaN         NaN        NaN   0.301873  0.965812   0.972973   \n",
       "25   NaN    NaN         NaN        NaN   0.267395  0.988679   0.974359   \n",
       "26   NaN    NaN         NaN        NaN   0.111090  0.982143   0.900000   \n",
       "27   NaN    NaN         NaN        NaN   0.198852  0.977346   0.965217   \n",
       "28   NaN    NaN         NaN        NaN   0.430154  0.948238   0.935780   \n",
       "29   NaN    NaN         NaN        NaN   0.183172  0.976268   0.957082   \n",
       "\n",
       "      recall        f1  f1_macro       mcc  elapsed_time            input  \\\n",
       "0   0.897099  0.927857  0.945328  0.891926  15753.684496     all_external   \n",
       "1   0.911111  0.921348  0.953243  0.906569    518.330959        manual_gh   \n",
       "2   0.965957  0.947808  0.964813  0.929933   3380.589169              all   \n",
       "3   1.000000  0.973214  0.686607  0.486782    223.580802  manual_gh_issue   \n",
       "4   0.933702  0.940849  0.954554  0.909175   8007.931883     all_external   \n",
       "5   0.975000  0.957055  0.974626  0.949479   1971.510443        manual_hf   \n",
       "6   0.899083  0.937799  0.708900  0.464872    237.166367  manual_gh_issue   \n",
       "7   0.931915  0.935897  0.957126  0.914268   3458.170598              all   \n",
       "8   0.946133  0.927870  0.943753  0.887932  16331.838002     all_external   \n",
       "9   0.977778  0.936170  0.961647  0.924531   1042.465266        manual_gh   \n",
       "10  0.975000  0.939759  0.964287  0.929444    973.460714        manual_hf   \n",
       "11  1.000000  1.000000  1.000000  1.000000   6284.000524     all_external   \n",
       "12  0.950000  0.950000  0.970556  0.941111    972.133759        manual_hf   \n",
       "13  0.933333  0.913043  0.947975  0.896262    584.782611        manual_gh   \n",
       "14  0.917127  0.924791  0.942239  0.884555   7599.123032     all_external   \n",
       "15  1.000000  0.981982  0.824324  0.694479    210.662634  manual_gh_issue   \n",
       "16  0.972477  0.972477  0.798739  0.597477    427.888581  manual_gh_issue   \n",
       "17  0.950000  0.944099  0.967044  0.934114   2014.018980        manual_hf   \n",
       "18  0.931915  0.946004  0.964016  0.928226   1750.049126              all   \n",
       "19  0.844444  0.873563  0.925154  0.851049    559.224996        manual_gh   \n",
       "20  0.897872  0.929515  0.953329  0.907684   1746.297576              all   \n",
       "21  0.911111  0.921348  0.953243  0.906569   1010.865430        manual_gh   \n",
       "22  1.000000  0.981982  0.824324  0.694479    434.641863  manual_gh_issue   \n",
       "23  0.962500  0.962500  0.977917  0.955833    954.863716        manual_hf   \n",
       "24  0.990826  0.981818  0.848052  0.704659    396.990788  manual_gh_issue   \n",
       "25  0.950000  0.962025  0.977687  0.955480   1846.634074        manual_hf   \n",
       "26  1.000000  0.947368  0.968308  0.938537    988.678939        manual_gh   \n",
       "27  0.944681  0.954839  0.969860  0.939819   1727.326307              all   \n",
       "28  0.915746  0.925654  0.942976  0.886082   7756.670320     all_external   \n",
       "29  0.948936  0.952991  0.968559  0.937134   3587.974625              all   \n",
       "\n",
       "      model_type                     subfolder        folder  \n",
       "0     securebert       all_external_securebert  ./prediction  \n",
       "1     distilbert          manual_gh_distilbert  ./prediction  \n",
       "2      bert_base                 all_bert_base  ./prediction  \n",
       "3     distilbert    manual_gh_issue_distilbert  ./prediction  \n",
       "4     distilbert       all_external_distilbert  ./prediction  \n",
       "5   roberta_base        manual_hf_roberta_base  ./prediction  \n",
       "6     secroberta    manual_gh_issue_secroberta  ./prediction  \n",
       "7     securebert                all_securebert  ./prediction  \n",
       "8      bert_base        all_external_bert_base  ./prediction  \n",
       "9   roberta_base        manual_gh_roberta_base  ./prediction  \n",
       "10    distilbert          manual_hf_distilbert  ./prediction  \n",
       "11  roberta_base     all_external_roberta_base  ./prediction  \n",
       "12    secroberta          manual_hf_secroberta  ./prediction  \n",
       "13       secbert             manual_gh_secbert  ./prediction  \n",
       "14       secbert          all_external_secbert  ./prediction  \n",
       "15       secbert       manual_gh_issue_secbert  ./prediction  \n",
       "16     bert_base     manual_gh_issue_bert_base  ./prediction  \n",
       "17    securebert          manual_hf_securebert  ./prediction  \n",
       "18    secroberta                all_secroberta  ./prediction  \n",
       "19    secroberta          manual_gh_secroberta  ./prediction  \n",
       "20       secbert                   all_secbert  ./prediction  \n",
       "21     bert_base           manual_gh_bert_base  ./prediction  \n",
       "22  roberta_base  manual_gh_issue_roberta_base  ./prediction  \n",
       "23       secbert             manual_hf_secbert  ./prediction  \n",
       "24    securebert    manual_gh_issue_securebert  ./prediction  \n",
       "25     bert_base           manual_hf_bert_base  ./prediction  \n",
       "26    securebert          manual_gh_securebert  ./prediction  \n",
       "27    distilbert                all_distilbert  ./prediction  \n",
       "28    secroberta       all_external_secroberta  ./prediction  \n",
       "29  roberta_base              all_roberta_base  ./prediction  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fold</th>\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>eval_loss</th>\n",
       "      <th>test_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>f1_macro</th>\n",
       "      <th>mcc</th>\n",
       "      <th>elapsed_time</th>\n",
       "      <th>input</th>\n",
       "      <th>model_type</th>\n",
       "      <th>subfolder</th>\n",
       "      <th>folder</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.361670</td>\n",
       "      <td>0.950911</td>\n",
       "      <td>0.960799</td>\n",
       "      <td>0.897099</td>\n",
       "      <td>0.927857</td>\n",
       "      <td>0.945328</td>\n",
       "      <td>0.891926</td>\n",
       "      <td>15753.684496</td>\n",
       "      <td>all_external</td>\n",
       "      <td>securebert</td>\n",
       "      <td>all_external_securebert</td>\n",
       "      <td>./prediction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.352707</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.931818</td>\n",
       "      <td>0.911111</td>\n",
       "      <td>0.921348</td>\n",
       "      <td>0.953243</td>\n",
       "      <td>0.906569</td>\n",
       "      <td>518.330959</td>\n",
       "      <td>manual_gh</td>\n",
       "      <td>distilbert</td>\n",
       "      <td>manual_gh_distilbert</td>\n",
       "      <td>./prediction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.180531</td>\n",
       "      <td>0.973031</td>\n",
       "      <td>0.930328</td>\n",
       "      <td>0.965957</td>\n",
       "      <td>0.947808</td>\n",
       "      <td>0.964813</td>\n",
       "      <td>0.929933</td>\n",
       "      <td>3380.589169</td>\n",
       "      <td>all</td>\n",
       "      <td>bert_base</td>\n",
       "      <td>all_bert_base</td>\n",
       "      <td>./prediction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.494877</td>\n",
       "      <td>0.948718</td>\n",
       "      <td>0.947826</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.973214</td>\n",
       "      <td>0.686607</td>\n",
       "      <td>0.486782</td>\n",
       "      <td>223.580802</td>\n",
       "      <td>manual_gh_issue</td>\n",
       "      <td>distilbert</td>\n",
       "      <td>manual_gh_issue_distilbert</td>\n",
       "      <td>./prediction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.382849</td>\n",
       "      <td>0.958688</td>\n",
       "      <td>0.948107</td>\n",
       "      <td>0.933702</td>\n",
       "      <td>0.940849</td>\n",
       "      <td>0.954554</td>\n",
       "      <td>0.909175</td>\n",
       "      <td>8007.931883</td>\n",
       "      <td>all_external</td>\n",
       "      <td>distilbert</td>\n",
       "      <td>all_external_distilbert</td>\n",
       "      <td>./prediction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.166664</td>\n",
       "      <td>0.986792</td>\n",
       "      <td>0.939759</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.957055</td>\n",
       "      <td>0.974626</td>\n",
       "      <td>0.949479</td>\n",
       "      <td>1971.510443</td>\n",
       "      <td>manual_hf</td>\n",
       "      <td>roberta_base</td>\n",
       "      <td>manual_hf_roberta_base</td>\n",
       "      <td>./prediction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.937315</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.980000</td>\n",
       "      <td>0.899083</td>\n",
       "      <td>0.937799</td>\n",
       "      <td>0.708900</td>\n",
       "      <td>0.464872</td>\n",
       "      <td>237.166367</td>\n",
       "      <td>manual_gh_issue</td>\n",
       "      <td>secroberta</td>\n",
       "      <td>manual_gh_issue_secroberta</td>\n",
       "      <td>./prediction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.254390</td>\n",
       "      <td>0.967638</td>\n",
       "      <td>0.939914</td>\n",
       "      <td>0.931915</td>\n",
       "      <td>0.935897</td>\n",
       "      <td>0.957126</td>\n",
       "      <td>0.914268</td>\n",
       "      <td>3458.170598</td>\n",
       "      <td>all</td>\n",
       "      <td>securebert</td>\n",
       "      <td>all_securebert</td>\n",
       "      <td>./prediction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.370002</td>\n",
       "      <td>0.948238</td>\n",
       "      <td>0.910299</td>\n",
       "      <td>0.946133</td>\n",
       "      <td>0.927870</td>\n",
       "      <td>0.943753</td>\n",
       "      <td>0.887932</td>\n",
       "      <td>16331.838002</td>\n",
       "      <td>all_external</td>\n",
       "      <td>bert_base</td>\n",
       "      <td>all_external_bert_base</td>\n",
       "      <td>./prediction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.133817</td>\n",
       "      <td>0.978571</td>\n",
       "      <td>0.897959</td>\n",
       "      <td>0.977778</td>\n",
       "      <td>0.936170</td>\n",
       "      <td>0.961647</td>\n",
       "      <td>0.924531</td>\n",
       "      <td>1042.465266</td>\n",
       "      <td>manual_gh</td>\n",
       "      <td>roberta_base</td>\n",
       "      <td>manual_gh_roberta_base</td>\n",
       "      <td>./prediction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.208375</td>\n",
       "      <td>0.981132</td>\n",
       "      <td>0.906977</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.939759</td>\n",
       "      <td>0.964287</td>\n",
       "      <td>0.929444</td>\n",
       "      <td>973.460714</td>\n",
       "      <td>manual_hf</td>\n",
       "      <td>distilbert</td>\n",
       "      <td>manual_hf_distilbert</td>\n",
       "      <td>./prediction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.005047</td>\n",
       "      <td>0.000421</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6284.000524</td>\n",
       "      <td>all_external</td>\n",
       "      <td>roberta_base</td>\n",
       "      <td>all_external_roberta_base</td>\n",
       "      <td>./prediction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.382160</td>\n",
       "      <td>0.984906</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.970556</td>\n",
       "      <td>0.941111</td>\n",
       "      <td>972.133759</td>\n",
       "      <td>manual_hf</td>\n",
       "      <td>secroberta</td>\n",
       "      <td>manual_hf_secroberta</td>\n",
       "      <td>./prediction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.150151</td>\n",
       "      <td>0.971429</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.913043</td>\n",
       "      <td>0.947975</td>\n",
       "      <td>0.896262</td>\n",
       "      <td>584.782611</td>\n",
       "      <td>manual_gh</td>\n",
       "      <td>secbert</td>\n",
       "      <td>manual_gh_secbert</td>\n",
       "      <td>./prediction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.396217</td>\n",
       "      <td>0.947509</td>\n",
       "      <td>0.932584</td>\n",
       "      <td>0.917127</td>\n",
       "      <td>0.924791</td>\n",
       "      <td>0.942239</td>\n",
       "      <td>0.884555</td>\n",
       "      <td>7599.123032</td>\n",
       "      <td>all_external</td>\n",
       "      <td>secbert</td>\n",
       "      <td>all_external_secbert</td>\n",
       "      <td>./prediction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.383204</td>\n",
       "      <td>0.965812</td>\n",
       "      <td>0.964602</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.981982</td>\n",
       "      <td>0.824324</td>\n",
       "      <td>0.694479</td>\n",
       "      <td>210.662634</td>\n",
       "      <td>manual_gh_issue</td>\n",
       "      <td>secbert</td>\n",
       "      <td>manual_gh_issue_secbert</td>\n",
       "      <td>./prediction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.372478</td>\n",
       "      <td>0.948718</td>\n",
       "      <td>0.972477</td>\n",
       "      <td>0.972477</td>\n",
       "      <td>0.972477</td>\n",
       "      <td>0.798739</td>\n",
       "      <td>0.597477</td>\n",
       "      <td>427.888581</td>\n",
       "      <td>manual_gh_issue</td>\n",
       "      <td>bert_base</td>\n",
       "      <td>manual_gh_issue_bert_base</td>\n",
       "      <td>./prediction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.220628</td>\n",
       "      <td>0.983019</td>\n",
       "      <td>0.938272</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.944099</td>\n",
       "      <td>0.967044</td>\n",
       "      <td>0.934114</td>\n",
       "      <td>2014.018980</td>\n",
       "      <td>manual_hf</td>\n",
       "      <td>securebert</td>\n",
       "      <td>manual_hf_securebert</td>\n",
       "      <td>./prediction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.295563</td>\n",
       "      <td>0.973031</td>\n",
       "      <td>0.960526</td>\n",
       "      <td>0.931915</td>\n",
       "      <td>0.946004</td>\n",
       "      <td>0.964016</td>\n",
       "      <td>0.928226</td>\n",
       "      <td>1750.049126</td>\n",
       "      <td>all</td>\n",
       "      <td>secroberta</td>\n",
       "      <td>all_secroberta</td>\n",
       "      <td>./prediction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.151344</td>\n",
       "      <td>0.960714</td>\n",
       "      <td>0.904762</td>\n",
       "      <td>0.844444</td>\n",
       "      <td>0.873563</td>\n",
       "      <td>0.925154</td>\n",
       "      <td>0.851049</td>\n",
       "      <td>559.224996</td>\n",
       "      <td>manual_gh</td>\n",
       "      <td>secroberta</td>\n",
       "      <td>manual_gh_secroberta</td>\n",
       "      <td>./prediction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.353284</td>\n",
       "      <td>0.965480</td>\n",
       "      <td>0.963470</td>\n",
       "      <td>0.897872</td>\n",
       "      <td>0.929515</td>\n",
       "      <td>0.953329</td>\n",
       "      <td>0.907684</td>\n",
       "      <td>1746.297576</td>\n",
       "      <td>all</td>\n",
       "      <td>secbert</td>\n",
       "      <td>all_secbert</td>\n",
       "      <td>./prediction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.223303</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.931818</td>\n",
       "      <td>0.911111</td>\n",
       "      <td>0.921348</td>\n",
       "      <td>0.953243</td>\n",
       "      <td>0.906569</td>\n",
       "      <td>1010.865430</td>\n",
       "      <td>manual_gh</td>\n",
       "      <td>bert_base</td>\n",
       "      <td>manual_gh_bert_base</td>\n",
       "      <td>./prediction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.452731</td>\n",
       "      <td>0.965812</td>\n",
       "      <td>0.964602</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.981982</td>\n",
       "      <td>0.824324</td>\n",
       "      <td>0.694479</td>\n",
       "      <td>434.641863</td>\n",
       "      <td>manual_gh_issue</td>\n",
       "      <td>roberta_base</td>\n",
       "      <td>manual_gh_issue_roberta_base</td>\n",
       "      <td>./prediction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.317121</td>\n",
       "      <td>0.988679</td>\n",
       "      <td>0.962500</td>\n",
       "      <td>0.962500</td>\n",
       "      <td>0.962500</td>\n",
       "      <td>0.977917</td>\n",
       "      <td>0.955833</td>\n",
       "      <td>954.863716</td>\n",
       "      <td>manual_hf</td>\n",
       "      <td>secbert</td>\n",
       "      <td>manual_hf_secbert</td>\n",
       "      <td>./prediction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.301873</td>\n",
       "      <td>0.965812</td>\n",
       "      <td>0.972973</td>\n",
       "      <td>0.990826</td>\n",
       "      <td>0.981818</td>\n",
       "      <td>0.848052</td>\n",
       "      <td>0.704659</td>\n",
       "      <td>396.990788</td>\n",
       "      <td>manual_gh_issue</td>\n",
       "      <td>securebert</td>\n",
       "      <td>manual_gh_issue_securebert</td>\n",
       "      <td>./prediction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.267395</td>\n",
       "      <td>0.988679</td>\n",
       "      <td>0.974359</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.962025</td>\n",
       "      <td>0.977687</td>\n",
       "      <td>0.955480</td>\n",
       "      <td>1846.634074</td>\n",
       "      <td>manual_hf</td>\n",
       "      <td>bert_base</td>\n",
       "      <td>manual_hf_bert_base</td>\n",
       "      <td>./prediction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.111090</td>\n",
       "      <td>0.982143</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.968308</td>\n",
       "      <td>0.938537</td>\n",
       "      <td>988.678939</td>\n",
       "      <td>manual_gh</td>\n",
       "      <td>securebert</td>\n",
       "      <td>manual_gh_securebert</td>\n",
       "      <td>./prediction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.198852</td>\n",
       "      <td>0.977346</td>\n",
       "      <td>0.965217</td>\n",
       "      <td>0.944681</td>\n",
       "      <td>0.954839</td>\n",
       "      <td>0.969860</td>\n",
       "      <td>0.939819</td>\n",
       "      <td>1727.326307</td>\n",
       "      <td>all</td>\n",
       "      <td>distilbert</td>\n",
       "      <td>all_distilbert</td>\n",
       "      <td>./prediction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.430154</td>\n",
       "      <td>0.948238</td>\n",
       "      <td>0.935780</td>\n",
       "      <td>0.915746</td>\n",
       "      <td>0.925654</td>\n",
       "      <td>0.942976</td>\n",
       "      <td>0.886082</td>\n",
       "      <td>7756.670320</td>\n",
       "      <td>all_external</td>\n",
       "      <td>secroberta</td>\n",
       "      <td>all_external_secroberta</td>\n",
       "      <td>./prediction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.183172</td>\n",
       "      <td>0.976268</td>\n",
       "      <td>0.957082</td>\n",
       "      <td>0.948936</td>\n",
       "      <td>0.952991</td>\n",
       "      <td>0.968559</td>\n",
       "      <td>0.937134</td>\n",
       "      <td>3587.974625</td>\n",
       "      <td>all</td>\n",
       "      <td>roberta_base</td>\n",
       "      <td>all_roberta_base</td>\n",
       "      <td>./prediction</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def collect_inference(path: str, exclude: list[str]):\n",
    "\tmodels = [\"llama\", \"deepseek\", \"phi4\", \"mistral\"]\n",
    "\tall_metrics_data = []\n",
    "\n",
    "\tsubfolders = [\n",
    "\t\tf.path\n",
    "\t\tfor f in os.scandir(path)\n",
    "\t\tif f.is_dir() and f.name not in exclude\n",
    "\t]\n",
    "\n",
    "\tfor subfolder in subfolders:\n",
    "\t\tmetrics_file_path = os.path.join(subfolder, \"metrics.json\")\n",
    "\t\tif not os.path.exists(metrics_file_path):\n",
    "\t\t\tcontinue\n",
    "\t\twith open(metrics_file_path, 'r') as f:\n",
    "\t\t\tdata = json.load(f)\n",
    "\t\tdf = pd.DataFrame([data])\n",
    "\t\t# only get the test result\n",
    "\t\tdf[\"subfolder\"] = os.path.basename(subfolder)\n",
    "\t\tmodel = None\n",
    "\t\tdata = None\n",
    "\t\tmatch = re.search(r\"_(llama\\d*|deepseekr\\d*|phi4|mistral(_small)?)\", os.path.basename(subfolder))\n",
    "\t\tif match:\n",
    "\t\t\tmodel = str(os.path.basename(subfolder)[match.start():]).replace(\"_\", \"\", 1)\n",
    "\t\t\tdata = os.path.basename(subfolder)[:match.start()]\n",
    "\t\tdf[\"input_type\"] = data\n",
    "\t\tdf[\"model\"] = model\n",
    "\t\tdf[\"path\"] = path\n",
    "\t\tall_metrics_data.append(df)\n",
    "\treturn pd.concat(all_metrics_data, ignore_index=True)\n",
    "\n",
    "\n",
    "path = \"./llm\"\n",
    "exclude = []\n",
    "metrics_infer = collect_inference(path, exclude)\n",
    "metrics_infer"
   ],
   "id": "31decfe7cda47e09"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import norm\n",
    "\n",
    "\n",
    "def get_subset(df_security, df_manual, col_ids: list[str]):\n",
    "\trecords_to_exclude = df_manual[col_ids].drop_duplicates()\n",
    "\tcol_ids_str = \"_\".join(col_ids)\n",
    "\trecords_to_exclude[col_ids_str] = records_to_exclude[col_ids[0]].astype(str) + \"_\" + records_to_exclude[\n",
    "\t\tcol_ids[1]].astype(str)\n",
    "\tprint(f\"{len(records_to_exclude)=}\")\n",
    "\tN = len(df_security)  # Population size\n",
    "\tZ = norm.ppf(0.975)  # Z-score for 95% confidence\n",
    "\tp = 0.5  # Worst-case scenario proportion\n",
    "\tE = 0.05  # Margin of error\n",
    "\n",
    "\t# Sample size formula\n",
    "\tnumerator = (N * (Z ** 2) * p * (1 - p))\n",
    "\tdenominator = ((E ** 2) * (N - 1)) + ((Z ** 2) * p * (1 - p))\n",
    "\tsample_size = int(numerator / denominator)\n",
    "\tprint(f\"Required sample size: {sample_size} out of {N}\")\n",
    "\n",
    "\tsample_df = pd.DataFrame()\n",
    "\twhile len(sample_df) < sample_size:\n",
    "\t\tprint(f\"Current {len(sample_df)=}, {sample_size-len(sample_df)=}\")\n",
    "\t\ttemp_sample = df_security.sample(n=(sample_size - len(sample_df)), random_state=42)\n",
    "\t\ttemp_sample[col_ids_str] = temp_sample[col_ids[0]].astype(str) + \"_\" + temp_sample[col_ids[1]].astype(str)\n",
    "\t\ttemp_sample = temp_sample[~temp_sample[col_ids_str].isin(records_to_exclude[col_ids_str])]\n",
    "\t\tsample_df = pd.concat([sample_df, temp_sample])\n",
    "\n",
    "\tprint(f\"Final sample {len(sample_df)=}\")\n",
    "\treturn sample_df\n",
    "\n",
    "\n",
    "expr = \"all_gh_bert_gh_last\"\n",
    "gh = pd.read_csv(f\"./inference/{expr}/raw_predictions.csv\")\n",
    "gh_security = gh[gh[\"is_security_prediction\"] == 1]\n",
    "gh_manual = pd.read_csv(f\"./merged_after_manual/merged_gh_discussions.csv\")\n",
    "gh_security_sampled = get_subset(gh_security, gh_manual, [\"repo_name\", \"discussion_number\"])\n",
    "gh_security_sampled.to_csv(f\"./sampled/{expr}_sampled.csv\", index=False)"
   ],
   "id": "1c30963ec06b6b1e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
