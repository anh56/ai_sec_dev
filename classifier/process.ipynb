{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T00:24:15.668129Z",
     "start_time": "2025-03-18T00:24:15.638384Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import duckdb\n",
    "import re\n",
    "import csv\n",
    "import sys\n",
    "import ahocorasick\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "import pandas as pd\n",
    "from Levenshtein import ratio\n",
    "\n",
    "\n",
    "def generate_corasick(kws: list[str]):\n",
    "\tautomation = ahocorasick.Automaton()\n",
    "\tfor idx, key in enumerate(kws):\n",
    "\t\tautomation.add_word(key, (idx, key))\n",
    "\tautomation.make_automaton()\n",
    "\treturn automation\n",
    "\n",
    "\n",
    "def extract_keywords(text: str, corasick_auto):\n",
    "\tkw = set()\n",
    "\tfor end_index, (insert_order, original_value) in corasick_auto.iter(str(text).lower()):\n",
    "\t\tkw.add(original_value)\n",
    "\treturn list(kw) if kw else None"
   ],
   "id": "9bf645964a22b698",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "con = duckdb.connect('../mining/result/models.db')\n",
    "\n",
    "tables = [\n",
    "\t\"models\",\n",
    "\t\"hf_discussions\",\n",
    "\t\"hf_discussion_events\",\n",
    "\t\"gh_repositories\",\n",
    "\t\"gh_discussions\",\n",
    "\t\"gh_comments\",\n",
    "\t\"gh_issues\",\n",
    "]\n",
    "\n",
    "for table in tables:\n",
    "\t# df = con.execute(f\"SELECT * from {table}\").df()\n",
    "\t# df.to_csv(f\"./db_dump_csv/{table}.csv\", index=False)\n",
    "\n",
    "\tcount = con.execute(f\"SELECT COUNT(1) from {table}\")\n",
    "\tprint(f\"{table=}, {count.fetchone()=}\")\n",
    "\n",
    "# con.execute(f\"COPY {table} TO './db_dump_csv/{table}.csv' (HEADER, DELIMITER ',')\")\n"
   ],
   "id": "c875f5acb48aa060"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df = pd.read_csv(\"./db_dump_csv/models.csv\")\n",
    "\n",
    "df[\"github_links_set\"] = None\n",
    "df[\"github_links_score\"] = None\n",
    "df[\"highest_score_link\"] = None\n",
    "df[\"highest_score\"] = None\n",
    "\n",
    "# file_path = \"huggingface_models_likes_all.csv\"\n",
    "# df = pd.read_csv(file_path, )\n",
    "# df.columns = [\n",
    "#     \"model_id\", \"downloads\", \"downloads_all_time\", \"likes\", \"trending_score\", \"pipeline_tags\",\n",
    "#     \"tags\", \"card_data\", \"base_model_from_card_data\", \"scan_done\", \"files_with_issues\",\n",
    "#     \"adapter_count\", \"merge_count\", \"quantized_count\", \"finetune_count\"\n",
    "# ]\n",
    "\n",
    "print(df.info())\n",
    "print(len(df))\n",
    "\n",
    "df.drop_duplicates(subset=[\"model_id\"], inplace=True)\n",
    "print(len(df))\n",
    "\n",
    "df.dropna(subset=[\"github_links\"], inplace=True)\n",
    "print(len(df))\n",
    "\n",
    "to_remove = [\n",
    "\t\")\", \"(\", \",\",\n",
    "]\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "\tprint(index, row[\"model_id\"])\n",
    "\tlink_str = row[\"github_links\"]\n",
    "\t# df[\"github_links_set\"] = df[\"github_links\"].apply(extract_github_repo_link_set)\n",
    "\t# def extract_github_repo_link_set(link_str: str):\n",
    "\tif pd.isna(link_str) or link_str == \"\" or link_str == \"[]\":\n",
    "\t\tdf.at[index, \"github_links_set\"] = None\n",
    "\t\tcontinue\n",
    "\n",
    "\tlinks = link_str.split(\",\")\n",
    "\trepo_link = None\n",
    "\tprocessed_links = set()\n",
    "\t# authors = set()\n",
    "\t# repos = set()\n",
    "\n",
    "\tfor link in links:\n",
    "\t\tif link == \"\":\n",
    "\t\t\tcontinue\n",
    "\t\tlink = link.strip()\n",
    "\t\t# remove special characters\n",
    "\t\tfor char in to_remove:\n",
    "\t\t\tlink = link.replace(char, \"\")\n",
    "\n",
    "\t\t# capture entire link\n",
    "\t\tif re.search(r\"https?:\\/\\/?github\\.com\\/[\\w-]+\\/[\\w-]+\", link):\n",
    "\t\t\trepo_link = re.search(r\"https?:\\/\\/?github\\.com\\/[\\w-]+\\/[\\w-]+\", link).group(0)\n",
    "\t\t\tprocessed_links.add(repo_link)\n",
    "\t\t\tauthor_repo = repo_link.replace(\"https://github.com/\", \"\")\n",
    "\n",
    "\t# capture each group\n",
    "\t# author, repo = re.search(r\"https?:\\/\\/?github\\.com\\/([\\w-]+)\\/([\\w-]+)\", link).groups()\n",
    "\t# authors.add(author)\n",
    "\t# repos.add(repo)\n",
    "\n",
    "\tif processed_links:\n",
    "\t\tprocessed_links = list(processed_links)\n",
    "\t\tdf.at[index, \"github_links_set\"] = processed_links\n",
    "\t\tscores = [ratio(link, row[\"model_id\"]) for link in processed_links]\n",
    "\t\tdf.at[index, \"github_links_score\"] = scores\n",
    "\t\tdf.at[index, \"highest_score_link\"] = processed_links[scores.index(max(scores))]\n",
    "\t\tdf.at[index, \"highest_score\"] = max(scores)\n",
    "# else:\n",
    "# \tdf.at[index, \"github_links_set\"] = None\n",
    "# \tdf.at[index, \"github_links_score\"] = None\n",
    "\n",
    "# print(list(processed_links))\n",
    "# row[\"github_author\"] = authors if authors else None\n",
    "# row[\"github_repos\"] = repos if repos else None\n",
    "\n",
    "df.to_csv(\"./filtered/hf_models_with_scored_link.csv\", index=False)\n",
    "\n",
    "df.dropna(subset=[\"highest_score\"], inplace=True)\n",
    "print(len(df))\n",
    "print(df.highest_score_link.nunique())\n",
    "df.to_csv(\"./filtered/hf_models.csv\", index=False)"
   ],
   "id": "32c51e12b64910af"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# print(len(df))\n",
    "# df.drop_duplicates(subset=[\"model_id\"], inplace=True)\n",
    "# print(len(df))"
   ],
   "id": "9b09c6c808737f54"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(df['highest_score'].dropna(), bins=10, color='skyblue', edgecolor='black')\n",
    "plt.title('Distribution of Highest Scores')\n",
    "plt.xlabel('Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ],
   "id": "3bf2898adc29b42a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# filter other files\n",
    "files = [\n",
    "\t\"gh_comments.csv\",\n",
    "\t\"gh_discussions.csv\",\n",
    "\t\"gh_issues.csv\",\n",
    "\t\"gh_repositories.csv\",\n",
    "\t\"hf_discussion_events.csv\",\n",
    "\t\"hf_discussions.csv\",\n",
    "]\n",
    "\n",
    "bots = [\n",
    "\t\"allcontributors[bot]\",\n",
    "\t\"allstar-app[bot]\",\n",
    "\t\"azure-pipelines[bot]\",\n",
    "\t\"codeant-ai[bot]\",\n",
    "\t\"coderabbitai[bot]\",\n",
    "\t\"copybara-service[bot]\",\n",
    "\t\"dagshub[bot]\",\n",
    "\t\"deepsource-autofix[bot]\",\n",
    "\t\"dependabot-preview[bot]\",\n",
    "\t\"dependabot[bot]\",\n",
    "\t\"devin-ai-integration[bot]\",\n",
    "\t\"ellipsis-dev[bot]\",\n",
    "\t\"github-actions[bot]\",\n",
    "\t\"google-allstar-prod[bot]\",\n",
    "\t\"greenkeeper[bot]\",\n",
    "\t\"imgbot[bot]\",\n",
    "\t\"learn-build-service-prod[bot]\",\n",
    "\t\"lgtm-com[bot]\",\n",
    "\t\"linear[bot]\",\n",
    "\t\"lumberbot-app[bot]\",\n",
    "\t\"mend-for-github-com[bot]\",\n",
    "\t\"mergify[bot]\",\n",
    "\t\"microsoft-github-operations[bot]\",\n",
    "\t\"microsoft-github-policy-service[bot]\",\n",
    "\t\"opensearch-trigger-bot[bot]\",\n",
    "\t\"pre-commit-ci[bot]\",\n",
    "\t\"pull[bot]\",\n",
    "\t\"pytorch-bot[bot]\",\n",
    "\t\"renovate[bot]\",\n",
    "\t\"restyled-io[bot]\",\n",
    "\t\"sentry-io[bot]\",\n",
    "\t\"sourcery-ai[bot]\",\n",
    "\t\"stainless-app[bot]\",\n",
    "\t\"sweep-ai[bot]\",\n",
    "\t\"sweep-nightly[bot]\",\n",
    "\t\"sync-by-unito[bot]\",\n",
    "\t\"vs-code-engineering[bot]\",\n",
    "]\n",
    "\n",
    "for file in files:\n",
    "\t# needs to remove bot issues\n",
    "\tdf = pd.read_csv(f\"./db_dump_csv/{file}\")\n",
    "\tprint(f\"{file} original count {len(df)}\")\n",
    "\n",
    "\tif \"author_login\" in list(df):\n",
    "\t\tdf = df[~df['author_login'].isin(bots)]\n",
    "\telif \"user_login\" in list(df):\n",
    "\t\tdf = df[~df['user_login'].isin(bots)]\n",
    "\n",
    "\tdf.drop_duplicates()\n",
    "\n",
    "\tprint(f\"{file} filtered count {len(df)}\")\n",
    "\tdf.to_csv(f\"./filtered/{file}\", index=False)"
   ],
   "id": "6f5433d05d63b3ba"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "kws1 = [\"vulnerability\", \"vulnerabilities\", \"cwe\", \"CWE\", \"cve\", \"CVE\", \"security\"]\n",
    "\n",
    "corasick = generate_corasick(kws1)\n",
    "\n",
    "# df = pd.read_csv(\"./result/hf/hf_models_commits.csv\")\n",
    "# df[\"keywords\"] = None\n",
    "# for index, row in df.iterrows():\n",
    "#     df.at[index, 'keywords_title'] = extract_keywords(row['title'], corasick)\n",
    "#     df.at[index, 'keywords_message'] = extract_keywords(row['message'], corasick)\n",
    "# print(f\"Record with KW founds: {len(df[df['keywords_message'].notna()])} in {len(df)}\")\n",
    "# df[(df['keywords_title'].notna()) | (df['keywords_message'].notna())].to_csv(f\"./result/hf/hf_commits_kws1.csv\",\n",
    "#                                                                              index=False)\n",
    "\n",
    "# Find kws in HF\n",
    "\n",
    "df = pd.read_csv(\"./filtered/hf_discussions.csv\")\n",
    "df[\"keywords\"] = None\n",
    "for index, row in df.iterrows():\n",
    "\tdf.at[index, 'keywords'] = extract_keywords(row['title'], corasick)\n",
    "print(f\"HF Discussions with KW founds: {len(df[df['keywords'].notna()])} in {len(df)}\")\n",
    "df[df['keywords'].notna()].to_csv(f\"./filtered_with_kw/hf_discussions.csv\", index=False)\n",
    "\n",
    "df = pd.read_csv(\"./filtered/hf_discussion_events.csv\")\n",
    "df[\"keywords\"] = None\n",
    "for index, row in df.iterrows():\n",
    "\tdf.at[index, 'keywords'] = extract_keywords(row['content'], corasick)\n",
    "print(f\"HF Discussions Events with KW founds: {len(df[df['keywords'].notna()])} in {len(df)}\")\n",
    "df[df['keywords'].notna()].to_csv(f\"./filtered_with_kw/hf_discussion_events.csv\", index=False)"
   ],
   "id": "ee51f9772c54bdf9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Find kws in GH\n",
    "\n",
    "df = pd.read_csv(\"./filtered/gh_discussions.csv\")\n",
    "df[\"keywords\"] = None\n",
    "for index, row in df.iterrows():\n",
    "\tdf.at[index, 'keywords'] = extract_keywords(\n",
    "\t\tstr(row['discussion_title']) + \"\\n\" + str(row['discussion_body']),\n",
    "\t\tcorasick\n",
    "\t)\n",
    "print(f\"GH Discussions KW founds: {len(df[df['keywords'].notna()])} in {len(df)}\")\n",
    "df[df['keywords'].notna()].to_csv(f\"./filtered_with_kw/gh_discussions.csv\", index=False)\n",
    "\n",
    "df = pd.read_csv(\"./filtered/gh_comments.csv\")\n",
    "df[\"keywords\"] = None\n",
    "for index, row in df.iterrows():\n",
    "\tdf.at[index, 'keywords'] = extract_keywords(row['comment_body'], corasick)\n",
    "print(f\"GH Comments KW founds: {len(df[df['keywords'].notna()])} in {len(df)}\")\n",
    "df[df['keywords'].notna()].to_csv(f\"./filtered_with_kw/gh_comments.csv\", index=False)\n",
    "\n",
    "# gh_issues.csv\n",
    "df = pd.read_csv(\"./filtered/gh_issues.csv\")\n",
    "df[\"keywords\"] = None\n",
    "for index, row in df.iterrows():\n",
    "\tdf.at[index, 'keywords'] = extract_keywords(\n",
    "\t\tstr(row['issue_title']) + \"\\n\" + str(row['issue_body']),\n",
    "\t\tcorasick\n",
    "\t)\n",
    "print(f\"GH Issues KW founds: {len(df[df['keywords'].notna()])} in {len(df)}\")\n",
    "df[df['keywords'].notna()].to_csv(f\"./filtered_with_kw/gh_issues.csv\", index=False)"
   ],
   "id": "19bc9ac27ea8fb4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# merge from filtered HF_DISCUSSION, GH_DISCUSSION and GH_ISSUES\n",
    "# keep all discussions even if they do not have any comments\n",
    "hf_discussions = pd.read_csv(\"./filtered/hf_discussions.csv\")\n",
    "hf_discussion_events = pd.read_csv(\"./filtered/hf_discussion_events.csv\")\n",
    "merged_hf = pd.merge(hf_discussions, hf_discussion_events, on=[\"model_id\", \"num\"], how=\"left\")\n",
    "print(\"HF merged \", len(merged_hf))\n",
    "merged_hf.drop(columns=[\"keywords_x\", \"keywords_y\"], inplace=True)\n",
    "merged_hf.to_csv(\"./merged/merged_hf_discussions.csv\", index=False)\n",
    "\n",
    "gh_discussions = pd.read_csv(\"./filtered/gh_discussions.csv\")\n",
    "gh_comments = pd.read_csv(\"./filtered/gh_comments.csv\")\n",
    "merged_gh = pd.merge(gh_discussions, gh_comments, on=[\"repo_name\", \"discussion_number\"], how=\"left\")\n",
    "print(\"GH merged\", len(merged_gh))\n",
    "merged_gh.drop(columns=[\"id_x\", \"id_y\"], inplace=True)\n",
    "merged_gh.to_csv(\"./merged/merged_gh_discussions.csv\", index=False)\n"
   ],
   "id": "4be34b370a933b90"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# merge from filtered with keywords HF_DISCUSSION, GH_DISCUSSION and GH_ISSUES\n",
    "def combine_keywords(row):\n",
    "\tkeywords = set()\n",
    "\tif not pd.isna(row[\"keywords_x\"]):\n",
    "\t\tkws = eval(row[\"keywords_x\"])\n",
    "\t\tfor kw in kws:\n",
    "\t\t\tkeywords.add(kw)\n",
    "\tif not pd.isna(row[\"keywords_y\"]):\n",
    "\t\tkws = eval(row[\"keywords_x\"])\n",
    "\t\tfor kw in kws:\n",
    "\t\t\tkeywords.add(kw)\n",
    "\tif keywords:  # Check if the list is not empty\n",
    "\t\treturn list(keywords)\n",
    "\telse:\n",
    "\t\treturn None\n",
    "\n",
    "\n",
    "hf_discussions = pd.read_csv(\"./filtered_with_kw/hf_discussions.csv\")\n",
    "hf_discussion_events = pd.read_csv(\"./filtered_with_kw/hf_discussion_events.csv\")\n",
    "merged_hf = pd.merge(hf_discussions, hf_discussion_events, on=[\"model_id\", \"num\"], how=\"left\")\n",
    "merged_hf.drop_duplicates(inplace=True)\n",
    "merged_hf[\"keywords\"] = merged_hf.apply(combine_keywords, axis=1)\n",
    "merged_hf.drop(columns=[\"keywords_x\", \"keywords_y\"], inplace=True)\n",
    "print(len(merged_hf))\n",
    "merged_hf.to_csv(\"./merged_with_kw/merged_hf_discussions.csv\", index=False)\n",
    "\n",
    "gh_discussions = pd.read_csv(\"./filtered_with_kw/gh_discussions.csv\")\n",
    "gh_comments = pd.read_csv(\"./filtered_with_kw/gh_comments.csv\")\n",
    "merged_gh = pd.merge(gh_discussions, gh_comments, on=[\"repo_name\", \"discussion_number\"], how=\"left\")\n",
    "merged_gh.drop_duplicates(inplace=True)\n",
    "merged_gh[\"keywords\"] = merged_gh.apply(combine_keywords, axis=1)\n",
    "merged_gh.drop(columns=[\"keywords_x\", \"keywords_y\", \"id_x\", \"id_y\"], inplace=True)\n",
    "print(\"GH merged\", len(merged_gh))\n",
    "merged_gh.to_csv(\"./merged_with_kw/merged_gh_discussions.csv\", index=False)"
   ],
   "id": "62505105b08a8c3d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Copy from filtered_with_kw to manual for manual labelling\n",
    "# Process:\n",
    "- manual label hf_discussion\n",
    "- manual label hf_discussion_events\n",
    "- find all discussion_event where hf_discussion is_security = 1\n",
    "- reverse find all discussion where hf_discussion_event is_security = 1\n",
    "=> manual dataset for HF discussion\n",
    "\n"
   ],
   "id": "ec8f65c616e769b1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# manually removed\n",
    "# hf discussion events\n",
    "# -> comment not english\n",
    "\n",
    "# 6240,taide/TAIDE-LX-7B-Chat,7,663c7c7e5e9f6a229b09fbc0,comment -> comment not english\n",
    "# 18855,BELLE-2/Belle-whisper-large-v3-turbo-zh,3,comment -> comment not english\n",
    "# 115921,taide/TAIDE-LX-7B-Chat-4bit,4,66262481571552066e022e3b,comment\n",
    "# 116245,taide/TAIDE-LX-7B-Chat-4bit,3,66274569a6a017b27d981884,comment\n",
    "# 201341,taide/Llama3-TAIDE-LX-8B-Chat-Alpha1,4,6638c793f31d59ae35fa9a71,comment\n",
    "# 201909,taide/Llama3-TAIDE-LX-8B-Chat-Alpha1,2,663389530175b82b87e76c43,comment\n",
    "# 253944,taide/Llama3-TAIDE-LX-8B-Chat-Alpha1-4bit,3,663b08a1ffc4bb91c1042526,comment\n",
    "# 399720,IDEA-CCNL/Ziya-LLaMA-13B-v1.1,3,64895cb884f4f879933f9bf0,comment\n",
    "# BAAI/bge-m3,25,66812cf32698e064710ab834,comment\n",
    "\n",
    "#   -> gated model\n",
    "# google/paligemma-3b-pt-224,2,6643d0633914b80624ebddcb,comment\n",
    "\n",
    "\n",
    "# 25674,CyberPeace-Institute/SecureBERT-NER,4,654e762654d044f09ee10c8f,comment\n",
    "# https://huggingface.co/CyberPeace-Institute/SecureBERT-NER/discussions/4\n",
    "# potential LLM issues, does not differentiate from the domain of the model and the content of the discussions\n",
    "\n",
    "# Phind/Phind-CodeLlama-34B-v2,15,6508cedb2feb9570c5f964d8 -> ethics related, does it count as security\n",
    "# linking between HF and GH: https://huggingface.co/OuteAI/OuteTTS-0.1-350M/discussions/5 -> https://github.com/edwko/OuteTTS/issues/16#issuecomment-2465288308\n",
    "\n",
    "# https://huggingface.co/cgato/TheSpice-7b-v0.1.1/discussions/1 -> safetensor suggestion\n",
    "\n",
    "\n",
    "# gh\n",
    "# keras-team/keras-hub,1393 -> contain pr to security policy -> https://github.com/keras-team/keras-hub/pull/1319\n",
    "# https://github.com/ShishirPatil/gorilla/discussions/457 -> contain pr to vul fix -> https://github.com/ShishirPatil/gorilla/pull/415\n",
    "\n"
   ],
   "id": "de8ac0acaabd4adb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# reconstruct the hf comments links\n",
    "# and set up label\n",
    "df = pd.read_csv(\"./filtered_with_kw/hf_discussions.csv\")\n",
    "df[\"is_security\"] = -1\n",
    "df[\"security_category\"] = None\n",
    "df[\"keyword_count\"] = df[\"keywords\"].apply(lambda x: len(eval(x)) if pd.notna(x) else 0)\n",
    "counts = {n: len(df[df[\"keyword_count\"] == n]) for n in range(0, 7)}\n",
    "df.drop_duplicates(inplace=True)\n",
    "print(\"HF Discussions with n keywords:\", counts)\n",
    "df.to_csv(\"./manual/hf_discussions_working.csv\", index=False)\n",
    "\n",
    "df = pd.read_csv(\"./filtered_with_kw/hf_discussion_events.csv\")\n",
    "df[\"url\"] = \"https://huggingface.co/\" + df[\"model_id\"] + \"/discussions/\" + df[\"num\"].astype(str) + \"#\" + df[\"event_id\"]\n",
    "df[\"is_security\"] = -1\n",
    "df[\"security_category\"] = None\n",
    "df[\"keyword_count\"] = df[\"keywords\"].apply(lambda x: len(eval(x)) if pd.notna(x) else 0)\n",
    "counts = {n: len(df[df[\"keyword_count\"] == n]) for n in range(0, 7)}\n",
    "df.drop_duplicates(inplace=True)\n",
    "print(\"HF Discussions Event with n keywords:\", counts)\n",
    "df.to_csv(\"./manual/hf_discussion_events_working.csv\", index=False)\n",
    "\n",
    "df = pd.read_csv(\"./filtered_with_kw/gh_discussions.csv\")\n",
    "df[\"url\"] = \"https://github.com/\" + df[\"repo_name\"] + \"/discussions/\" + df[\"discussion_number\"].astype(str)\n",
    "df[\"is_security\"] = -1\n",
    "df[\"security_category\"] = None\n",
    "df[\"keyword_count\"] = df[\"keywords\"].apply(lambda x: len(eval(x)) if pd.notna(x) else 0)\n",
    "counts = {n: len(df[df[\"keyword_count\"] == n]) for n in range(0, 7)}\n",
    "df = df.drop(columns=[\"id\"]).drop_duplicates(\n",
    "\tsubset=[\"repo_name\", \"discussion_number\", \"discussion_title\", \"author_login\"])\n",
    "print(\"GH Discussions with n keywords:\", counts)\n",
    "df.to_csv(\"./manual/gh_discussions_working.csv\", index=False)\n",
    "\n",
    "df = pd.read_csv(\"./filtered_with_kw/gh_comments.csv\")\n",
    "df[\"url\"] = \"https://github.com/\" + df[\"repo_name\"] + \"/discussions/\" + df[\"discussion_number\"].astype(str)\n",
    "df[\"is_security\"] = -1\n",
    "df[\"security_category\"] = None\n",
    "df[\"keyword_count\"] = df[\"keywords\"].apply(lambda x: len(eval(x)) if pd.notna(x) else 0)\n",
    "counts = {n: len(df[df[\"keyword_count\"] == n]) for n in range(0, 7)}\n",
    "df = df.drop(columns=[\"id\"]).drop_duplicates()\n",
    "print(\"GH Discussions Comment with n keywords:\", counts)\n",
    "df.to_csv(\"./manual/gh_comments_working.csv\", index=False)\n",
    "\n",
    "df = pd.read_csv(\"./filtered_with_kw/gh_issues.csv\")\n",
    "df[\"url\"] = \"https://github.com/\" + df[\"repo_name\"] + \"/issues/\" + df[\"issue_number\"].astype(str)\n",
    "df[\"is_security\"] = -1\n",
    "df[\"security_category\"] = None\n",
    "df[\"keyword_count\"] = df[\"keywords\"].apply(lambda x: len(eval(x)) if pd.notna(x) else 0)\n",
    "counts = {n: len(df[df[\"keyword_count\"] == n]) for n in [0, 1, 2, 3, 4, 5, 6]}\n",
    "df = df.drop(columns=[\"id\"]).drop_duplicates()\n",
    "print(\"GH issues with n keywords:\", counts)\n",
    "df[df[\"keyword_count\"] >= 4].to_csv(\"./manual/gh_issues_subset_working.csv\", index=False)\n",
    "df.to_csv(\"./manual/gh_issues_working.csv\", index=False)\n",
    "# 1st pass without removing dups\n",
    "# HF Discussions with n keywords: {0: 0, 1: 17, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0}\n",
    "# HF Discussions Event with n keywords: {0: 0, 1: 276, 2: 10, 3: 5, 4: 0, 5: 0, 6: 0}\n",
    "# GH Discussions with n keywords: {0: 0, 1: 142, 2: 9, 3: 1, 4: 1, 5: 0, 6: 0}\n",
    "# GH Discussions Comment with n keywords: {0: 0, 1: 92, 2: 2, 3: 0, 4: 0, 5: 0, 6: 0}\n",
    "# GH issues with n keywords: {0: 0, 1: 21790, 2: 1381, 3: 507, 4: 73, 5: 4, 6: 0}"
   ],
   "id": "2bbe434abbb5154d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# test get cve in gh issues\n",
    "# df = pd.read_csv(\"./filtered_with_kw/gh_issues.csv\")\n",
    "# df[\"keyword_count\"] = df[\"keywords\"].apply(lambda x: len(eval(x)) if pd.notna(x) else 0)\n",
    "# counts = {n: len(df[df[\"keyword_count\"] == n]) for n in [0, 1, 2, 3, 4, 5, 6]}\n",
    "# print(\"GH issues with n keywords:\", counts)\n",
    "# df_cve = df[df[\"keywords\"].str.contains(\"cve|CVE\", na=False)]\n",
    "# print(len(df_cve))\n",
    "# df_cve\n",
    "# NOTES: cve contains a lot of false positive\n",
    "\n",
    "df = pd.read_csv(\"./filtered_with_kw/gh_issues.csv\")\n",
    "df[\"url\"] = \"https://github.com/\" + df[\"repo_name\"] + \"/issues/\" + df[\"issue_number\"].astype(str)\n",
    "df[\"is_security\"] = -1\n",
    "df[\"security_category\"] = None\n",
    "df[\"keyword_count\"] = df[\"keywords\"].apply(lambda x: len(eval(x)) if pd.notna(x) else 0)\n",
    "counts = {n: len(df[df[\"keyword_count\"] == n]) for n in [0, 1, 2, 3, 4, 5, 6]}\n",
    "df = df.drop(columns=[\"id\"]).drop_duplicates()\n",
    "print(\"GH issues with n keywords:\", counts)\n",
    "df[df[\"keyword_count\"] >= 3].to_csv(\"./manual/gh_issues_subset_3_working.csv\", index=False)"
   ],
   "id": "3ee5f67fbf4bd40a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# HF label reverse look up\n",
    "hf_discussion = pd.read_csv(\"./manual/hf_discussions_done.csv\")\n",
    "hf_discussion_event = pd.read_csv(\"./manual/hf_discussion_events_done.csv\")\n",
    "merged_hf = pd.read_csv(\"./merged/merged_hf_discussions.csv\")\n",
    "print(f\"{len(merged_hf)=}\")\n",
    "merged_hf.drop_duplicates(inplace=True)\n",
    "# merged_hf.dropna(subset=[\"content\"], inplace=True)\n",
    "merged_hf[\"is_security\"] = 0\n",
    "\n",
    "df_non_security_disc = hf_discussion[hf_discussion[\"is_security\"] == 0][[\"model_id\", \"num\"]]\n",
    "df_non_security_event = hf_discussion_event[hf_discussion_event[\"is_security\"] == 0][[\"model_id\", \"num\"]]\n",
    "non_security_union = pd.concat([df_non_security_disc, df_non_security_event]).drop_duplicates()\n",
    "print(f\"{len(non_security_union)=}\")\n",
    "\n",
    "df_security_disc = hf_discussion[hf_discussion[\"is_security\"] == 1][[\"model_id\", \"num\"]]\n",
    "df_security_event = hf_discussion_event[hf_discussion_event[\"is_security\"] == 1][[\"model_id\", \"num\"]]\n",
    "security_union = pd.concat([df_security_disc, df_security_event]).drop_duplicates()\n",
    "print(f\"{len(security_union)=}\")\n",
    "\n",
    "# keep only the rows we labelled in manual set, either 0 or 1\n",
    "merged_hf = merged_hf.merge(\n",
    "\tpd.concat([non_security_union, security_union]).drop_duplicates(),\n",
    "\ton=[\"model_id\", \"num\"],\n",
    "\thow=\"inner\"\n",
    ")\n",
    "print(f\"{len(merged_hf)=}\")\n",
    "\n",
    "merged_hf[\"is_security\"] = merged_hf[[\"model_id\", \"num\"]].apply(tuple, axis=1).isin(\n",
    "\tpd.concat([df_security_disc, df_security_event]).apply(tuple, axis=1)\n",
    ").astype(int)\n",
    "\n",
    "merged_hf.loc[\n",
    "\tmerged_hf[[\"model_id\", \"num\"]].apply(tuple, axis=1).isin(security_union.apply(tuple, axis=1)),\n",
    "\t\"is_security\"\n",
    "] = 1\n",
    "\n",
    "merged_hf = merged_hf[merged_hf[\"event_type\"] == \"comment\"]\n",
    "\n",
    "merged_hf.to_csv(\"./merged_after_manual/merged_hf_discussions.csv\", index=False)\n",
    "\n",
    "merged_hf_sec = merged_hf[merged_hf[\"is_security\"] == 1]\n",
    "print(f\"{len(merged_hf_sec)=}\")\n",
    "merged_hf_sec.to_csv(\"./merged_after_manual/merged_hf_discussions_security.csv\", index=False)\n",
    "\n",
    "# count the distinct combination of model_id and num\n",
    "distinct_discussions = merged_hf[['model_id', 'num']].drop_duplicates().shape[0]\n",
    "print(f\"Distinct HF discussion: {distinct_discussions}\")\n",
    "\n",
    "distinct_discussions = merged_hf_sec[['model_id', 'num']].drop_duplicates().shape[0]\n",
    "print(f\"Distinct Security HF discussion: {distinct_discussions}\")\n",
    "# old\n",
    "# len(merged_hf)=666055\n",
    "# len(non_security_union)=130\n",
    "# len(security_union)=127\n",
    "# len(merged_hf)=2861\n",
    "# len(merged_hf_sec)=1634\n",
    "# Distinct HF discussion: 255\n",
    "# Distinct Security HF discussion: 127"
   ],
   "id": "cdc82a56c3256347"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# GH label reverse look up\n",
    "gh_discussion = pd.read_csv(\"./manual/gh_discussions_done.csv\")\n",
    "gh_comments = pd.read_csv(\"./manual/gh_comments_done.csv\")\n",
    "merged_gh = pd.read_csv(\"./merged/merged_gh_discussions.csv\")\n",
    "print(f\"{len(merged_gh)=}\")\n",
    "merged_gh.drop_duplicates(inplace=True)\n",
    "merged_gh[\"is_security\"] = 0\n",
    "\n",
    "df_non_security_disc = gh_discussion[gh_discussion[\"is_security\"] == 0][[\"repo_name\", \"discussion_number\"]]\n",
    "print(f\"{len(df_non_security_disc)=}\")\n",
    "df_non_security_cmt = gh_comments[gh_comments[\"is_security\"] == 0][[\"repo_name\", \"discussion_number\"]]\n",
    "print(f\"{len(df_non_security_cmt)=}\")\n",
    "non_security_union = pd.concat([df_non_security_disc, df_non_security_cmt]).drop_duplicates()\n",
    "print(f\"{len(non_security_union)=}\")\n",
    "\n",
    "df_security_disc = gh_discussion[gh_discussion[\"is_security\"] == 1][[\"repo_name\", \"discussion_number\"]]\n",
    "print(f\"{len(df_security_disc)=}\")\n",
    "df_security_cmt = gh_comments[gh_comments[\"is_security\"] == 1][[\"repo_name\", \"discussion_number\"]]\n",
    "print(f\"{len(df_security_cmt)=}\")\n",
    "security_union = pd.concat([df_security_disc, df_security_cmt]).drop_duplicates()\n",
    "# print(security_union)\n",
    "print(f\"{len(security_union)=}\")\n",
    "\n",
    "merged_gh = merged_gh.merge(\n",
    "\tpd.concat([non_security_union, security_union]).drop_duplicates(),\n",
    "\ton=[\"repo_name\", \"discussion_number\"],\n",
    "\thow=\"inner\"\n",
    ")\n",
    "print(f\"{len(merged_gh)=}\")\n",
    "\n",
    "merged_gh.loc[\n",
    "\tmerged_gh[[\"repo_name\", \"discussion_number\"]].apply(tuple, axis=1).isin(security_union.apply(tuple, axis=1)),\n",
    "\t\"is_security\"\n",
    "] = 1\n",
    "print(f\"{len(merged_gh)=}\")\n",
    "merged_gh.to_csv(\"./merged_after_manual/merged_gh_discussions.csv\", index=False)\n",
    "\n",
    "merged_gh_sec = merged_gh[merged_gh[\"is_security\"] == 1]\n",
    "print(f\"{len(merged_gh_sec)=}\")\n",
    "merged_gh_sec.to_csv(\"./merged_after_manual/merged_gh_discussions_security.csv\", index=False)\n",
    "\n",
    "# count the distinct combination of model_id and num\n",
    "distinct_discussions = merged_gh[['repo_name', 'discussion_number']].drop_duplicates().shape[0]\n",
    "print(f\"Distinct GH discussion: {distinct_discussions}\")\n",
    "\n",
    "distinct_discussions = merged_gh_sec[['repo_name', 'discussion_number']].drop_duplicates().shape[0]\n",
    "print(f\"Distinct Security HF discussion: {distinct_discussions}\")"
   ],
   "id": "9c169a9dc7fb36c9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# More manual check here on the before the final dataset",
   "id": "be05b2fde98805f8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# manual + data from external\n",
    "merged_gh = pd.read_csv(\"./merged_after_manual/merged_gh_discussions.csv\")\n",
    "merged_hf = pd.read_csv(\"./merged_after_manual/merged_hf_discussions.csv\")\n",
    "issues = pd.read_csv(\"./manual/gh_issues_subset_3_done.csv\")\n",
    "issues_external_sec = pd.read_csv(\"./external_issues/github_sec_issues.csv\", delimiter=\";\")\n",
    "issues_external_non_sec = pd.read_csv(\"./external_issues/github_nonsec_issues.csv\", delimiter=\";\")\n",
    "\n",
    "# repo_name,discussion_number,discussion_title,discussion_body,author_login_x,author_login_y,comment_body,is_security\n",
    "merged_columns = [\n",
    "\t\"id_name\", \"id_num\", \"type\", \"content\", \"is_security\"\n",
    "]\n",
    "\n",
    "# all_df = pd.DataFrame(columns=merged_columns)\n",
    "merged_gh[\"id_name\"] = merged_gh[\"repo_name\"]\n",
    "merged_gh[\"id_num\"] = merged_gh[\"discussion_number\"]\n",
    "merged_gh[\"content\"] = (\n",
    "\tmerged_gh[\"discussion_title\"].fillna(\"\").str.strip() + \" \" +\n",
    "\tmerged_gh[\"discussion_body\"].fillna(\"\").str.strip() + \" \" +\n",
    "\tmerged_gh[\"comment_body\"].fillna(\"\").str.strip()\n",
    ").str.strip()\n",
    "merged_gh[\"type\"] = \"GH_DISCUSSIONS\"\n",
    "\n",
    "# model_id,num,title,git_ref,url,event_id,event_type,content,is_security\n",
    "merged_hf[\"id_name\"] = merged_hf[\"model_id\"]\n",
    "merged_hf[\"id_num\"] = merged_hf[\"num\"]\n",
    "merged_hf[\"content\"] = (\n",
    "\tmerged_hf[\"title\"].fillna(\"\").str.strip() + \" \" +\n",
    "\tmerged_hf[\"content\"].fillna(\"\").str.strip()\n",
    ").str.strip()\n",
    "merged_hf[\"type\"] = \"HF_DISCUSSIONS\"\n",
    "\n",
    "# repo_name,issue_url,pr_from_issue,user_login,issue_number,keywords,url,issue_title,issue_body,is_security,security_category,keyword_count\n",
    "issues[\"id_name\"] = issues[\"repo_name\"]\n",
    "issues[\"id_num\"] = issues[\"issue_number\"]\n",
    "issues[\"content\"] = (\n",
    "\tissues[\"issue_title\"].fillna(\"\").str.strip() + \" \" +\n",
    "\tissues[\"issue_body\"].fillna(\"\").str.strip()\n",
    ")\n",
    "issues[\"type\"] = \"GH_ISSUES\"\n",
    "\n",
    "all_df = pd.concat(\n",
    "\t[\n",
    "\t\tmerged_gh[merged_columns],\n",
    "\t\tmerged_hf[merged_columns],\n",
    "\t\tissues[merged_columns]\n",
    "\t]\n",
    ")\n",
    "print(\"All manual records\", len(all_df))\n",
    "# shuffle\n",
    "all_df = all_df.sample(frac=1)\n",
    "all_df.to_csv(\"./merged_after_manual/merged_all.csv\", index=False)\n",
    "\n",
    "issues_external_sec[\"id_name\"] = issues_external_sec[\"repository\"]\n",
    "issues_external_sec[\"id_num\"] = issues_external_sec.apply(lambda x: int(str(x[\"issue_api_url\"]).strip().split(\"/\")[-1]),\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t  axis=1)\n",
    "issues_external_sec[\"content\"] = (\n",
    "\tissues_external_sec[\"issue_title\"].fillna(\"\").str.strip() + \" \" +\n",
    "\tissues_external_sec[\"description\"].fillna(\"\").str.strip()\n",
    ")\n",
    "issues_external_sec[\"type\"] = \"GH_ISSUES_EXTERNAL\"\n",
    "issues_external_sec[\"is_security\"] = 1\n",
    "\n",
    "issues_external_non_sec[\"id_name\"] = issues_external_non_sec[\"repository\"]\n",
    "issues_external_non_sec[\"id_num\"] = issues_external_non_sec.apply(\n",
    "\tlambda x: int(str(x[\"issue_api_url\"]).strip().split(\"/\")[-1]), axis=1)\n",
    "issues_external_non_sec[\"content\"] = (\n",
    "\tissues_external_non_sec[\"issue_title\"].fillna(\"\").str.strip() + \" \" +\n",
    "\tissues_external_non_sec[\"description\"].fillna(\"\").str.strip()\n",
    ")\n",
    "issues_external_non_sec[\"type\"] = \"GH_ISSUES_EXTERNAL\"\n",
    "issues_external_non_sec[\"is_security\"] = 0\n",
    "\n",
    "all_df = pd.concat(\n",
    "\t[\n",
    "\t\tall_df,\n",
    "\t\tissues_external_sec[merged_columns],\n",
    "\t\tissues_external_non_sec[merged_columns]\n",
    "\t]\n",
    ")\n",
    "print(\"Manual records + external issues\", len(all_df))\n",
    "# shuffle\n",
    "all_df = all_df.sample(frac=1)\n",
    "all_df.to_csv(\"./merged_after_manual/merged_all_with_external.csv\", index=False)"
   ],
   "id": "f603f79779599816"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "all_df",
   "id": "11e587aa1ed1c056"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "\n",
    "# test stratify\n",
    "train_df, temp_df = train_test_split(\n",
    "\tall_df,\n",
    "\ttest_size=0.2,\n",
    "\trandom_state=42,\n",
    "\tstratify=all_df[[\"type\", \"is_security\"]]\n",
    ")\n",
    "print(len(train_df[train_df[\"type\"] == \"GH_DISCUSSIONS\"]))\n",
    "print(len(temp_df[temp_df[\"type\"] == \"GH_DISCUSSIONS\"]))\n",
    "print(len(train_df[train_df[\"type\"] == \"GH_ISSUES\"]))\n",
    "print(len(temp_df[temp_df[\"type\"] == \"GH_ISSUES\"]))\n",
    "\n",
    "print(len(train_df[train_df[\"is_security\"] == 1]))\n",
    "print(len(temp_df[temp_df[\"is_security\"] == 1]))\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "for i, (train_index, test_index) in enumerate(skf.split(train_df, train_df[\"is_security\"])):\n",
    "\tprint(f\"Fold {i}:\")\n",
    "\tprint(f\"  Train: index={train_index}\")\n",
    "\tprint(f\"  Test:  index={test_index}\")\n"
   ],
   "id": "e306ad56b00afa6d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "c910251ebbd89021"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Multiple HF model can link to the same GH repo\n",
    "-> We consider all of these related to 1 AI project, for example:\n",
    "- meta-llama/Llama-2-7b\n",
    "- meta-llama/Llama-2-7b-chat-hf\n",
    "- meta-llama/Llama-2-70b-chat-hf\n",
    "- meta-llama/Llama-2-7b-hf\n",
    "- meta-llama/Llama-2-13b-chat-hf\n",
    "- meta-llama/Llama-2-70b-hf\n",
    "- meta-llama/Llama-2-13b-hf\n",
    "- meta-llama/Llama-2-7b-chat\n",
    "- meta-llama/Llama-2-70b\n",
    "- meta-llama/Llama-2-70b-chat\n",
    "- meta-llama/Llama-2-13b\n",
    "- meta-llama/Llama-2-13b-chat\n",
    "\n",
    "- All of these models link to http://github.com/facebookresearch/llama: -> belong to 1 project -> facebookresearch/llama."
   ],
   "id": "53c49dfbc149e5d5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# run model in classifier",
   "id": "a8fe8177e5876b15"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "note: https://www.oscs1024.com/ -> murphysec"
   ],
   "id": "27af9740e34fc4b0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df = pd.read_csv(\"prediction/backup_2402/merged_gh_discussions/raw_predictions.csv\")\n",
    "print(df[\"is_security_prediction\"].value_counts())\n",
    "df[df[\"is_security_prediction\"] == 1]"
   ],
   "id": "512ac08de8324c8b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T12:54:18.232627Z",
     "start_time": "2025-03-12T12:53:15.816993Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# New test set to verify the model performance\n",
    "# %load_ext cudf.pandas\n",
    "\n",
    "# all data\n",
    "merged_gh = pd.read_csv(\"./merged/merged_gh_discussions.csv\")\n",
    "merged_hf = pd.read_csv(\"./merged/merged_hf_discussions.csv\")\n",
    "issues = pd.read_csv(\"./filtered/gh_issues.csv\")\n",
    "print(f\"{len(merged_gh)=}, {len(merged_hf)=}, {len(issues)=}\")\n",
    "# repo_name,discussion_number,discussion_title,discussion_body,author_login_x,author_login_y,comment_body,is_security\n",
    "merged_columns = [\n",
    "\t\"id_name\", \"id_num\", \"type\", \"content\"\n",
    "]\n",
    "\n",
    "# all_df = pd.DataFrame(columns=merged_columns)\n",
    "merged_gh[\"id_name\"] = merged_gh[\"repo_name\"]\n",
    "merged_gh[\"id_num\"] = merged_gh[\"discussion_number\"]\n",
    "merged_gh[\"content\"] = (\n",
    "\tmerged_gh[\"discussion_title\"].fillna(\"\").str.strip() + \" \" +\n",
    "\tmerged_gh[\"discussion_body\"].fillna(\"\").str.strip() + \" \" +\n",
    "\tmerged_gh[\"comment_body\"].fillna(\"\").str.strip()\n",
    ").str.strip()\n",
    "merged_gh[\"type\"] = \"GH_DISCUSSIONS\"\n",
    "\n",
    "# model_id,num,title,git_ref,url,event_id,event_type,content,is_security\n",
    "merged_hf[\"id_name\"] = merged_hf[\"model_id\"]\n",
    "merged_hf[\"id_num\"] = merged_hf[\"num\"]\n",
    "merged_hf[\"content\"] = (\n",
    "\tmerged_hf[\"title\"].fillna(\"\").str.strip() + \" \" +\n",
    "\tmerged_hf[\"content\"].fillna(\"\").str.strip()\n",
    ").str.strip()\n",
    "merged_hf[\"type\"] = \"HF_DISCUSSIONS\"\n",
    "\n",
    "# repo_name,issue_url,pr_from_issue,user_login,issue_number,keywords,url,issue_title,issue_body,is_security,security_category,keyword_count\n",
    "issues[\"id_name\"] = issues[\"repo_name\"]\n",
    "issues[\"id_num\"] = issues[\"issue_number\"]\n",
    "issues[\"content\"] = (\n",
    "\tissues[\"issue_title\"].fillna(\"\").str.strip() + \" \" +\n",
    "\tissues[\"issue_body\"].fillna(\"\").str.strip()\n",
    ")\n",
    "issues[\"type\"] = \"GH_ISSUES\"\n",
    "\n",
    "all_df = pd.concat(\n",
    "\t[\n",
    "\t\tmerged_gh[merged_columns],\n",
    "\t\tmerged_hf[merged_columns],\n",
    "\t\tissues[merged_columns]\n",
    "\t]\n",
    ")\n",
    "print(\"All records\", len(all_df))\n",
    "# shuffle\n",
    "all_df = all_df.sample(frac=1)\n",
    "all_df.to_csv(\"./merged/merged_all.csv\", index=False)\n",
    "\n",
    "# filter for the following:\n",
    "# no manual label (also no keywords)\n",
    "col_ids = [\"id_name\", \"id_num\"]\n",
    "col_ids_str = \"_\".join(col_ids)\n",
    "all_df[col_ids_str] = all_df[col_ids[0]].astype(str) + \"_\" + all_df[col_ids[1]].astype(str)\n",
    "\n",
    "manual_merge_all = pd.read_csv(\"./merged_after_manual/merged_all.csv\")\n",
    "records_to_exclude = manual_merge_all[col_ids].drop_duplicates()\n",
    "records_to_exclude[col_ids_str] = (\n",
    "\trecords_to_exclude[col_ids[0]].astype(str) + \"_\" +\n",
    "\trecords_to_exclude[col_ids[1]].astype(str)\n",
    ")\n",
    "\n",
    "all_df_filtered = all_df[~all_df[col_ids_str].isin(records_to_exclude[col_ids_str])]\n",
    "print(\"All records no manual label\", len(all_df_filtered))\n",
    "# no hf safetensor and bot commit\n",
    "all_df_filtered = all_df_filtered[\n",
    "\t~all_df_filtered[\"content\"].str.startswith(\n",
    "\t\t(\"Adding `safetensors` variant of this model\", \"Upload folder using huggingface_hub\", \"Adding `diffusers` weights of this model\"), na=False\n",
    "\t)\n",
    "]\n",
    "print(\"All records no manual label and no safetensor\", len(all_df_filtered))\n",
    "\n",
    "# no license-related\n",
    "all_df_filtered = all_df_filtered[\n",
    "\t~all_df_filtered[\"content\"].str.contains(\n",
    "\t\t(\"license|licenses|License|Licenses\"), na=False, regex=True\n",
    "\t)\n",
    "]\n",
    "print(\"All records no manual label, no safetensor, no license\", len(all_df_filtered))\n",
    "\n",
    "# no keywords in gh issues (21k)\n",
    "issues_with_kws = pd.read_csv(\"./filtered_with_kw/gh_issues.csv\")\n",
    "print(\"GH Issues with keywords\", len(issues_with_kws))\n",
    "records_to_exclude = issues_with_kws[[\"repo_name\", \"issue_number\"]].drop_duplicates()\n",
    "records_to_exclude[col_ids_str] = (\n",
    "\trecords_to_exclude[\"repo_name\"].astype(str) + \"_\" +\n",
    "\trecords_to_exclude[\"issue_number\"].astype(str)\n",
    ")\n",
    "print(\"GH Issues with keywords distinct count \", len(records_to_exclude))\n",
    "\n",
    "all_df_filtered = all_df_filtered[~all_df_filtered[col_ids_str].isin(records_to_exclude[col_ids_str])]\n",
    "print(\"All records no manual label, no safetensor, no license, no keywords\", len(all_df_filtered))\n",
    "all_df_filtered[\"is_security\"] = -1"
   ],
   "id": "d2ae1b5d819d1e08",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(merged_gh)=47902, len(merged_hf)=666055, len(issues)=1994770\n",
      "All records 2708727\n",
      "All records no manual label 2703895\n",
      "All records no manual label and no safetensor 2416730\n",
      "All records no manual label, no safetensor, no license 2392552\n",
      "GH Issues with keywords 23755\n",
      "GH Issues with keywords distinct count  23750\n",
      "All records no manual label, no safetensor, no license, no keywords 2372791\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T12:54:18.716922Z",
     "start_time": "2025-03-12T12:54:18.283486Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# test sampling\n",
    "excluded_ids = [\n",
    "\t\"open-compass/opencompass\",\n",
    "\t\"alibaba-damo-academy/FunASR\",\n",
    "\t\"QwenLM/Qwen-7B\",\n",
    "\t\"BAAI/llm-embedder\",\n",
    "\t\"fnlp/moss-moon-003-sft-plugin-int4\",\n",
    "\t\"alimama-creative/FLUX.1-dev-Controlnet-Inpainting-Beta\",\n",
    "\t\"hiyouga/LLaMA-Factory\",\n",
    "\t\"OpenBMB/OmniLMM\",\n",
    "\t\"PaddlePaddle/PaddleOCR\",\n",
    "\t\"microsoft/vscode\",\n",
    "\t\"THUDM/ChatGLM-6B\",\n",
    "\t\"ymcui/Chinese-LLaMA-Alpaca-2\",\n",
    "\t\"netease-youdao/QAnything\",\n",
    "\t\"immersive-translate/immersive-translate\",\n",
    "\t\"ymcui/Chinese-LLaMA-Alpaca\",\n",
    "\t\"QwenLM/Qwen\",\n",
    "\t\"THUDM/GLM-4\",\n",
    "\t\"RVC-Boss/GPT-SoVITS\",\n",
    "\t\"viitor-ai/viitor-voice\",\n",
    "\t\"01-ai/Yi\",\n",
    "\t\"diffusers/controlnet-canny-sdxl-1.0\",\n",
    "\t\"shibing624/pycorrector\",\n",
    "\t\"baidu/Senta\",\n",
    "\t\"TuGraph-family/tugraph-db\",\n",
    "\t\"InternLM/InternLM\"\n",
    "]\n",
    "filtered_df = all_df_filtered.loc[~all_df_filtered['id_name'].isin(excluded_ids)]\n",
    "sampled_df = filtered_df.groupby(\"type\").apply(lambda x: x.sample(n=33, random_state=42)).reset_index(drop=True)\n",
    "sampled_df.to_csv(\"./test/test_99.csv\", index=False)"
   ],
   "id": "825a4b510d0b348e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_355116/364849533.py:29: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  sampled_df = filtered_df.groupby(\"type\").apply(lambda x: x.sample(n=33, random_state=42)).reset_index(drop=True)\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T12:56:09.706757Z",
     "start_time": "2025-03-12T12:56:09.693002Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_99 = pd.read_csv(\"./test/test_99.csv\")\n",
    "test_99_done = pd.read_csv(\"./test/test_99_done.csv\")\n",
    "test_99.sort_values(by=[\"id_name\", \"id_num\"], inplace=True)\n",
    "test_99.to_csv(\"./test/test_99.csv\", index=False)\n",
    "test_99_done.sort_values(by=[\"id_name\", \"id_num\"], inplace=True)\n",
    "test_99_done.to_csv(\"./test/test_99_done.csv\", index=False)"
   ],
   "id": "dc397253101f444",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Inference result",
   "id": "bbca94361032788"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df = pd.read_csv(\"inference/all_gh_distilbert/raw_predictions.csv\")\n",
    "df_security = df[df[\"is_security_prediction\"] == 1]\n",
    "print(len(df_security))\n",
    "df_security\n",
    "\n"
   ],
   "id": "bc36ef3cadd2a948"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def visualize_prob_sigmoid_distribution(file_path):\n",
    "\ttry:\n",
    "\t\tdf = pd.read_csv(file_path)\n",
    "\n",
    "\t\tif 'prob_sigmoid' not in df.columns:\n",
    "\t\t\tprint(f\"Error: 'prob_sigmoid' column not found in {file_path}\")\n",
    "\t\t\treturn\n",
    "\n",
    "\t\tif not all(0 <= x <= 1 for x in df['prob_sigmoid']):\n",
    "\t\t\tprint(\"Warning: data points found out of 0-1 range. will try to filter...\")\n",
    "\t\t\tdf = df[(df['prob_sigmoid'] >= 0) & (df['prob_sigmoid'] <= 1)]\n",
    "\t\t\tprint(f\"Filtered {sum(not (0 <= x <= 1) for x in df['prob_sigmoid'])} out of range datapoints.\")\n",
    "\n",
    "\t\tplt.figure(figsize=(10, 6))  # Adjust figure size as needed\n",
    "\t\tsns.histplot(df['prob_sigmoid'], kde=True, bins=30, color='skyblue')  #Histogram\n",
    "\t\tplt.title('Distribution of prob_sigmoid')\n",
    "\t\tplt.xlabel('prob_sigmoid')\n",
    "\t\tplt.ylabel('Frequency')\n",
    "\t\tplt.grid(axis='y', alpha=0.75)\n",
    "\n",
    "\t\tmean = df['prob_sigmoid'].mean()\n",
    "\t\tmedian = df['prob_sigmoid'].median()\n",
    "\t\tstd = df['prob_sigmoid'].std()\n",
    "\t\tplt.axvline(mean, color='red', linestyle='dashed', linewidth=1, label=f'Mean: {mean:.2f}')\n",
    "\t\tplt.axvline(median, color='green', linestyle='dashed', linewidth=1, label=f'Median: {median:.2f}')\n",
    "\t\tplt.legend()\n",
    "\n",
    "\t\tplt.tight_layout()\n",
    "\t\tplt.show()\n",
    "\n",
    "\texcept FileNotFoundError:\n",
    "\t\tprint(f\"Error: File not found at {file_path}\")\n",
    "\texcept pd.errors.EmptyDataError:\n",
    "\t\tprint(f\"Error: File {file_path} is empty.\")\n",
    "\texcept pd.errors.ParserError:\n",
    "\t\tprint(f\"Error: Could not parse {file_path}. Check file format.\")\n",
    "\texcept Exception as e:\n",
    "\t\tprint(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "\n",
    "file_path = 'inference/all_gh_bert_gh_last/raw_predictions.csv'\n",
    "visualize_prob_sigmoid_distribution(file_path)"
   ],
   "id": "e6e661cdf6c7fb7a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df = pd.read_csv(\"inference/all_hf_bert_hf_best/raw_predictions.csv\")\n",
    "df_security = df[df[\"is_security_prediction\"] == 1]\n",
    "print(len(df_security))\n",
    "\n",
    "file_path = 'inference/all_hf_bert_hf_best/raw_predictions.csv'\n",
    "visualize_prob_sigmoid_distribution(file_path)\n"
   ],
   "id": "1d607629e0f4b210"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T00:55:19.523324Z",
     "start_time": "2025-03-13T00:55:19.488783Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "\n",
    "# prediction result collection\n",
    "def collect_prediction(path: str, exclude: list[str]):\n",
    "\tmodels = [\"bert_base\", \"distilbert\", \"securebert\", \"roberta_base\", \"secbert\", \"secroberta\", ]\n",
    "\tall_metrics_data = []\n",
    "\n",
    "\tsubfolders = [\n",
    "\t\tf.path\n",
    "\t\tfor f in os.scandir(path)\n",
    "\t\tif f.is_dir() and f.name not in exclude\n",
    "\t]\n",
    "\n",
    "\tfor subfolder in subfolders:\n",
    "\t\tmetrics_file_path = os.path.join(subfolder, \"metrics.csv\")\n",
    "\t\tif not os.path.exists(metrics_file_path):\n",
    "\t\t\tcontinue\n",
    "\t\tdf = pd.read_csv(metrics_file_path)\n",
    "\t\t# only get the test result\n",
    "\t\tdf = df.tail(1)\n",
    "\t\tmodel_type = None\n",
    "\t\tdata_type = None\n",
    "\n",
    "\t\tmatch = re.search(r\"_({})\".format(\"|\".join(models)), os.path.basename(subfolder))\n",
    "\t\tif match:\n",
    "\t\t\tmodel_type = str(os.path.basename(subfolder)[match.start():]).replace(\"_\", \"\", 1)\n",
    "\t\t\tdata_type = os.path.basename(subfolder)[:match.start()]\n",
    "\n",
    "\t\tdf[\"input\"] = data_type\n",
    "\t\tdf[\"model_type\"] = model_type\n",
    "\t\tdf[\"subfolder\"] = os.path.basename(subfolder)\n",
    "\t\tdf[\"folder\"] = path\n",
    "\t\tall_metrics_data.append(df)\n",
    "\n",
    "\treturn pd.concat(all_metrics_data, ignore_index=True)\n",
    "\n",
    "\n",
    "path = \"./prediction\"\n",
    "exclude = [\"backup_2402\"]\n",
    "metrics_predictions = collect_prediction(path, exclude)\n",
    "metrics_predictions"
   ],
   "id": "a3e5b17bfb68f25f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "    fold  epoch  train_loss  eval_loss  test_loss  accuracy  precision  \\\n",
       "0    NaN    NaN         NaN        NaN   0.361670  0.950911   0.960799   \n",
       "1    NaN    NaN         NaN        NaN   0.352707  0.975000   0.931818   \n",
       "2    NaN    NaN         NaN        NaN   0.180531  0.973031   0.930328   \n",
       "3    NaN    NaN         NaN        NaN   0.494877  0.948718   0.947826   \n",
       "4    NaN    NaN         NaN        NaN   0.382849  0.958688   0.948107   \n",
       "5    NaN    NaN         NaN        NaN   0.166664  0.986792   0.939759   \n",
       "6    NaN    NaN         NaN        NaN   0.937315  0.888889   0.980000   \n",
       "7    NaN    NaN         NaN        NaN   0.254390  0.967638   0.939914   \n",
       "8    NaN    NaN         NaN        NaN   0.370002  0.948238   0.910299   \n",
       "9    NaN    NaN         NaN        NaN   0.133817  0.978571   0.897959   \n",
       "10   NaN    NaN         NaN        NaN   0.208375  0.981132   0.906977   \n",
       "11   4.0    1.0    0.005047   0.000421        NaN  1.000000   1.000000   \n",
       "12   NaN    NaN         NaN        NaN   0.382160  0.984906   0.950000   \n",
       "13   NaN    NaN         NaN        NaN   0.150151  0.971429   0.893617   \n",
       "14   NaN    NaN         NaN        NaN   0.396217  0.947509   0.932584   \n",
       "15   NaN    NaN         NaN        NaN   0.383204  0.965812   0.964602   \n",
       "16   NaN    NaN         NaN        NaN   0.372478  0.948718   0.972477   \n",
       "17   NaN    NaN         NaN        NaN   0.220628  0.983019   0.938272   \n",
       "18   NaN    NaN         NaN        NaN   0.295563  0.973031   0.960526   \n",
       "19   NaN    NaN         NaN        NaN   0.151344  0.960714   0.904762   \n",
       "20   NaN    NaN         NaN        NaN   0.353284  0.965480   0.963470   \n",
       "21   NaN    NaN         NaN        NaN   0.223303  0.975000   0.931818   \n",
       "22   NaN    NaN         NaN        NaN   0.452731  0.965812   0.964602   \n",
       "23   NaN    NaN         NaN        NaN   0.317121  0.988679   0.962500   \n",
       "24   NaN    NaN         NaN        NaN   0.301873  0.965812   0.972973   \n",
       "25   NaN    NaN         NaN        NaN   0.267395  0.988679   0.974359   \n",
       "26   NaN    NaN         NaN        NaN   0.111090  0.982143   0.900000   \n",
       "27   NaN    NaN         NaN        NaN   0.198852  0.977346   0.965217   \n",
       "28   NaN    NaN         NaN        NaN   0.430154  0.948238   0.935780   \n",
       "29   NaN    NaN         NaN        NaN   0.183172  0.976268   0.957082   \n",
       "\n",
       "      recall        f1  f1_macro       mcc  elapsed_time            input  \\\n",
       "0   0.897099  0.927857  0.945328  0.891926  15753.684496     all_external   \n",
       "1   0.911111  0.921348  0.953243  0.906569    518.330959        manual_gh   \n",
       "2   0.965957  0.947808  0.964813  0.929933   3380.589169              all   \n",
       "3   1.000000  0.973214  0.686607  0.486782    223.580802  manual_gh_issue   \n",
       "4   0.933702  0.940849  0.954554  0.909175   8007.931883     all_external   \n",
       "5   0.975000  0.957055  0.974626  0.949479   1971.510443        manual_hf   \n",
       "6   0.899083  0.937799  0.708900  0.464872    237.166367  manual_gh_issue   \n",
       "7   0.931915  0.935897  0.957126  0.914268   3458.170598              all   \n",
       "8   0.946133  0.927870  0.943753  0.887932  16331.838002     all_external   \n",
       "9   0.977778  0.936170  0.961647  0.924531   1042.465266        manual_gh   \n",
       "10  0.975000  0.939759  0.964287  0.929444    973.460714        manual_hf   \n",
       "11  1.000000  1.000000  1.000000  1.000000   6284.000524     all_external   \n",
       "12  0.950000  0.950000  0.970556  0.941111    972.133759        manual_hf   \n",
       "13  0.933333  0.913043  0.947975  0.896262    584.782611        manual_gh   \n",
       "14  0.917127  0.924791  0.942239  0.884555   7599.123032     all_external   \n",
       "15  1.000000  0.981982  0.824324  0.694479    210.662634  manual_gh_issue   \n",
       "16  0.972477  0.972477  0.798739  0.597477    427.888581  manual_gh_issue   \n",
       "17  0.950000  0.944099  0.967044  0.934114   2014.018980        manual_hf   \n",
       "18  0.931915  0.946004  0.964016  0.928226   1750.049126              all   \n",
       "19  0.844444  0.873563  0.925154  0.851049    559.224996        manual_gh   \n",
       "20  0.897872  0.929515  0.953329  0.907684   1746.297576              all   \n",
       "21  0.911111  0.921348  0.953243  0.906569   1010.865430        manual_gh   \n",
       "22  1.000000  0.981982  0.824324  0.694479    434.641863  manual_gh_issue   \n",
       "23  0.962500  0.962500  0.977917  0.955833    954.863716        manual_hf   \n",
       "24  0.990826  0.981818  0.848052  0.704659    396.990788  manual_gh_issue   \n",
       "25  0.950000  0.962025  0.977687  0.955480   1846.634074        manual_hf   \n",
       "26  1.000000  0.947368  0.968308  0.938537    988.678939        manual_gh   \n",
       "27  0.944681  0.954839  0.969860  0.939819   1727.326307              all   \n",
       "28  0.915746  0.925654  0.942976  0.886082   7756.670320     all_external   \n",
       "29  0.948936  0.952991  0.968559  0.937134   3587.974625              all   \n",
       "\n",
       "      model_type                     subfolder        folder  \n",
       "0     securebert       all_external_securebert  ./prediction  \n",
       "1     distilbert          manual_gh_distilbert  ./prediction  \n",
       "2      bert_base                 all_bert_base  ./prediction  \n",
       "3     distilbert    manual_gh_issue_distilbert  ./prediction  \n",
       "4     distilbert       all_external_distilbert  ./prediction  \n",
       "5   roberta_base        manual_hf_roberta_base  ./prediction  \n",
       "6     secroberta    manual_gh_issue_secroberta  ./prediction  \n",
       "7     securebert                all_securebert  ./prediction  \n",
       "8      bert_base        all_external_bert_base  ./prediction  \n",
       "9   roberta_base        manual_gh_roberta_base  ./prediction  \n",
       "10    distilbert          manual_hf_distilbert  ./prediction  \n",
       "11  roberta_base     all_external_roberta_base  ./prediction  \n",
       "12    secroberta          manual_hf_secroberta  ./prediction  \n",
       "13       secbert             manual_gh_secbert  ./prediction  \n",
       "14       secbert          all_external_secbert  ./prediction  \n",
       "15       secbert       manual_gh_issue_secbert  ./prediction  \n",
       "16     bert_base     manual_gh_issue_bert_base  ./prediction  \n",
       "17    securebert          manual_hf_securebert  ./prediction  \n",
       "18    secroberta                all_secroberta  ./prediction  \n",
       "19    secroberta          manual_gh_secroberta  ./prediction  \n",
       "20       secbert                   all_secbert  ./prediction  \n",
       "21     bert_base           manual_gh_bert_base  ./prediction  \n",
       "22  roberta_base  manual_gh_issue_roberta_base  ./prediction  \n",
       "23       secbert             manual_hf_secbert  ./prediction  \n",
       "24    securebert    manual_gh_issue_securebert  ./prediction  \n",
       "25     bert_base           manual_hf_bert_base  ./prediction  \n",
       "26    securebert          manual_gh_securebert  ./prediction  \n",
       "27    distilbert                all_distilbert  ./prediction  \n",
       "28    secroberta       all_external_secroberta  ./prediction  \n",
       "29  roberta_base              all_roberta_base  ./prediction  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fold</th>\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>eval_loss</th>\n",
       "      <th>test_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>f1_macro</th>\n",
       "      <th>mcc</th>\n",
       "      <th>elapsed_time</th>\n",
       "      <th>input</th>\n",
       "      <th>model_type</th>\n",
       "      <th>subfolder</th>\n",
       "      <th>folder</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.361670</td>\n",
       "      <td>0.950911</td>\n",
       "      <td>0.960799</td>\n",
       "      <td>0.897099</td>\n",
       "      <td>0.927857</td>\n",
       "      <td>0.945328</td>\n",
       "      <td>0.891926</td>\n",
       "      <td>15753.684496</td>\n",
       "      <td>all_external</td>\n",
       "      <td>securebert</td>\n",
       "      <td>all_external_securebert</td>\n",
       "      <td>./prediction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.352707</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.931818</td>\n",
       "      <td>0.911111</td>\n",
       "      <td>0.921348</td>\n",
       "      <td>0.953243</td>\n",
       "      <td>0.906569</td>\n",
       "      <td>518.330959</td>\n",
       "      <td>manual_gh</td>\n",
       "      <td>distilbert</td>\n",
       "      <td>manual_gh_distilbert</td>\n",
       "      <td>./prediction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.180531</td>\n",
       "      <td>0.973031</td>\n",
       "      <td>0.930328</td>\n",
       "      <td>0.965957</td>\n",
       "      <td>0.947808</td>\n",
       "      <td>0.964813</td>\n",
       "      <td>0.929933</td>\n",
       "      <td>3380.589169</td>\n",
       "      <td>all</td>\n",
       "      <td>bert_base</td>\n",
       "      <td>all_bert_base</td>\n",
       "      <td>./prediction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.494877</td>\n",
       "      <td>0.948718</td>\n",
       "      <td>0.947826</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.973214</td>\n",
       "      <td>0.686607</td>\n",
       "      <td>0.486782</td>\n",
       "      <td>223.580802</td>\n",
       "      <td>manual_gh_issue</td>\n",
       "      <td>distilbert</td>\n",
       "      <td>manual_gh_issue_distilbert</td>\n",
       "      <td>./prediction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.382849</td>\n",
       "      <td>0.958688</td>\n",
       "      <td>0.948107</td>\n",
       "      <td>0.933702</td>\n",
       "      <td>0.940849</td>\n",
       "      <td>0.954554</td>\n",
       "      <td>0.909175</td>\n",
       "      <td>8007.931883</td>\n",
       "      <td>all_external</td>\n",
       "      <td>distilbert</td>\n",
       "      <td>all_external_distilbert</td>\n",
       "      <td>./prediction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.166664</td>\n",
       "      <td>0.986792</td>\n",
       "      <td>0.939759</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.957055</td>\n",
       "      <td>0.974626</td>\n",
       "      <td>0.949479</td>\n",
       "      <td>1971.510443</td>\n",
       "      <td>manual_hf</td>\n",
       "      <td>roberta_base</td>\n",
       "      <td>manual_hf_roberta_base</td>\n",
       "      <td>./prediction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.937315</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.980000</td>\n",
       "      <td>0.899083</td>\n",
       "      <td>0.937799</td>\n",
       "      <td>0.708900</td>\n",
       "      <td>0.464872</td>\n",
       "      <td>237.166367</td>\n",
       "      <td>manual_gh_issue</td>\n",
       "      <td>secroberta</td>\n",
       "      <td>manual_gh_issue_secroberta</td>\n",
       "      <td>./prediction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.254390</td>\n",
       "      <td>0.967638</td>\n",
       "      <td>0.939914</td>\n",
       "      <td>0.931915</td>\n",
       "      <td>0.935897</td>\n",
       "      <td>0.957126</td>\n",
       "      <td>0.914268</td>\n",
       "      <td>3458.170598</td>\n",
       "      <td>all</td>\n",
       "      <td>securebert</td>\n",
       "      <td>all_securebert</td>\n",
       "      <td>./prediction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.370002</td>\n",
       "      <td>0.948238</td>\n",
       "      <td>0.910299</td>\n",
       "      <td>0.946133</td>\n",
       "      <td>0.927870</td>\n",
       "      <td>0.943753</td>\n",
       "      <td>0.887932</td>\n",
       "      <td>16331.838002</td>\n",
       "      <td>all_external</td>\n",
       "      <td>bert_base</td>\n",
       "      <td>all_external_bert_base</td>\n",
       "      <td>./prediction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.133817</td>\n",
       "      <td>0.978571</td>\n",
       "      <td>0.897959</td>\n",
       "      <td>0.977778</td>\n",
       "      <td>0.936170</td>\n",
       "      <td>0.961647</td>\n",
       "      <td>0.924531</td>\n",
       "      <td>1042.465266</td>\n",
       "      <td>manual_gh</td>\n",
       "      <td>roberta_base</td>\n",
       "      <td>manual_gh_roberta_base</td>\n",
       "      <td>./prediction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.208375</td>\n",
       "      <td>0.981132</td>\n",
       "      <td>0.906977</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.939759</td>\n",
       "      <td>0.964287</td>\n",
       "      <td>0.929444</td>\n",
       "      <td>973.460714</td>\n",
       "      <td>manual_hf</td>\n",
       "      <td>distilbert</td>\n",
       "      <td>manual_hf_distilbert</td>\n",
       "      <td>./prediction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.005047</td>\n",
       "      <td>0.000421</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6284.000524</td>\n",
       "      <td>all_external</td>\n",
       "      <td>roberta_base</td>\n",
       "      <td>all_external_roberta_base</td>\n",
       "      <td>./prediction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.382160</td>\n",
       "      <td>0.984906</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.970556</td>\n",
       "      <td>0.941111</td>\n",
       "      <td>972.133759</td>\n",
       "      <td>manual_hf</td>\n",
       "      <td>secroberta</td>\n",
       "      <td>manual_hf_secroberta</td>\n",
       "      <td>./prediction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.150151</td>\n",
       "      <td>0.971429</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.913043</td>\n",
       "      <td>0.947975</td>\n",
       "      <td>0.896262</td>\n",
       "      <td>584.782611</td>\n",
       "      <td>manual_gh</td>\n",
       "      <td>secbert</td>\n",
       "      <td>manual_gh_secbert</td>\n",
       "      <td>./prediction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.396217</td>\n",
       "      <td>0.947509</td>\n",
       "      <td>0.932584</td>\n",
       "      <td>0.917127</td>\n",
       "      <td>0.924791</td>\n",
       "      <td>0.942239</td>\n",
       "      <td>0.884555</td>\n",
       "      <td>7599.123032</td>\n",
       "      <td>all_external</td>\n",
       "      <td>secbert</td>\n",
       "      <td>all_external_secbert</td>\n",
       "      <td>./prediction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.383204</td>\n",
       "      <td>0.965812</td>\n",
       "      <td>0.964602</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.981982</td>\n",
       "      <td>0.824324</td>\n",
       "      <td>0.694479</td>\n",
       "      <td>210.662634</td>\n",
       "      <td>manual_gh_issue</td>\n",
       "      <td>secbert</td>\n",
       "      <td>manual_gh_issue_secbert</td>\n",
       "      <td>./prediction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.372478</td>\n",
       "      <td>0.948718</td>\n",
       "      <td>0.972477</td>\n",
       "      <td>0.972477</td>\n",
       "      <td>0.972477</td>\n",
       "      <td>0.798739</td>\n",
       "      <td>0.597477</td>\n",
       "      <td>427.888581</td>\n",
       "      <td>manual_gh_issue</td>\n",
       "      <td>bert_base</td>\n",
       "      <td>manual_gh_issue_bert_base</td>\n",
       "      <td>./prediction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.220628</td>\n",
       "      <td>0.983019</td>\n",
       "      <td>0.938272</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.944099</td>\n",
       "      <td>0.967044</td>\n",
       "      <td>0.934114</td>\n",
       "      <td>2014.018980</td>\n",
       "      <td>manual_hf</td>\n",
       "      <td>securebert</td>\n",
       "      <td>manual_hf_securebert</td>\n",
       "      <td>./prediction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.295563</td>\n",
       "      <td>0.973031</td>\n",
       "      <td>0.960526</td>\n",
       "      <td>0.931915</td>\n",
       "      <td>0.946004</td>\n",
       "      <td>0.964016</td>\n",
       "      <td>0.928226</td>\n",
       "      <td>1750.049126</td>\n",
       "      <td>all</td>\n",
       "      <td>secroberta</td>\n",
       "      <td>all_secroberta</td>\n",
       "      <td>./prediction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.151344</td>\n",
       "      <td>0.960714</td>\n",
       "      <td>0.904762</td>\n",
       "      <td>0.844444</td>\n",
       "      <td>0.873563</td>\n",
       "      <td>0.925154</td>\n",
       "      <td>0.851049</td>\n",
       "      <td>559.224996</td>\n",
       "      <td>manual_gh</td>\n",
       "      <td>secroberta</td>\n",
       "      <td>manual_gh_secroberta</td>\n",
       "      <td>./prediction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.353284</td>\n",
       "      <td>0.965480</td>\n",
       "      <td>0.963470</td>\n",
       "      <td>0.897872</td>\n",
       "      <td>0.929515</td>\n",
       "      <td>0.953329</td>\n",
       "      <td>0.907684</td>\n",
       "      <td>1746.297576</td>\n",
       "      <td>all</td>\n",
       "      <td>secbert</td>\n",
       "      <td>all_secbert</td>\n",
       "      <td>./prediction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.223303</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.931818</td>\n",
       "      <td>0.911111</td>\n",
       "      <td>0.921348</td>\n",
       "      <td>0.953243</td>\n",
       "      <td>0.906569</td>\n",
       "      <td>1010.865430</td>\n",
       "      <td>manual_gh</td>\n",
       "      <td>bert_base</td>\n",
       "      <td>manual_gh_bert_base</td>\n",
       "      <td>./prediction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.452731</td>\n",
       "      <td>0.965812</td>\n",
       "      <td>0.964602</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.981982</td>\n",
       "      <td>0.824324</td>\n",
       "      <td>0.694479</td>\n",
       "      <td>434.641863</td>\n",
       "      <td>manual_gh_issue</td>\n",
       "      <td>roberta_base</td>\n",
       "      <td>manual_gh_issue_roberta_base</td>\n",
       "      <td>./prediction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.317121</td>\n",
       "      <td>0.988679</td>\n",
       "      <td>0.962500</td>\n",
       "      <td>0.962500</td>\n",
       "      <td>0.962500</td>\n",
       "      <td>0.977917</td>\n",
       "      <td>0.955833</td>\n",
       "      <td>954.863716</td>\n",
       "      <td>manual_hf</td>\n",
       "      <td>secbert</td>\n",
       "      <td>manual_hf_secbert</td>\n",
       "      <td>./prediction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.301873</td>\n",
       "      <td>0.965812</td>\n",
       "      <td>0.972973</td>\n",
       "      <td>0.990826</td>\n",
       "      <td>0.981818</td>\n",
       "      <td>0.848052</td>\n",
       "      <td>0.704659</td>\n",
       "      <td>396.990788</td>\n",
       "      <td>manual_gh_issue</td>\n",
       "      <td>securebert</td>\n",
       "      <td>manual_gh_issue_securebert</td>\n",
       "      <td>./prediction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.267395</td>\n",
       "      <td>0.988679</td>\n",
       "      <td>0.974359</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.962025</td>\n",
       "      <td>0.977687</td>\n",
       "      <td>0.955480</td>\n",
       "      <td>1846.634074</td>\n",
       "      <td>manual_hf</td>\n",
       "      <td>bert_base</td>\n",
       "      <td>manual_hf_bert_base</td>\n",
       "      <td>./prediction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.111090</td>\n",
       "      <td>0.982143</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.968308</td>\n",
       "      <td>0.938537</td>\n",
       "      <td>988.678939</td>\n",
       "      <td>manual_gh</td>\n",
       "      <td>securebert</td>\n",
       "      <td>manual_gh_securebert</td>\n",
       "      <td>./prediction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.198852</td>\n",
       "      <td>0.977346</td>\n",
       "      <td>0.965217</td>\n",
       "      <td>0.944681</td>\n",
       "      <td>0.954839</td>\n",
       "      <td>0.969860</td>\n",
       "      <td>0.939819</td>\n",
       "      <td>1727.326307</td>\n",
       "      <td>all</td>\n",
       "      <td>distilbert</td>\n",
       "      <td>all_distilbert</td>\n",
       "      <td>./prediction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.430154</td>\n",
       "      <td>0.948238</td>\n",
       "      <td>0.935780</td>\n",
       "      <td>0.915746</td>\n",
       "      <td>0.925654</td>\n",
       "      <td>0.942976</td>\n",
       "      <td>0.886082</td>\n",
       "      <td>7756.670320</td>\n",
       "      <td>all_external</td>\n",
       "      <td>secroberta</td>\n",
       "      <td>all_external_secroberta</td>\n",
       "      <td>./prediction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.183172</td>\n",
       "      <td>0.976268</td>\n",
       "      <td>0.957082</td>\n",
       "      <td>0.948936</td>\n",
       "      <td>0.952991</td>\n",
       "      <td>0.968559</td>\n",
       "      <td>0.937134</td>\n",
       "      <td>3587.974625</td>\n",
       "      <td>all</td>\n",
       "      <td>roberta_base</td>\n",
       "      <td>all_roberta_base</td>\n",
       "      <td>./prediction</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "path = \"./prediction_0403\"\n",
    "collect_prediction(path, exclude)"
   ],
   "id": "c149052c963ae270"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T00:55:23.033497Z",
     "start_time": "2025-03-13T00:55:22.998769Z"
    }
   },
   "cell_type": "code",
   "source": [
    "path = \"./prediction_tuned\"\n",
    "collect_prediction(path, exclude)"
   ],
   "id": "81b91e036e5d4d8f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "    f1_macro       mcc  accuracy  precision    recall        f1  fold  epoch  \\\n",
       "0   0.977271  0.954541  0.982759   0.966102  0.966102  0.966102   NaN    NaN   \n",
       "1   0.973111  0.946319  0.979504   0.950000  0.970213  0.960000   NaN    NaN   \n",
       "2   0.965798  0.931595  0.974110   0.948936  0.948936  0.948936   NaN    NaN   \n",
       "3   0.983140  0.966527  0.987069   0.959016  0.991525  0.975000   NaN    NaN   \n",
       "4   0.971603  0.943380  0.978365   0.944444  0.971429  0.957746  10.0    3.0   \n",
       "5   0.966989  0.934078  0.975189   0.960870  0.940426  0.950538   NaN    NaN   \n",
       "6   0.428571  0.000000  0.750000   0.000000  0.000000  0.000000   NaN    NaN   \n",
       "7   0.427160  0.000000  0.745690   0.000000  0.000000  0.000000   NaN    NaN   \n",
       "8   0.965906  0.931812  0.974138   0.949153  0.949153  0.949153   NaN    NaN   \n",
       "9   0.964119  0.928336  0.973031   0.956522  0.936170  0.946237   NaN    NaN   \n",
       "10  0.971428  0.942919  0.978448   0.965517  0.949153  0.957265   NaN    NaN   \n",
       "11  0.965701  0.931418  0.974110   0.952790  0.944681  0.948718   NaN    NaN   \n",
       "12  0.966895  0.933984  0.975189   0.964912  0.936170  0.950324   NaN    NaN   \n",
       "\n",
       "    train_loss  eval_loss  test_loss  elapsed_time        input    model_type  \\\n",
       "0          NaN        NaN   0.125889  70921.045327  all_9_train     bert_base   \n",
       "1          NaN        NaN   0.100091  25556.112959          all     bert_base   \n",
       "2          NaN        NaN   0.075606  17573.265170          all    securebert   \n",
       "3          NaN        NaN   0.047132  49292.643998  all_9_train       secbert   \n",
       "4     0.012996   0.083561        NaN  40273.369340  all_9_train    secroberta   \n",
       "5          NaN        NaN   0.097227   9693.202135          all    secroberta   \n",
       "6          NaN        NaN   0.475672     61.062290         None          None   \n",
       "7          NaN        NaN   0.567494  44149.730483  all_9_train    securebert   \n",
       "8          NaN        NaN   0.113657  75160.765860  all_9_train  roberta_base   \n",
       "9          NaN        NaN   0.102124  13538.628879          all       secbert   \n",
       "10         NaN        NaN   0.081600  32376.317811  all_9_train    distilbert   \n",
       "11         NaN        NaN   0.122056   8128.326666          all    distilbert   \n",
       "12         NaN        NaN   0.076767  17562.904923          all  roberta_base   \n",
       "\n",
       "                   subfolder              folder  \n",
       "0      all_9_train_bert_base  ./prediction_tuned  \n",
       "1              all_bert_base  ./prediction_tuned  \n",
       "2             all_securebert  ./prediction_tuned  \n",
       "3        all_9_train_secbert  ./prediction_tuned  \n",
       "4     all_9_train_secroberta  ./prediction_tuned  \n",
       "5             all_secroberta  ./prediction_tuned  \n",
       "6                     sample  ./prediction_tuned  \n",
       "7     all_9_train_securebert  ./prediction_tuned  \n",
       "8   all_9_train_roberta_base  ./prediction_tuned  \n",
       "9                all_secbert  ./prediction_tuned  \n",
       "10    all_9_train_distilbert  ./prediction_tuned  \n",
       "11            all_distilbert  ./prediction_tuned  \n",
       "12          all_roberta_base  ./prediction_tuned  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1_macro</th>\n",
       "      <th>mcc</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>fold</th>\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>eval_loss</th>\n",
       "      <th>test_loss</th>\n",
       "      <th>elapsed_time</th>\n",
       "      <th>input</th>\n",
       "      <th>model_type</th>\n",
       "      <th>subfolder</th>\n",
       "      <th>folder</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.977271</td>\n",
       "      <td>0.954541</td>\n",
       "      <td>0.982759</td>\n",
       "      <td>0.966102</td>\n",
       "      <td>0.966102</td>\n",
       "      <td>0.966102</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.125889</td>\n",
       "      <td>70921.045327</td>\n",
       "      <td>all_9_train</td>\n",
       "      <td>bert_base</td>\n",
       "      <td>all_9_train_bert_base</td>\n",
       "      <td>./prediction_tuned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.973111</td>\n",
       "      <td>0.946319</td>\n",
       "      <td>0.979504</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.970213</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.100091</td>\n",
       "      <td>25556.112959</td>\n",
       "      <td>all</td>\n",
       "      <td>bert_base</td>\n",
       "      <td>all_bert_base</td>\n",
       "      <td>./prediction_tuned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.965798</td>\n",
       "      <td>0.931595</td>\n",
       "      <td>0.974110</td>\n",
       "      <td>0.948936</td>\n",
       "      <td>0.948936</td>\n",
       "      <td>0.948936</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.075606</td>\n",
       "      <td>17573.265170</td>\n",
       "      <td>all</td>\n",
       "      <td>securebert</td>\n",
       "      <td>all_securebert</td>\n",
       "      <td>./prediction_tuned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.983140</td>\n",
       "      <td>0.966527</td>\n",
       "      <td>0.987069</td>\n",
       "      <td>0.959016</td>\n",
       "      <td>0.991525</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.047132</td>\n",
       "      <td>49292.643998</td>\n",
       "      <td>all_9_train</td>\n",
       "      <td>secbert</td>\n",
       "      <td>all_9_train_secbert</td>\n",
       "      <td>./prediction_tuned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.971603</td>\n",
       "      <td>0.943380</td>\n",
       "      <td>0.978365</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>0.971429</td>\n",
       "      <td>0.957746</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.012996</td>\n",
       "      <td>0.083561</td>\n",
       "      <td>NaN</td>\n",
       "      <td>40273.369340</td>\n",
       "      <td>all_9_train</td>\n",
       "      <td>secroberta</td>\n",
       "      <td>all_9_train_secroberta</td>\n",
       "      <td>./prediction_tuned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.966989</td>\n",
       "      <td>0.934078</td>\n",
       "      <td>0.975189</td>\n",
       "      <td>0.960870</td>\n",
       "      <td>0.940426</td>\n",
       "      <td>0.950538</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.097227</td>\n",
       "      <td>9693.202135</td>\n",
       "      <td>all</td>\n",
       "      <td>secroberta</td>\n",
       "      <td>all_secroberta</td>\n",
       "      <td>./prediction_tuned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.475672</td>\n",
       "      <td>61.062290</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>sample</td>\n",
       "      <td>./prediction_tuned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.427160</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.745690</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.567494</td>\n",
       "      <td>44149.730483</td>\n",
       "      <td>all_9_train</td>\n",
       "      <td>securebert</td>\n",
       "      <td>all_9_train_securebert</td>\n",
       "      <td>./prediction_tuned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.965906</td>\n",
       "      <td>0.931812</td>\n",
       "      <td>0.974138</td>\n",
       "      <td>0.949153</td>\n",
       "      <td>0.949153</td>\n",
       "      <td>0.949153</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.113657</td>\n",
       "      <td>75160.765860</td>\n",
       "      <td>all_9_train</td>\n",
       "      <td>roberta_base</td>\n",
       "      <td>all_9_train_roberta_base</td>\n",
       "      <td>./prediction_tuned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.964119</td>\n",
       "      <td>0.928336</td>\n",
       "      <td>0.973031</td>\n",
       "      <td>0.956522</td>\n",
       "      <td>0.936170</td>\n",
       "      <td>0.946237</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.102124</td>\n",
       "      <td>13538.628879</td>\n",
       "      <td>all</td>\n",
       "      <td>secbert</td>\n",
       "      <td>all_secbert</td>\n",
       "      <td>./prediction_tuned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.971428</td>\n",
       "      <td>0.942919</td>\n",
       "      <td>0.978448</td>\n",
       "      <td>0.965517</td>\n",
       "      <td>0.949153</td>\n",
       "      <td>0.957265</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.081600</td>\n",
       "      <td>32376.317811</td>\n",
       "      <td>all_9_train</td>\n",
       "      <td>distilbert</td>\n",
       "      <td>all_9_train_distilbert</td>\n",
       "      <td>./prediction_tuned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.965701</td>\n",
       "      <td>0.931418</td>\n",
       "      <td>0.974110</td>\n",
       "      <td>0.952790</td>\n",
       "      <td>0.944681</td>\n",
       "      <td>0.948718</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.122056</td>\n",
       "      <td>8128.326666</td>\n",
       "      <td>all</td>\n",
       "      <td>distilbert</td>\n",
       "      <td>all_distilbert</td>\n",
       "      <td>./prediction_tuned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.966895</td>\n",
       "      <td>0.933984</td>\n",
       "      <td>0.975189</td>\n",
       "      <td>0.964912</td>\n",
       "      <td>0.936170</td>\n",
       "      <td>0.950324</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.076767</td>\n",
       "      <td>17562.904923</td>\n",
       "      <td>all</td>\n",
       "      <td>roberta_base</td>\n",
       "      <td>all_roberta_base</td>\n",
       "      <td>./prediction_tuned</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T05:19:44.283416Z",
     "start_time": "2025-03-17T05:19:44.250677Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def collect_inference(path: str, exclude: list[str]):\n",
    "\tmodels = [\"distilbert\", \"securebert\", \"roberta\", \"secbert\", \"secroberta\", \"bert\"]\n",
    "\tall_metrics_data = []\n",
    "\n",
    "\tsubfolders = [\n",
    "\t\tf.path\n",
    "\t\tfor f in os.scandir(path)\n",
    "\t\tif f.is_dir() and f.name not in exclude\n",
    "\t]\n",
    "\n",
    "\tfor subfolder in subfolders:\n",
    "\t\tmetrics_file_path = os.path.join(subfolder, \"metrics.json\")\n",
    "\t\tif not os.path.exists(metrics_file_path):\n",
    "\t\t\tcontinue\n",
    "\t\twith open(metrics_file_path, 'r') as f:\n",
    "\t\t\tdata = json.load(f)\n",
    "\t\tdf = pd.DataFrame([data])\n",
    "\t\t# only get the test result\n",
    "\t\tmodel_type = None\n",
    "\t\tdata_type = None\n",
    "\n",
    "\t\tmatch = re.search(r\"_({})\".format(\"|\".join(models)), os.path.basename(subfolder))\n",
    "\t\tif match:\n",
    "\t\t\tmodel_type = str(os.path.basename(subfolder)[match.start():match.end()]).replace(\"_\", \"\", 1)\n",
    "\t\t\tdata_type = os.path.basename(subfolder)[:match.start()]\n",
    "\n",
    "\t\tdf[\"input\"] = data_type\n",
    "\t\tdf[\"model_type\"] = model_type\n",
    "\t\tdf[\"subfolder\"] = os.path.basename(subfolder)\n",
    "\t\tdf[\"path\"] = path\n",
    "\t\tall_metrics_data.append(df)\n",
    "\treturn pd.concat(all_metrics_data, ignore_index=True)\n",
    "\n",
    "\n",
    "path = \"./inference\"\n",
    "exclude = []\n",
    "metrics_infer = collect_inference(path, exclude)\n",
    "metrics_infer"
   ],
   "id": "1b357ebc2ffcb24f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      time_taken  security_comments  total_comments  ratio_comments  \\\n",
       "0   29953.621459             606096         1994770        0.303843   \n",
       "1   58758.298360             809643         1994770        0.405883   \n",
       "2     472.382529               2977           47902        0.062148   \n",
       "3   23529.002314             206502          666055        0.310037   \n",
       "4   21785.914553             313939          666055        0.471341   \n",
       "5     496.260377               5812           47902        0.121331   \n",
       "6   20417.561865             194379         1994770        0.097444   \n",
       "7       6.764755                 74             423        0.174941   \n",
       "8   14327.584995             352399          666055        0.529084   \n",
       "9       0.960328                 12              99        0.121212   \n",
       "10      1.470950                 28              99        0.282828   \n",
       "11      3.010072                 20              87        0.229885   \n",
       "12  66188.934144             639696         1994770        0.320687   \n",
       "13      6.890830                 93             423        0.219858   \n",
       "14  25361.799899             293519          666055        0.440683   \n",
       "15      1.419323                 17              99        0.171717   \n",
       "16    499.454906               4708           47902        0.098284   \n",
       "17      0.978622                 19              99        0.191919   \n",
       "18      4.528733                 69             423        0.163121   \n",
       "19      5.721787                 70             423        0.165485   \n",
       "20  70403.097988             659294         1994770        0.330511   \n",
       "21   9652.936933             220137          666055        0.330509   \n",
       "22    880.402125               5219           47902        0.108952   \n",
       "23  24871.668902             230022          666055        0.345350   \n",
       "24  71606.995023             707899         1994770        0.354878   \n",
       "25   1489.533891               9141           47902        0.190827   \n",
       "26      1.035778                 28              99        0.282828   \n",
       "27      3.111294                 16              87        0.183908   \n",
       "28      2.915604                 15              87        0.172414   \n",
       "29  60617.808413             554789         1994770        0.278122   \n",
       "30    267.204011               9041           47902        0.188740   \n",
       "31  14325.029742             287102          666055        0.431048   \n",
       "32   1488.118644               7531           47902        0.157217   \n",
       "33  60667.837833             682497         1994770        0.342143   \n",
       "34   1385.064683               8938           47902        0.186589   \n",
       "35      2.632390                 10              87        0.114943   \n",
       "36   6155.984443             174431          666055        0.261887   \n",
       "37      0.861370                  0               2        0.000000   \n",
       "38  11645.064439             223827          666055        0.336049   \n",
       "39  18678.573859             287030          666055        0.430940   \n",
       "40  29267.673163            1985819         1994770        0.995513   \n",
       "41    263.305641               6747           47902        0.140850   \n",
       "42    953.017890               6444           47902        0.134525   \n",
       "43  29273.551469            1647912         1994770        0.826116   \n",
       "44    961.172332               8853           47902        0.184815   \n",
       "45  33357.983018             739930         1994770        0.370935   \n",
       "46  21785.376943             181809          666055        0.272964   \n",
       "47      1.603648                 20              95        0.210526   \n",
       "48      6.645070                 81             423        0.191489   \n",
       "\n",
       "    security_discussion_sigmoid  security_discussions_softmax  \\\n",
       "0                        605972                        605972   \n",
       "1                        809454                        809454   \n",
       "2                          1852                          1852   \n",
       "3                         86186                         86186   \n",
       "4                        102272                        102272   \n",
       "5                          3654                          3654   \n",
       "6                        194332                        194332   \n",
       "7                            29                            29   \n",
       "8                        116014                        116014   \n",
       "9                            12                            12   \n",
       "10                           28                            28   \n",
       "11                           20                            20   \n",
       "12                       639551                        639551   \n",
       "13                           32                            32   \n",
       "14                        99302                         99302   \n",
       "15                           17                            17   \n",
       "16                         2766                          2766   \n",
       "17                           19                            19   \n",
       "18                           22                            22   \n",
       "19                           23                            23   \n",
       "20                       659138                        659138   \n",
       "21                        91034                         91034   \n",
       "22                         2875                          2875   \n",
       "23                        94665                         94665   \n",
       "24                       707751                        707751   \n",
       "25                         5499                          5499   \n",
       "26                           28                            28   \n",
       "27                           16                            16   \n",
       "28                           15                            15   \n",
       "29                       554658                        554658   \n",
       "30                         5524                          5524   \n",
       "31                        98769                         98769   \n",
       "32                         4528                          4528   \n",
       "33                       682335                        682335   \n",
       "34                         5177                          5177   \n",
       "35                           10                            10   \n",
       "36                        75824                         75824   \n",
       "37                            0                             0   \n",
       "38                        89796                         89796   \n",
       "39                       104345                        104345   \n",
       "40                      1985491                       1985491   \n",
       "41                         3905                          3905   \n",
       "42                         4055                          4055   \n",
       "43                      1647620                       1647620   \n",
       "44                         5161                          5161   \n",
       "45                       739756                        739756   \n",
       "46                        79189                         79189   \n",
       "47                           20                            20   \n",
       "48                           24                            24   \n",
       "\n",
       "    total_discussions  ratio_discussions_sigmoid  ratio_discussions_softmax  \\\n",
       "0             1994441                   0.303830                   0.303830   \n",
       "1             1994441                   0.405855                   0.405855   \n",
       "2               28185                   0.065709                   0.065709   \n",
       "3              205650                   0.419091                   0.419091   \n",
       "4              205650                   0.497311                   0.497311   \n",
       "5               28185                   0.129643                   0.129643   \n",
       "6             1994441                   0.097437                   0.097437   \n",
       "7                  98                   0.295918                   0.295918   \n",
       "8              205650                   0.564133                   0.564133   \n",
       "9                  99                   0.121212                   0.121212   \n",
       "10                 99                   0.282828                   0.282828   \n",
       "11                 87                   0.229885                   0.229885   \n",
       "12            1994441                   0.320667                   0.320667   \n",
       "13                 98                   0.326531                   0.326531   \n",
       "14             205650                   0.482869                   0.482869   \n",
       "15                 99                   0.171717                   0.171717   \n",
       "16              28185                   0.098137                   0.098137   \n",
       "17                 99                   0.191919                   0.191919   \n",
       "18                 98                   0.224490                   0.224490   \n",
       "19                 98                   0.234694                   0.234694   \n",
       "20            1994441                   0.330488                   0.330488   \n",
       "21             205650                   0.442665                   0.442665   \n",
       "22              28185                   0.102005                   0.102005   \n",
       "23             205650                   0.460321                   0.460321   \n",
       "24            1994441                   0.354862                   0.354862   \n",
       "25              28185                   0.195104                   0.195104   \n",
       "26                 99                   0.282828                   0.282828   \n",
       "27                 87                   0.183908                   0.183908   \n",
       "28                 87                   0.172414                   0.172414   \n",
       "29            1994441                   0.278102                   0.278102   \n",
       "30              28185                   0.195991                   0.195991   \n",
       "31             205650                   0.480277                   0.480277   \n",
       "32              28185                   0.160653                   0.160653   \n",
       "33            1994441                   0.342118                   0.342118   \n",
       "34              28185                   0.183679                   0.183679   \n",
       "35                 87                   0.114943                   0.114943   \n",
       "36             205650                   0.368704                   0.368704   \n",
       "37                  1                   0.000000                   0.000000   \n",
       "38             205650                   0.436645                   0.436645   \n",
       "39             205650                   0.507391                   0.507391   \n",
       "40            1994441                   0.995513                   0.995513   \n",
       "41              28185                   0.138549                   0.138549   \n",
       "42              28185                   0.143871                   0.143871   \n",
       "43            1994441                   0.826106                   0.826106   \n",
       "44              28185                   0.183112                   0.183112   \n",
       "45            1994441                   0.370909                   0.370909   \n",
       "46             205650                   0.385067                   0.385067   \n",
       "47                 95                   0.210526                   0.210526   \n",
       "48                 98                   0.244898                   0.244898   \n",
       "\n",
       "           input  model_type                                  subfolder  \\\n",
       "0   all_gh_issue  distilbert  all_gh_issue_distilbert_tuned_all_9_train   \n",
       "1   all_gh_issue        bert        all_gh_issue_bert_tuned_all_9_train   \n",
       "2         all_gh     roberta                   all_gh_roberta_tuned_all   \n",
       "3         all_hf     secbert                   all_hf_secbert_tuned_all   \n",
       "4         all_hf  secroberta                all_hf_secroberta_tuned_all   \n",
       "5         all_gh  distilbert                all_gh_distilbert_tuned_all   \n",
       "6   all_gh_issue     roberta             all_gh_issue_roberta_tuned_all   \n",
       "7      test_full  securebert     test_full_securebert_tuned_all_9_train   \n",
       "8         all_hf  distilbert                  all_hf_distilbert_hf_last   \n",
       "9        test_99  distilbert               test_99_distilbert_tuned_all   \n",
       "10       test_99     roberta          test_99_roberta_tuned_all_9_train   \n",
       "11       test_99        bert             test_99_bert_tuned_all_9_train   \n",
       "12  all_gh_issue     secbert             all_gh_issue_secbert_tuned_all   \n",
       "13     test_full        bert           test_full_bert_tuned_all_9_train   \n",
       "14        all_hf        bert                      all_hf_bert_tuned_all   \n",
       "15       test_99        bert                     test_99_bert_tuned_all   \n",
       "16        all_gh  secroberta                all_gh_secroberta_tuned_all   \n",
       "17       test_99     secbert                  test_99_secbert_tuned_all   \n",
       "18     test_full     roberta        test_full_roberta_tuned_all_9_train   \n",
       "19     test_full  distilbert     test_full_distilbert_tuned_all_9_train   \n",
       "20  all_gh_issue  securebert          all_gh_issue_securebert_tuned_all   \n",
       "21        all_hf     secbert           all_hf_secbert_tuned_all_9_train   \n",
       "22        all_gh  distilbert        all_gh_distilbert_tuned_all_9_train   \n",
       "23        all_hf  securebert                all_hf_securebert_tuned_all   \n",
       "24  all_gh_issue        bert                all_gh_issue_bert_tuned_all   \n",
       "25        all_gh  distilbert                  all_gh_distilbert_gh_best   \n",
       "26       test_99  secroberta               test_99_secroberta_tuned_all   \n",
       "27       test_99  securebert       test_99_securebert_tuned_all_9_train   \n",
       "28       test_99     secbert          test_99_secbert_tuned_all_9_train   \n",
       "29  all_gh_issue  distilbert          all_gh_issue_distilbert_tuned_all   \n",
       "30        all_gh     secbert           all_gh_secbert_tuned_all_9_train   \n",
       "31        all_hf  distilbert                  all_hf_distilbert_hf_best   \n",
       "32        all_gh  distilbert                  all_gh_distilbert_gh_last   \n",
       "33  all_gh_issue  secroberta          all_gh_issue_secroberta_tuned_all   \n",
       "34        all_gh        bert              all_gh_bert_tuned_all_9_train   \n",
       "35       test_99  distilbert       test_99_distilbert_tuned_all_9_train   \n",
       "36        all_hf     roberta                   all_hf_roberta_tuned_all   \n",
       "37          None        None                                       test   \n",
       "38        all_hf  distilbert        all_hf_distilbert_tuned_all_9_train   \n",
       "39        all_hf        bert              all_hf_bert_tuned_all_9_train   \n",
       "40  all_gh_issue  distilbert     all_gh_issue_distilbert_gh_issues_last   \n",
       "41        all_gh     secbert                   all_gh_secbert_tuned_all   \n",
       "42        all_gh  securebert                all_gh_securebert_tuned_all   \n",
       "43  all_gh_issue  distilbert     all_gh_issue_distilbert_gh_issues_best   \n",
       "44        all_gh        bert                      all_gh_bert_tuned_all   \n",
       "45  all_gh_issue     secbert     all_gh_issue_secbert_tuned_all_9_train   \n",
       "46        all_hf  distilbert                all_hf_distilbert_tuned_all   \n",
       "47       test_99  securebert               test_99_securebert_tuned_all   \n",
       "48     test_full     secbert        test_full_secbert_tuned_all_9_train   \n",
       "\n",
       "           path  \n",
       "0   ./inference  \n",
       "1   ./inference  \n",
       "2   ./inference  \n",
       "3   ./inference  \n",
       "4   ./inference  \n",
       "5   ./inference  \n",
       "6   ./inference  \n",
       "7   ./inference  \n",
       "8   ./inference  \n",
       "9   ./inference  \n",
       "10  ./inference  \n",
       "11  ./inference  \n",
       "12  ./inference  \n",
       "13  ./inference  \n",
       "14  ./inference  \n",
       "15  ./inference  \n",
       "16  ./inference  \n",
       "17  ./inference  \n",
       "18  ./inference  \n",
       "19  ./inference  \n",
       "20  ./inference  \n",
       "21  ./inference  \n",
       "22  ./inference  \n",
       "23  ./inference  \n",
       "24  ./inference  \n",
       "25  ./inference  \n",
       "26  ./inference  \n",
       "27  ./inference  \n",
       "28  ./inference  \n",
       "29  ./inference  \n",
       "30  ./inference  \n",
       "31  ./inference  \n",
       "32  ./inference  \n",
       "33  ./inference  \n",
       "34  ./inference  \n",
       "35  ./inference  \n",
       "36  ./inference  \n",
       "37  ./inference  \n",
       "38  ./inference  \n",
       "39  ./inference  \n",
       "40  ./inference  \n",
       "41  ./inference  \n",
       "42  ./inference  \n",
       "43  ./inference  \n",
       "44  ./inference  \n",
       "45  ./inference  \n",
       "46  ./inference  \n",
       "47  ./inference  \n",
       "48  ./inference  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time_taken</th>\n",
       "      <th>security_comments</th>\n",
       "      <th>total_comments</th>\n",
       "      <th>ratio_comments</th>\n",
       "      <th>security_discussion_sigmoid</th>\n",
       "      <th>security_discussions_softmax</th>\n",
       "      <th>total_discussions</th>\n",
       "      <th>ratio_discussions_sigmoid</th>\n",
       "      <th>ratio_discussions_softmax</th>\n",
       "      <th>input</th>\n",
       "      <th>model_type</th>\n",
       "      <th>subfolder</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>29953.621459</td>\n",
       "      <td>606096</td>\n",
       "      <td>1994770</td>\n",
       "      <td>0.303843</td>\n",
       "      <td>605972</td>\n",
       "      <td>605972</td>\n",
       "      <td>1994441</td>\n",
       "      <td>0.303830</td>\n",
       "      <td>0.303830</td>\n",
       "      <td>all_gh_issue</td>\n",
       "      <td>distilbert</td>\n",
       "      <td>all_gh_issue_distilbert_tuned_all_9_train</td>\n",
       "      <td>./inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>58758.298360</td>\n",
       "      <td>809643</td>\n",
       "      <td>1994770</td>\n",
       "      <td>0.405883</td>\n",
       "      <td>809454</td>\n",
       "      <td>809454</td>\n",
       "      <td>1994441</td>\n",
       "      <td>0.405855</td>\n",
       "      <td>0.405855</td>\n",
       "      <td>all_gh_issue</td>\n",
       "      <td>bert</td>\n",
       "      <td>all_gh_issue_bert_tuned_all_9_train</td>\n",
       "      <td>./inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>472.382529</td>\n",
       "      <td>2977</td>\n",
       "      <td>47902</td>\n",
       "      <td>0.062148</td>\n",
       "      <td>1852</td>\n",
       "      <td>1852</td>\n",
       "      <td>28185</td>\n",
       "      <td>0.065709</td>\n",
       "      <td>0.065709</td>\n",
       "      <td>all_gh</td>\n",
       "      <td>roberta</td>\n",
       "      <td>all_gh_roberta_tuned_all</td>\n",
       "      <td>./inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23529.002314</td>\n",
       "      <td>206502</td>\n",
       "      <td>666055</td>\n",
       "      <td>0.310037</td>\n",
       "      <td>86186</td>\n",
       "      <td>86186</td>\n",
       "      <td>205650</td>\n",
       "      <td>0.419091</td>\n",
       "      <td>0.419091</td>\n",
       "      <td>all_hf</td>\n",
       "      <td>secbert</td>\n",
       "      <td>all_hf_secbert_tuned_all</td>\n",
       "      <td>./inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21785.914553</td>\n",
       "      <td>313939</td>\n",
       "      <td>666055</td>\n",
       "      <td>0.471341</td>\n",
       "      <td>102272</td>\n",
       "      <td>102272</td>\n",
       "      <td>205650</td>\n",
       "      <td>0.497311</td>\n",
       "      <td>0.497311</td>\n",
       "      <td>all_hf</td>\n",
       "      <td>secroberta</td>\n",
       "      <td>all_hf_secroberta_tuned_all</td>\n",
       "      <td>./inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>496.260377</td>\n",
       "      <td>5812</td>\n",
       "      <td>47902</td>\n",
       "      <td>0.121331</td>\n",
       "      <td>3654</td>\n",
       "      <td>3654</td>\n",
       "      <td>28185</td>\n",
       "      <td>0.129643</td>\n",
       "      <td>0.129643</td>\n",
       "      <td>all_gh</td>\n",
       "      <td>distilbert</td>\n",
       "      <td>all_gh_distilbert_tuned_all</td>\n",
       "      <td>./inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>20417.561865</td>\n",
       "      <td>194379</td>\n",
       "      <td>1994770</td>\n",
       "      <td>0.097444</td>\n",
       "      <td>194332</td>\n",
       "      <td>194332</td>\n",
       "      <td>1994441</td>\n",
       "      <td>0.097437</td>\n",
       "      <td>0.097437</td>\n",
       "      <td>all_gh_issue</td>\n",
       "      <td>roberta</td>\n",
       "      <td>all_gh_issue_roberta_tuned_all</td>\n",
       "      <td>./inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6.764755</td>\n",
       "      <td>74</td>\n",
       "      <td>423</td>\n",
       "      <td>0.174941</td>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>98</td>\n",
       "      <td>0.295918</td>\n",
       "      <td>0.295918</td>\n",
       "      <td>test_full</td>\n",
       "      <td>securebert</td>\n",
       "      <td>test_full_securebert_tuned_all_9_train</td>\n",
       "      <td>./inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>14327.584995</td>\n",
       "      <td>352399</td>\n",
       "      <td>666055</td>\n",
       "      <td>0.529084</td>\n",
       "      <td>116014</td>\n",
       "      <td>116014</td>\n",
       "      <td>205650</td>\n",
       "      <td>0.564133</td>\n",
       "      <td>0.564133</td>\n",
       "      <td>all_hf</td>\n",
       "      <td>distilbert</td>\n",
       "      <td>all_hf_distilbert_hf_last</td>\n",
       "      <td>./inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.960328</td>\n",
       "      <td>12</td>\n",
       "      <td>99</td>\n",
       "      <td>0.121212</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>99</td>\n",
       "      <td>0.121212</td>\n",
       "      <td>0.121212</td>\n",
       "      <td>test_99</td>\n",
       "      <td>distilbert</td>\n",
       "      <td>test_99_distilbert_tuned_all</td>\n",
       "      <td>./inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.470950</td>\n",
       "      <td>28</td>\n",
       "      <td>99</td>\n",
       "      <td>0.282828</td>\n",
       "      <td>28</td>\n",
       "      <td>28</td>\n",
       "      <td>99</td>\n",
       "      <td>0.282828</td>\n",
       "      <td>0.282828</td>\n",
       "      <td>test_99</td>\n",
       "      <td>roberta</td>\n",
       "      <td>test_99_roberta_tuned_all_9_train</td>\n",
       "      <td>./inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3.010072</td>\n",
       "      <td>20</td>\n",
       "      <td>87</td>\n",
       "      <td>0.229885</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>87</td>\n",
       "      <td>0.229885</td>\n",
       "      <td>0.229885</td>\n",
       "      <td>test_99</td>\n",
       "      <td>bert</td>\n",
       "      <td>test_99_bert_tuned_all_9_train</td>\n",
       "      <td>./inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>66188.934144</td>\n",
       "      <td>639696</td>\n",
       "      <td>1994770</td>\n",
       "      <td>0.320687</td>\n",
       "      <td>639551</td>\n",
       "      <td>639551</td>\n",
       "      <td>1994441</td>\n",
       "      <td>0.320667</td>\n",
       "      <td>0.320667</td>\n",
       "      <td>all_gh_issue</td>\n",
       "      <td>secbert</td>\n",
       "      <td>all_gh_issue_secbert_tuned_all</td>\n",
       "      <td>./inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>6.890830</td>\n",
       "      <td>93</td>\n",
       "      <td>423</td>\n",
       "      <td>0.219858</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>98</td>\n",
       "      <td>0.326531</td>\n",
       "      <td>0.326531</td>\n",
       "      <td>test_full</td>\n",
       "      <td>bert</td>\n",
       "      <td>test_full_bert_tuned_all_9_train</td>\n",
       "      <td>./inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>25361.799899</td>\n",
       "      <td>293519</td>\n",
       "      <td>666055</td>\n",
       "      <td>0.440683</td>\n",
       "      <td>99302</td>\n",
       "      <td>99302</td>\n",
       "      <td>205650</td>\n",
       "      <td>0.482869</td>\n",
       "      <td>0.482869</td>\n",
       "      <td>all_hf</td>\n",
       "      <td>bert</td>\n",
       "      <td>all_hf_bert_tuned_all</td>\n",
       "      <td>./inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.419323</td>\n",
       "      <td>17</td>\n",
       "      <td>99</td>\n",
       "      <td>0.171717</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>99</td>\n",
       "      <td>0.171717</td>\n",
       "      <td>0.171717</td>\n",
       "      <td>test_99</td>\n",
       "      <td>bert</td>\n",
       "      <td>test_99_bert_tuned_all</td>\n",
       "      <td>./inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>499.454906</td>\n",
       "      <td>4708</td>\n",
       "      <td>47902</td>\n",
       "      <td>0.098284</td>\n",
       "      <td>2766</td>\n",
       "      <td>2766</td>\n",
       "      <td>28185</td>\n",
       "      <td>0.098137</td>\n",
       "      <td>0.098137</td>\n",
       "      <td>all_gh</td>\n",
       "      <td>secroberta</td>\n",
       "      <td>all_gh_secroberta_tuned_all</td>\n",
       "      <td>./inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.978622</td>\n",
       "      <td>19</td>\n",
       "      <td>99</td>\n",
       "      <td>0.191919</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>99</td>\n",
       "      <td>0.191919</td>\n",
       "      <td>0.191919</td>\n",
       "      <td>test_99</td>\n",
       "      <td>secbert</td>\n",
       "      <td>test_99_secbert_tuned_all</td>\n",
       "      <td>./inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>4.528733</td>\n",
       "      <td>69</td>\n",
       "      <td>423</td>\n",
       "      <td>0.163121</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>98</td>\n",
       "      <td>0.224490</td>\n",
       "      <td>0.224490</td>\n",
       "      <td>test_full</td>\n",
       "      <td>roberta</td>\n",
       "      <td>test_full_roberta_tuned_all_9_train</td>\n",
       "      <td>./inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>5.721787</td>\n",
       "      <td>70</td>\n",
       "      <td>423</td>\n",
       "      <td>0.165485</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>98</td>\n",
       "      <td>0.234694</td>\n",
       "      <td>0.234694</td>\n",
       "      <td>test_full</td>\n",
       "      <td>distilbert</td>\n",
       "      <td>test_full_distilbert_tuned_all_9_train</td>\n",
       "      <td>./inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>70403.097988</td>\n",
       "      <td>659294</td>\n",
       "      <td>1994770</td>\n",
       "      <td>0.330511</td>\n",
       "      <td>659138</td>\n",
       "      <td>659138</td>\n",
       "      <td>1994441</td>\n",
       "      <td>0.330488</td>\n",
       "      <td>0.330488</td>\n",
       "      <td>all_gh_issue</td>\n",
       "      <td>securebert</td>\n",
       "      <td>all_gh_issue_securebert_tuned_all</td>\n",
       "      <td>./inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>9652.936933</td>\n",
       "      <td>220137</td>\n",
       "      <td>666055</td>\n",
       "      <td>0.330509</td>\n",
       "      <td>91034</td>\n",
       "      <td>91034</td>\n",
       "      <td>205650</td>\n",
       "      <td>0.442665</td>\n",
       "      <td>0.442665</td>\n",
       "      <td>all_hf</td>\n",
       "      <td>secbert</td>\n",
       "      <td>all_hf_secbert_tuned_all_9_train</td>\n",
       "      <td>./inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>880.402125</td>\n",
       "      <td>5219</td>\n",
       "      <td>47902</td>\n",
       "      <td>0.108952</td>\n",
       "      <td>2875</td>\n",
       "      <td>2875</td>\n",
       "      <td>28185</td>\n",
       "      <td>0.102005</td>\n",
       "      <td>0.102005</td>\n",
       "      <td>all_gh</td>\n",
       "      <td>distilbert</td>\n",
       "      <td>all_gh_distilbert_tuned_all_9_train</td>\n",
       "      <td>./inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24871.668902</td>\n",
       "      <td>230022</td>\n",
       "      <td>666055</td>\n",
       "      <td>0.345350</td>\n",
       "      <td>94665</td>\n",
       "      <td>94665</td>\n",
       "      <td>205650</td>\n",
       "      <td>0.460321</td>\n",
       "      <td>0.460321</td>\n",
       "      <td>all_hf</td>\n",
       "      <td>securebert</td>\n",
       "      <td>all_hf_securebert_tuned_all</td>\n",
       "      <td>./inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>71606.995023</td>\n",
       "      <td>707899</td>\n",
       "      <td>1994770</td>\n",
       "      <td>0.354878</td>\n",
       "      <td>707751</td>\n",
       "      <td>707751</td>\n",
       "      <td>1994441</td>\n",
       "      <td>0.354862</td>\n",
       "      <td>0.354862</td>\n",
       "      <td>all_gh_issue</td>\n",
       "      <td>bert</td>\n",
       "      <td>all_gh_issue_bert_tuned_all</td>\n",
       "      <td>./inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1489.533891</td>\n",
       "      <td>9141</td>\n",
       "      <td>47902</td>\n",
       "      <td>0.190827</td>\n",
       "      <td>5499</td>\n",
       "      <td>5499</td>\n",
       "      <td>28185</td>\n",
       "      <td>0.195104</td>\n",
       "      <td>0.195104</td>\n",
       "      <td>all_gh</td>\n",
       "      <td>distilbert</td>\n",
       "      <td>all_gh_distilbert_gh_best</td>\n",
       "      <td>./inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1.035778</td>\n",
       "      <td>28</td>\n",
       "      <td>99</td>\n",
       "      <td>0.282828</td>\n",
       "      <td>28</td>\n",
       "      <td>28</td>\n",
       "      <td>99</td>\n",
       "      <td>0.282828</td>\n",
       "      <td>0.282828</td>\n",
       "      <td>test_99</td>\n",
       "      <td>secroberta</td>\n",
       "      <td>test_99_secroberta_tuned_all</td>\n",
       "      <td>./inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>3.111294</td>\n",
       "      <td>16</td>\n",
       "      <td>87</td>\n",
       "      <td>0.183908</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>87</td>\n",
       "      <td>0.183908</td>\n",
       "      <td>0.183908</td>\n",
       "      <td>test_99</td>\n",
       "      <td>securebert</td>\n",
       "      <td>test_99_securebert_tuned_all_9_train</td>\n",
       "      <td>./inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2.915604</td>\n",
       "      <td>15</td>\n",
       "      <td>87</td>\n",
       "      <td>0.172414</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>87</td>\n",
       "      <td>0.172414</td>\n",
       "      <td>0.172414</td>\n",
       "      <td>test_99</td>\n",
       "      <td>secbert</td>\n",
       "      <td>test_99_secbert_tuned_all_9_train</td>\n",
       "      <td>./inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>60617.808413</td>\n",
       "      <td>554789</td>\n",
       "      <td>1994770</td>\n",
       "      <td>0.278122</td>\n",
       "      <td>554658</td>\n",
       "      <td>554658</td>\n",
       "      <td>1994441</td>\n",
       "      <td>0.278102</td>\n",
       "      <td>0.278102</td>\n",
       "      <td>all_gh_issue</td>\n",
       "      <td>distilbert</td>\n",
       "      <td>all_gh_issue_distilbert_tuned_all</td>\n",
       "      <td>./inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>267.204011</td>\n",
       "      <td>9041</td>\n",
       "      <td>47902</td>\n",
       "      <td>0.188740</td>\n",
       "      <td>5524</td>\n",
       "      <td>5524</td>\n",
       "      <td>28185</td>\n",
       "      <td>0.195991</td>\n",
       "      <td>0.195991</td>\n",
       "      <td>all_gh</td>\n",
       "      <td>secbert</td>\n",
       "      <td>all_gh_secbert_tuned_all_9_train</td>\n",
       "      <td>./inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>14325.029742</td>\n",
       "      <td>287102</td>\n",
       "      <td>666055</td>\n",
       "      <td>0.431048</td>\n",
       "      <td>98769</td>\n",
       "      <td>98769</td>\n",
       "      <td>205650</td>\n",
       "      <td>0.480277</td>\n",
       "      <td>0.480277</td>\n",
       "      <td>all_hf</td>\n",
       "      <td>distilbert</td>\n",
       "      <td>all_hf_distilbert_hf_best</td>\n",
       "      <td>./inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1488.118644</td>\n",
       "      <td>7531</td>\n",
       "      <td>47902</td>\n",
       "      <td>0.157217</td>\n",
       "      <td>4528</td>\n",
       "      <td>4528</td>\n",
       "      <td>28185</td>\n",
       "      <td>0.160653</td>\n",
       "      <td>0.160653</td>\n",
       "      <td>all_gh</td>\n",
       "      <td>distilbert</td>\n",
       "      <td>all_gh_distilbert_gh_last</td>\n",
       "      <td>./inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>60667.837833</td>\n",
       "      <td>682497</td>\n",
       "      <td>1994770</td>\n",
       "      <td>0.342143</td>\n",
       "      <td>682335</td>\n",
       "      <td>682335</td>\n",
       "      <td>1994441</td>\n",
       "      <td>0.342118</td>\n",
       "      <td>0.342118</td>\n",
       "      <td>all_gh_issue</td>\n",
       "      <td>secroberta</td>\n",
       "      <td>all_gh_issue_secroberta_tuned_all</td>\n",
       "      <td>./inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1385.064683</td>\n",
       "      <td>8938</td>\n",
       "      <td>47902</td>\n",
       "      <td>0.186589</td>\n",
       "      <td>5177</td>\n",
       "      <td>5177</td>\n",
       "      <td>28185</td>\n",
       "      <td>0.183679</td>\n",
       "      <td>0.183679</td>\n",
       "      <td>all_gh</td>\n",
       "      <td>bert</td>\n",
       "      <td>all_gh_bert_tuned_all_9_train</td>\n",
       "      <td>./inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2.632390</td>\n",
       "      <td>10</td>\n",
       "      <td>87</td>\n",
       "      <td>0.114943</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>87</td>\n",
       "      <td>0.114943</td>\n",
       "      <td>0.114943</td>\n",
       "      <td>test_99</td>\n",
       "      <td>distilbert</td>\n",
       "      <td>test_99_distilbert_tuned_all_9_train</td>\n",
       "      <td>./inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>6155.984443</td>\n",
       "      <td>174431</td>\n",
       "      <td>666055</td>\n",
       "      <td>0.261887</td>\n",
       "      <td>75824</td>\n",
       "      <td>75824</td>\n",
       "      <td>205650</td>\n",
       "      <td>0.368704</td>\n",
       "      <td>0.368704</td>\n",
       "      <td>all_hf</td>\n",
       "      <td>roberta</td>\n",
       "      <td>all_hf_roberta_tuned_all</td>\n",
       "      <td>./inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.861370</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>test</td>\n",
       "      <td>./inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>11645.064439</td>\n",
       "      <td>223827</td>\n",
       "      <td>666055</td>\n",
       "      <td>0.336049</td>\n",
       "      <td>89796</td>\n",
       "      <td>89796</td>\n",
       "      <td>205650</td>\n",
       "      <td>0.436645</td>\n",
       "      <td>0.436645</td>\n",
       "      <td>all_hf</td>\n",
       "      <td>distilbert</td>\n",
       "      <td>all_hf_distilbert_tuned_all_9_train</td>\n",
       "      <td>./inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>18678.573859</td>\n",
       "      <td>287030</td>\n",
       "      <td>666055</td>\n",
       "      <td>0.430940</td>\n",
       "      <td>104345</td>\n",
       "      <td>104345</td>\n",
       "      <td>205650</td>\n",
       "      <td>0.507391</td>\n",
       "      <td>0.507391</td>\n",
       "      <td>all_hf</td>\n",
       "      <td>bert</td>\n",
       "      <td>all_hf_bert_tuned_all_9_train</td>\n",
       "      <td>./inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>29267.673163</td>\n",
       "      <td>1985819</td>\n",
       "      <td>1994770</td>\n",
       "      <td>0.995513</td>\n",
       "      <td>1985491</td>\n",
       "      <td>1985491</td>\n",
       "      <td>1994441</td>\n",
       "      <td>0.995513</td>\n",
       "      <td>0.995513</td>\n",
       "      <td>all_gh_issue</td>\n",
       "      <td>distilbert</td>\n",
       "      <td>all_gh_issue_distilbert_gh_issues_last</td>\n",
       "      <td>./inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>263.305641</td>\n",
       "      <td>6747</td>\n",
       "      <td>47902</td>\n",
       "      <td>0.140850</td>\n",
       "      <td>3905</td>\n",
       "      <td>3905</td>\n",
       "      <td>28185</td>\n",
       "      <td>0.138549</td>\n",
       "      <td>0.138549</td>\n",
       "      <td>all_gh</td>\n",
       "      <td>secbert</td>\n",
       "      <td>all_gh_secbert_tuned_all</td>\n",
       "      <td>./inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>953.017890</td>\n",
       "      <td>6444</td>\n",
       "      <td>47902</td>\n",
       "      <td>0.134525</td>\n",
       "      <td>4055</td>\n",
       "      <td>4055</td>\n",
       "      <td>28185</td>\n",
       "      <td>0.143871</td>\n",
       "      <td>0.143871</td>\n",
       "      <td>all_gh</td>\n",
       "      <td>securebert</td>\n",
       "      <td>all_gh_securebert_tuned_all</td>\n",
       "      <td>./inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>29273.551469</td>\n",
       "      <td>1647912</td>\n",
       "      <td>1994770</td>\n",
       "      <td>0.826116</td>\n",
       "      <td>1647620</td>\n",
       "      <td>1647620</td>\n",
       "      <td>1994441</td>\n",
       "      <td>0.826106</td>\n",
       "      <td>0.826106</td>\n",
       "      <td>all_gh_issue</td>\n",
       "      <td>distilbert</td>\n",
       "      <td>all_gh_issue_distilbert_gh_issues_best</td>\n",
       "      <td>./inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>961.172332</td>\n",
       "      <td>8853</td>\n",
       "      <td>47902</td>\n",
       "      <td>0.184815</td>\n",
       "      <td>5161</td>\n",
       "      <td>5161</td>\n",
       "      <td>28185</td>\n",
       "      <td>0.183112</td>\n",
       "      <td>0.183112</td>\n",
       "      <td>all_gh</td>\n",
       "      <td>bert</td>\n",
       "      <td>all_gh_bert_tuned_all</td>\n",
       "      <td>./inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>33357.983018</td>\n",
       "      <td>739930</td>\n",
       "      <td>1994770</td>\n",
       "      <td>0.370935</td>\n",
       "      <td>739756</td>\n",
       "      <td>739756</td>\n",
       "      <td>1994441</td>\n",
       "      <td>0.370909</td>\n",
       "      <td>0.370909</td>\n",
       "      <td>all_gh_issue</td>\n",
       "      <td>secbert</td>\n",
       "      <td>all_gh_issue_secbert_tuned_all_9_train</td>\n",
       "      <td>./inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>21785.376943</td>\n",
       "      <td>181809</td>\n",
       "      <td>666055</td>\n",
       "      <td>0.272964</td>\n",
       "      <td>79189</td>\n",
       "      <td>79189</td>\n",
       "      <td>205650</td>\n",
       "      <td>0.385067</td>\n",
       "      <td>0.385067</td>\n",
       "      <td>all_hf</td>\n",
       "      <td>distilbert</td>\n",
       "      <td>all_hf_distilbert_tuned_all</td>\n",
       "      <td>./inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>1.603648</td>\n",
       "      <td>20</td>\n",
       "      <td>95</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>95</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>test_99</td>\n",
       "      <td>securebert</td>\n",
       "      <td>test_99_securebert_tuned_all</td>\n",
       "      <td>./inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>6.645070</td>\n",
       "      <td>81</td>\n",
       "      <td>423</td>\n",
       "      <td>0.191489</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>98</td>\n",
       "      <td>0.244898</td>\n",
       "      <td>0.244898</td>\n",
       "      <td>test_full</td>\n",
       "      <td>secbert</td>\n",
       "      <td>test_full_secbert_tuned_all_9_train</td>\n",
       "      <td>./inference</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def collect_inference(path: str, exclude: list[str]):\n",
    "\tmodels = [\"llama\", \"deepseek\", \"phi4\", \"mistral\"]\n",
    "\tall_metrics_data = []\n",
    "\n",
    "\tsubfolders = [\n",
    "\t\tf.path\n",
    "\t\tfor f in os.scandir(path)\n",
    "\t\tif f.is_dir() and f.name not in exclude\n",
    "\t]\n",
    "\n",
    "\tfor subfolder in subfolders:\n",
    "\t\tmetrics_file_path = os.path.join(subfolder, \"metrics.json\")\n",
    "\t\tif not os.path.exists(metrics_file_path):\n",
    "\t\t\tcontinue\n",
    "\t\twith open(metrics_file_path, 'r') as f:\n",
    "\t\t\tdata = json.load(f)\n",
    "\t\tdf = pd.DataFrame([data])\n",
    "\t\t# only get the test result\n",
    "\t\tdf[\"subfolder\"] = os.path.basename(subfolder)\n",
    "\t\tmodel = None\n",
    "\t\tdata = None\n",
    "\t\tmatch = re.search(r\"_(llama\\d*|deepseekr\\d*|phi4|mistral(_small)?)\", os.path.basename(subfolder))\n",
    "\t\tif match:\n",
    "\t\t\tmodel = str(os.path.basename(subfolder)[match.start():]).replace(\"_\", \"\", 1)\n",
    "\t\t\tdata = os.path.basename(subfolder)[:match.start()]\n",
    "\t\tdf[\"input_type\"] = data\n",
    "\t\tdf[\"model\"] = model\n",
    "\t\tdf[\"path\"] = path\n",
    "\t\tall_metrics_data.append(df)\n",
    "\treturn pd.concat(all_metrics_data, ignore_index=True)\n",
    "\n",
    "\n",
    "path = \"./llm\"\n",
    "exclude = []\n",
    "metrics_infer = collect_inference(path, exclude)\n",
    "metrics_infer"
   ],
   "id": "31decfe7cda47e09"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import norm\n",
    "\n",
    "\n",
    "def get_subset(df_security, df_manual, col_ids: list[str]):\n",
    "\trecords_to_exclude = df_manual[col_ids].drop_duplicates()\n",
    "\tcol_ids_str = \"_\".join(col_ids)\n",
    "\trecords_to_exclude[col_ids_str] = records_to_exclude[col_ids[0]].astype(str) + \"_\" + records_to_exclude[\n",
    "\t\tcol_ids[1]].astype(str)\n",
    "\tprint(f\"{len(records_to_exclude)=}\")\n",
    "\tN = len(df_security)  # Population size\n",
    "\tZ = norm.ppf(0.975)  # Z-score for 95% confidence\n",
    "\tp = 0.5  # Worst-case scenario proportion\n",
    "\tE = 0.05  # Margin of error\n",
    "\n",
    "\t# Sample size formula\n",
    "\tnumerator = (N * (Z ** 2) * p * (1 - p))\n",
    "\tdenominator = ((E ** 2) * (N - 1)) + ((Z ** 2) * p * (1 - p))\n",
    "\tsample_size = int(numerator / denominator)\n",
    "\tprint(f\"Required sample size: {sample_size} out of {N}\")\n",
    "\n",
    "\tsample_df = pd.DataFrame()\n",
    "\twhile len(sample_df) < sample_size:\n",
    "\t\tprint(f\"Current {len(sample_df)=}, {sample_size-len(sample_df)=}\")\n",
    "\t\ttemp_sample = df_security.sample(n=(sample_size - len(sample_df)), random_state=42)\n",
    "\t\ttemp_sample[col_ids_str] = temp_sample[col_ids[0]].astype(str) + \"_\" + temp_sample[col_ids[1]].astype(str)\n",
    "\t\ttemp_sample = temp_sample[~temp_sample[col_ids_str].isin(records_to_exclude[col_ids_str])]\n",
    "\t\tsample_df = pd.concat([sample_df, temp_sample])\n",
    "\n",
    "\tprint(f\"Final sample {len(sample_df)=}\")\n",
    "\treturn sample_df\n",
    "\n",
    "# expr = \"all_gh_bert_gh_last\"\n",
    "# gh = pd.read_csv(f\"./inference/{expr}/raw_predictions.csv\")\n",
    "# gh_security = gh[gh[\"is_security_prediction\"] == 1]\n",
    "# gh_manual = pd.read_csv(f\"./merged_after_manual/merged_gh_discussions.csv\")\n",
    "# gh_security_sampled = get_subset(gh_security, gh_manual, [\"repo_name\", \"discussion_number\"])\n",
    "# gh_security_sampled.to_csv(f\"./sampled/{expr}_sampled.csv\", index=False)"
   ],
   "id": "fb5f4c68436ddea0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "expr = \"all_gh_issue_distilbert_tuned_all\"\n",
    "hf = pd.read_csv(f\"./inference/{expr}/raw_predictions.csv\")\n",
    "hf_security = hf[\n",
    "\t(hf[\"is_security_prediction\"] == 1) & ~(\n",
    "\t\t(hf[\"title\"] == \"Adding `safetensors` variant of this model\") |\n",
    "\t\t(hf[\"title\"] == \"Upload folder using huggingface_hub\")\n",
    "\t)\n",
    "\t]\n",
    "hf_manual = pd.read_csv(f\"./merged_after_manual/merged_hf_discussions.csv\")\n",
    "hf_manual = hf_manual[\n",
    "\t~(\n",
    "\t\t(hf_manual[\"title\"] == \"Adding `safetensors` variant of this model\") |\n",
    "\t\t(hf_manual[\"title\"] == \"Upload folder using huggingface_hub\")\n",
    "\t)\n",
    "]\n",
    "hf_security_sampled = get_subset(hf_security, hf_manual, [\"model_id\", \"num\"])\n",
    "hf_security_sampled.to_csv(f\"./sampled/{expr}_sampled.csv\", index=False)"
   ],
   "id": "56553e92423b9b44"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "expr = \"all_hf_roberta_hf_best\"\n",
    "hf = pd.read_csv(f\"./inference/{expr}/raw_predictions.csv\")\n",
    "print(len(hf))\n",
    "hf = hf[\n",
    "\t~(\n",
    "\t\t(hf[\"title\"] == \"Adding `safetensors` variant of this model\") |\n",
    "\t\t(hf[\"title\"] == \"Upload folder using huggingface_hub\")\n",
    "\t)\n",
    "]\n",
    "len(hf)\n",
    "\n",
    "path = \"./llm\"\n",
    "exclude = []\n",
    "metrics_infer = collect_inference(path, exclude)\n",
    "metrics_infer"
   ],
   "id": "8888b4b43a9b910e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import norm\n",
    "\n",
    "\n",
    "def get_subset(df_security, df_manual, col_ids: list[str]):\n",
    "\trecords_to_exclude = df_manual[col_ids].drop_duplicates()\n",
    "\tcol_ids_str = \"_\".join(col_ids)\n",
    "\trecords_to_exclude[col_ids_str] = records_to_exclude[col_ids[0]].astype(str) + \"_\" + records_to_exclude[\n",
    "\t\tcol_ids[1]].astype(str)\n",
    "\tprint(f\"{len(records_to_exclude)=}\")\n",
    "\tN = len(df_security)  # Population size\n",
    "\tZ = norm.ppf(0.975)  # Z-score for 95% confidence\n",
    "\tp = 0.5  # Worst-case scenario proportion\n",
    "\tE = 0.05  # Margin of error\n",
    "\n",
    "\t# Sample size formula\n",
    "\tnumerator = (N * (Z ** 2) * p * (1 - p))\n",
    "\tdenominator = ((E ** 2) * (N - 1)) + ((Z ** 2) * p * (1 - p))\n",
    "\tsample_size = int(numerator / denominator)\n",
    "\tprint(f\"Required sample size: {sample_size} out of {N}\")\n",
    "\n",
    "\tsample_df = pd.DataFrame()\n",
    "\twhile len(sample_df) < sample_size:\n",
    "\t\tprint(f\"Current {len(sample_df)=}, {sample_size-len(sample_df)=}\")\n",
    "\t\ttemp_sample = df_security.sample(n=(sample_size - len(sample_df)), random_state=42)\n",
    "\t\ttemp_sample[col_ids_str] = temp_sample[col_ids[0]].astype(str) + \"_\" + temp_sample[col_ids[1]].astype(str)\n",
    "\t\ttemp_sample = temp_sample[~temp_sample[col_ids_str].isin(records_to_exclude[col_ids_str])]\n",
    "\t\tsample_df = pd.concat([sample_df, temp_sample])\n",
    "\n",
    "\tprint(f\"Final sample {len(sample_df)=}\")\n",
    "\treturn sample_df\n",
    "\n",
    "\n",
    "expr = \"all_gh_bert_gh_last\"\n",
    "gh = pd.read_csv(f\"./inference/{expr}/raw_predictions.csv\")\n",
    "gh_security = gh[gh[\"is_security_prediction\"] == 1]\n",
    "gh_manual = pd.read_csv(f\"./merged_after_manual/merged_gh_discussions.csv\")\n",
    "gh_security_sampled = get_subset(gh_security, gh_manual, [\"repo_name\", \"discussion_number\"])\n",
    "gh_security_sampled.to_csv(f\"./sampled/{expr}_sampled.csv\", index=False)"
   ],
   "id": "1c30963ec06b6b1e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "expr = \"all_hf_bert_tuned_all\"\n",
    "hf = pd.read_csv(f\"./inference/{expr}/raw_predictions.csv\")\n",
    "hf_security = hf[\n",
    "\t(hf[\"is_security_prediction\"] == 1) & ~(\n",
    "\t\t(hf[\"title\"] == \"Adding `safetensors` variant of this model\") |\n",
    "\t\t(hf[\"title\"] == \"Upload folder using huggingface_hub\")\n",
    "\t)\n",
    "\t]\n",
    "hf_manual = pd.read_csv(f\"./merged_after_manual/merged_hf_discussions.csv\")\n",
    "hf_manual = hf_manual[\n",
    "\t~(\n",
    "\t\t(hf_manual[\"title\"] == \"Adding `safetensors` variant of this model\") |\n",
    "\t\t(hf_manual[\"title\"] == \"Upload folder using huggingface_hub\")\n",
    "\t)\n",
    "]\n",
    "hf_security_sampled = get_subset(hf_security, hf_manual, [\"model_id\", \"num\"])\n",
    "hf_security_sampled.to_csv(f\"./sampled/{expr}_sampled.csv\", index=False)"
   ],
   "id": "cca5a8d136564df3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T11:58:38.050882Z",
     "start_time": "2025-03-12T11:58:34.946802Z"
    }
   },
   "cell_type": "code",
   "source": [
    "expr = \"all_hf_secbert_tuned_all_9_train\"\n",
    "hf = pd.read_csv(f\"./inference/{expr}/raw_predictions.csv\")\n",
    "print(\"Total \",len(hf))\n",
    "print(\"Total class 1 \",len(hf[hf[\"is_security_prediction\"] == 1]))\n",
    "hf = hf[\n",
    "\t~(\n",
    "\t\t(hf[\"title\"] == \"Adding `safetensors` variant of this model\") |\n",
    "\t\t(hf[\"title\"] == \"Upload folder using huggingface_hub\")\n",
    "\t)\n",
    "]\n",
    "print(\"Filtered \", len(hf))\n",
    "hf = hf[hf[\"is_security_prediction\"] == 1]\n",
    "print(\"Filtered class 1 \", len(hf))\n",
    "# hf.to_csv(\"./sampled/bert_hf_test.csv\", index=False)"
   ],
   "id": "1ccc56d6ebfca2eb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total  666055\n",
      "Total class 1  220137\n",
      "Filtered  382782\n",
      "Filtered class 1  67383\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# find overlapping results",
   "id": "94c01211e1abcfd8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# # gh\n",
    "# # keep all the duplicates rows where is_security_prediction is 1, drop the rest\n",
    "# gh_dfs = [\n",
    "# \tpd.read_csv(f\"./inference/{folder}/raw_predictions.csv\")\n",
    "# \tfor folder in [\n",
    "# \t\t\"all_gh_roberta_tuned_all\",\n",
    "# \t\t\"all_gh_bert_tuned_all\",\n",
    "# \t\t\"all_gh_distilbert_tuned_all\",\n",
    "# \t\t\"all_gh_securebert_tuned_all\",\n",
    "# \t\t\"all_gh_secbert_tuned_all\",\n",
    "# \t\t\"all_gh_secroberta_tuned_all\",\n",
    "# \t]\n",
    "# ]\n",
    "# \n",
    "# for df in gh_dfs:\n",
    "# \tdf = df[df[\"is_security_prediction\"] == 1]\n",
    "# \tprint(len(df))\n",
    "# \n",
    "# # duplicated = gh_df.duplicated(subset=[\"repo_name\", \"discussion_number\", \"is_security_prediction\"], keep=\"first\")\n",
    "# # duplicated = gh_df.duplicated()\n",
    "# \n",
    "# gh_df = (\n",
    "# \tgh_dfs[0]\n",
    "# \t.merge(gh_dfs[1], on=[\"repo_name\", \"discussion_number\", \"is_security_prediction\"], how=\"inner\")\n",
    "# \t.merge(gh_dfs[2], on=[\"repo_name\", \"discussion_number\", \"is_security_prediction\"], how=\"inner\")\n",
    "# \t.merge(gh_dfs[3], on=[\"repo_name\", \"discussion_number\", \"is_security_prediction\"], how=\"inner\")\n",
    "# \t.merge(gh_dfs[4], on=[\"repo_name\", \"discussion_number\", \"is_security_prediction\"], how=\"inner\")\n",
    "# \t.merge(gh_dfs[5], on=[\"repo_name\", \"discussion_number\", \"is_security_prediction\"], how=\"inner\")\n",
    "# )\n",
    "# \n",
    "# gh_df = gh_df.drop_duplicates(\n",
    "# \tsubset=[\"repo_name\", \"discussion_number\", \"is_security_prediction\"],\n",
    "# \tkeep=\"first\"\n",
    "# )\n",
    "# # gh_df = gh_df[duplicated].drop_duplicates()\n",
    "# print(len(gh_df))\n",
    "# gh_df = gh_df[gh_df[\"is_security_prediction\"] == 1]\n",
    "# print(len(gh_df))\n",
    "# gh_df.to_csv(\"./overlap/gh.csv\", index=False)\n",
    "# gh_df.head()"
   ],
   "id": "2e0b1781c13f25f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T04:19:34.690756Z",
     "start_time": "2025-03-11T04:19:34.688171Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_overlap(folders, columns, export_to):\n",
    "\tdfs = [\n",
    "\t\tpd.read_csv(f\"./inference/{folder}/raw_predictions.csv\")\n",
    "\t\tfor folder in folders\n",
    "\t]\n",
    "\tdf = pd.concat(dfs)\n",
    "\tprint(f\"{len(df)=}\")\n",
    "\tdf = df[df[\"is_security_prediction\"] == 1]\n",
    "\tprint(f\"Security only {len(df)=}\")\n",
    "\tdf_counts = df.value_counts(columns, dropna=False)\n",
    "\tprint(f\"Unique value counts over columns {len(df_counts)=}\")\n",
    "\tdf_counts = df_counts[df_counts >= len(folders)].reset_index()\n",
    "\tprint(f\"Filtered value counts over columns {len(df_counts)=}\")\n",
    "\tdf_counts.drop_duplicates(subset=columns, inplace=True)\n",
    "\tprint(f\"Drop duplicated over columns {len(df_counts)=}\")\n",
    "\tdf_counts.to_csv(export_to, index=False)\n",
    "\treturn df_counts"
   ],
   "id": "408d85c00cdd2973",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T04:24:20.321082Z",
     "start_time": "2025-03-11T04:24:11.996671Z"
    }
   },
   "cell_type": "code",
   "source": [
    "gh_overlap = get_overlap(\n",
    "\tfolders=[\n",
    "\t\t\"all_gh_roberta_tuned_all\",\n",
    "\t\t\"all_gh_bert_tuned_all\",\n",
    "\t\t\"all_gh_distilbert_tuned_all\",\n",
    "\t\t\"all_gh_securebert_tuned_all\",\n",
    "\t\t\"all_gh_secbert_tuned_all\",\n",
    "\t\t\"all_gh_secroberta_tuned_all\",\n",
    "\t],\n",
    "\tcolumns=[\n",
    "\t\t\"repo_name\", \"discussion_number\", \"discussion_title\", \"discussion_body\", \"author_login_x\",\n",
    "\t\t\"author_login_y\", \"comment_body\", \"full_comment\", \"is_security_prediction\"\n",
    "\t],\n",
    "\texport_to=\"./overlap/gh_no_filter.csv\",\n",
    ")\n",
    "print(len(gh_overlap))\n",
    "\n",
    "gh_overlap = gh_overlap[\n",
    "\t~(\n",
    "\t\t(gh_overlap[\"repo_name\"] == \"prometheus/prometheus\") |\n",
    "\t\t(gh_overlap[\"repo_name\"] == \"git-lfs/git-lfs\")\n",
    "\t)\n",
    "]\n",
    "\n",
    "print(len(gh_overlap))\n",
    "gh_overlap.sort_values(by=[\"repo_name\", \"discussion_number\"], inplace=True)\n",
    "gh_overlap.to_csv(\"./overlap/gh.csv\", index=False)\n",
    "gh_overlap.head(5)"
   ],
   "id": "cfd48502ce045223",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(df)=287412\n",
      "Security only len(df)=35541\n",
      "Unique value counts over columns len(df_counts)=17108\n",
      "Filtered value counts over columns len(df_counts)=432\n",
      "Drop duplicated over columns len(df_counts)=432\n",
      "432\n",
      "325\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                                          repo_name  discussion_number  \\\n",
       "308                                    ACEsuit/mace                337   \n",
       "431                          Akegarasu/lora-scripts                323   \n",
       "385  AlUlkesh/stable-diffusion-webui-images-browser                 44   \n",
       "5                                 Bing-su/adetailer                470   \n",
       "50                                Bing-su/adetailer                470   \n",
       "\n",
       "                                      discussion_title  \\\n",
       "308                     ML-MACE enabled LAMMPS install   \n",
       "431                       Run lora-scripts on cpu only   \n",
       "385  Please rebuild cache if you use prompts with n...   \n",
       "5                                      Unsafe Files???   \n",
       "50                                     Unsafe Files???   \n",
       "\n",
       "                                       discussion_body  author_login_x  \\\n",
       "308  Dear users,\\nI am running into C++ compiler co...  sumanbhasker89   \n",
       "431  Is there any way to install lora-scripts on a ...         brcisna   \n",
       "385  We just found a bug, where only the last line ...        AlUlkesh   \n",
       "5    What's up with Huggingface saying 5 Adetailer ...       gohan2091   \n",
       "50   What's up with Huggingface saying 5 Adetailer ...       gohan2091   \n",
       "\n",
       "    author_login_y                                       comment_body  \\\n",
       "308         wcwitt  Hi, apologies, but we don't have tons of exper...   \n",
       "431            NaN                                                NaN   \n",
       "385            NaN                                                NaN   \n",
       "5          Bing-su  https://huggingface.co/docs/hub/security-pickl...   \n",
       "50       gohan2091  Does that answer my question though? Is it say...   \n",
       "\n",
       "                                          full_comment  \\\n",
       "308  ML-MACE enabled LAMMPS install Dear users,\\nI ...   \n",
       "431  Run lora-scripts on cpu only Is there any way ...   \n",
       "385  Please rebuild cache if you use prompts with n...   \n",
       "5    Unsafe Files??? What's up with Huggingface say...   \n",
       "50   Unsafe Files??? What's up with Huggingface say...   \n",
       "\n",
       "     is_security_prediction  count  \n",
       "308                     1.0      6  \n",
       "431                     1.0      6  \n",
       "385                     1.0      6  \n",
       "5                       1.0      6  \n",
       "50                      1.0      6  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>repo_name</th>\n",
       "      <th>discussion_number</th>\n",
       "      <th>discussion_title</th>\n",
       "      <th>discussion_body</th>\n",
       "      <th>author_login_x</th>\n",
       "      <th>author_login_y</th>\n",
       "      <th>comment_body</th>\n",
       "      <th>full_comment</th>\n",
       "      <th>is_security_prediction</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>ACEsuit/mace</td>\n",
       "      <td>337</td>\n",
       "      <td>ML-MACE enabled LAMMPS install</td>\n",
       "      <td>Dear users,\\nI am running into C++ compiler co...</td>\n",
       "      <td>sumanbhasker89</td>\n",
       "      <td>wcwitt</td>\n",
       "      <td>Hi, apologies, but we don't have tons of exper...</td>\n",
       "      <td>ML-MACE enabled LAMMPS install Dear users,\\nI ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431</th>\n",
       "      <td>Akegarasu/lora-scripts</td>\n",
       "      <td>323</td>\n",
       "      <td>Run lora-scripts on cpu only</td>\n",
       "      <td>Is there any way to install lora-scripts on a ...</td>\n",
       "      <td>brcisna</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Run lora-scripts on cpu only Is there any way ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385</th>\n",
       "      <td>AlUlkesh/stable-diffusion-webui-images-browser</td>\n",
       "      <td>44</td>\n",
       "      <td>Please rebuild cache if you use prompts with n...</td>\n",
       "      <td>We just found a bug, where only the last line ...</td>\n",
       "      <td>AlUlkesh</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Please rebuild cache if you use prompts with n...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Bing-su/adetailer</td>\n",
       "      <td>470</td>\n",
       "      <td>Unsafe Files???</td>\n",
       "      <td>What's up with Huggingface saying 5 Adetailer ...</td>\n",
       "      <td>gohan2091</td>\n",
       "      <td>Bing-su</td>\n",
       "      <td>https://huggingface.co/docs/hub/security-pickl...</td>\n",
       "      <td>Unsafe Files??? What's up with Huggingface say...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Bing-su/adetailer</td>\n",
       "      <td>470</td>\n",
       "      <td>Unsafe Files???</td>\n",
       "      <td>What's up with Huggingface saying 5 Adetailer ...</td>\n",
       "      <td>gohan2091</td>\n",
       "      <td>gohan2091</td>\n",
       "      <td>Does that answer my question though? Is it say...</td>\n",
       "      <td>Unsafe Files??? What's up with Huggingface say...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T04:43:40.857817Z",
     "start_time": "2025-03-11T04:43:19.490530Z"
    }
   },
   "cell_type": "code",
   "source": [
    "hf_overlap = get_overlap(\n",
    "\tfolders=[\n",
    "\t\t\"all_hf_securebert_tuned_all\",\n",
    "\t\t\"all_hf_distilbert_tuned_all\",\n",
    "\t\t\"all_hf_bert_tuned_all\",\n",
    "\t\t\"all_hf_roberta_tuned_all\",\n",
    "\t\t\"all_hf_secroberta_tuned_all\",\n",
    "\t\t\"all_hf_secbert_tuned_all\",\n",
    "\t],\n",
    "\tcolumns=[\n",
    "\t\t\"model_id\", \"num\", \"title\", \"git_ref\", \"url\",\n",
    "\t\t# \"event_id\",\n",
    "\t\t\"event_type\", \n",
    "\t\t\"content\", \"full_comment\", \"is_security_prediction\",\n",
    "\t],\n",
    "\texport_to=\"./overlap/hf_no_filter.csv\",\n",
    ")\n",
    "# hf further removal of safetensor and bot commit\n",
    "hf_overlap = hf_overlap[\n",
    "\t~(\n",
    "\t\t(hf_overlap[\"full_comment\"].str.contains(\"Adding `safetensors` variant of this model\")) |\n",
    "\t\t(hf_overlap[\"full_comment\"].str.contains(\"Upload folder using huggingface_hub\")) |\n",
    "\t\t(hf_overlap[\"full_comment\"].str.contains((\"license|licenses|License|Licenses\"), na=False, regex=True)) |\n",
    "\t\t(hf_overlap[\"event_type\"] != \"comment\")\n",
    "\t)\n",
    "]\n",
    "print(len(hf_overlap))\n",
    "hf_overlap.sort_values(by=[\"model_id\", \"num\"], inplace=True)\n",
    "hf_overlap.to_csv(\"./overlap/hf.csv\", index=False)"
   ],
   "id": "913f71dc5bdbf5d9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(df)=3996330\n",
      "Security only len(df)=1400222\n",
      "Unique value counts over columns len(df_counts)=403918\n",
      "Filtered value counts over columns len(df_counts)=155853\n",
      "Drop duplicated over columns len(df_counts)=155853\n",
      "2180\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T04:43:40.929339Z",
     "start_time": "2025-03-11T04:43:40.924016Z"
    }
   },
   "cell_type": "code",
   "source": "hf_overlap",
   "id": "87d38b37fa2903ca",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                            model_id  num  \\\n",
       "103847                        0dAI/0dAI-7.5B-v2-4bpw    1   \n",
       "103802               0xJustin/Dungeons-and-Diffusion    8   \n",
       "103801               0xJustin/Dungeons-and-Diffusion   16   \n",
       "104547                         152334H/miqu-1-70b-sf   13   \n",
       "104755                       1bitLLM/bitnet_b1_58-3B    3   \n",
       "...                                              ...  ...   \n",
       "104197           zuzhe/Ancient-Chinese-head-portrait    1   \n",
       "104146                         zuzhe/Chinese-wedding    2   \n",
       "104215                             zuzhe/Mecha-model    1   \n",
       "104042  zyh3826/llama2-13b-ft-openllm-leaderboard-v1    5   \n",
       "104053  zyh3826/llama2-13b-ft-openllm-leaderboard-v1    5   \n",
       "\n",
       "                                      title    git_ref  \\\n",
       "103847                    Files are missing        NaN   \n",
       "103802                  Safetensor versions  refs/pr/8   \n",
       "103801            Which .ckpt-files to use?        NaN   \n",
       "104547                      Model load fail        NaN   \n",
       "104755  lm_head is missing in *.safetensors        NaN   \n",
       "...                                     ...        ...   \n",
       "104197              Add safetensors variant  refs/pr/1   \n",
       "104146              Add safetensors variant  refs/pr/2   \n",
       "104215              Add safetensors variant  refs/pr/1   \n",
       "104042  Upload model.safetensors.index.json  refs/pr/5   \n",
       "104053  Upload model.safetensors.index.json  refs/pr/5   \n",
       "\n",
       "                                                      url event_type  \\\n",
       "103847  https://huggingface.co/0dAI/0dAI-7.5B-v2-4bpw/...    comment   \n",
       "103802  https://huggingface.co/0xJustin/Dungeons-and-D...    comment   \n",
       "103801  https://huggingface.co/0xJustin/Dungeons-and-D...    comment   \n",
       "104547  https://huggingface.co/152334H/miqu-1-70b-sf/d...    comment   \n",
       "104755  https://huggingface.co/1bitLLM/bitnet_b1_58-3B...    comment   \n",
       "...                                                   ...        ...   \n",
       "104197  https://huggingface.co/zuzhe/Ancient-Chinese-h...    comment   \n",
       "104146  https://huggingface.co/zuzhe/Chinese-wedding/d...    comment   \n",
       "104215  https://huggingface.co/zuzhe/Mecha-model/discu...    comment   \n",
       "104042  https://huggingface.co/zyh3826/llama2-13b-ft-o...    comment   \n",
       "104053  https://huggingface.co/zyh3826/llama2-13b-ft-o...    comment   \n",
       "\n",
       "                                                  content  \\\n",
       "103847  En el `model.safetensors.index.json` se indica...   \n",
       "103802  Could someone make safetensor versions of each...   \n",
       "103801  By default, use D&Diffusion3.0_Protogen-fp32.s...   \n",
       "104547                            pip install safetensors   \n",
       "104755   I did not notice the tie_word_embedding is True.   \n",
       "...                                                   ...   \n",
       "104197                                                NaN   \n",
       "104146                                                NaN   \n",
       "104215                                                NaN   \n",
       "104042                                                NaN   \n",
       "104053                       model.safetensors.index.json   \n",
       "\n",
       "                                             full_comment  \\\n",
       "103847  Files are missing En el `model.safetensors.ind...   \n",
       "103802  Safetensor versions Could someone make safeten...   \n",
       "103801  Which .ckpt-files to use? By default, use D&Di...   \n",
       "104547            Model load fail pip install safetensors   \n",
       "104755  lm_head is missing in *.safetensors I did not ...   \n",
       "...                                                   ...   \n",
       "104197                           Add safetensors variant    \n",
       "104146                           Add safetensors variant    \n",
       "104215                           Add safetensors variant    \n",
       "104042               Upload model.safetensors.index.json    \n",
       "104053  Upload model.safetensors.index.json model.safe...   \n",
       "\n",
       "        is_security_prediction  count  \n",
       "103847                     1.0      6  \n",
       "103802                     1.0      6  \n",
       "103801                     1.0      6  \n",
       "104547                     1.0      6  \n",
       "104755                     1.0      6  \n",
       "...                        ...    ...  \n",
       "104197                     1.0      6  \n",
       "104146                     1.0      6  \n",
       "104215                     1.0      6  \n",
       "104042                     1.0      6  \n",
       "104053                     1.0      6  \n",
       "\n",
       "[2180 rows x 10 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_id</th>\n",
       "      <th>num</th>\n",
       "      <th>title</th>\n",
       "      <th>git_ref</th>\n",
       "      <th>url</th>\n",
       "      <th>event_type</th>\n",
       "      <th>content</th>\n",
       "      <th>full_comment</th>\n",
       "      <th>is_security_prediction</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>103847</th>\n",
       "      <td>0dAI/0dAI-7.5B-v2-4bpw</td>\n",
       "      <td>1</td>\n",
       "      <td>Files are missing</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://huggingface.co/0dAI/0dAI-7.5B-v2-4bpw/...</td>\n",
       "      <td>comment</td>\n",
       "      <td>En el `model.safetensors.index.json` se indica...</td>\n",
       "      <td>Files are missing En el `model.safetensors.ind...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103802</th>\n",
       "      <td>0xJustin/Dungeons-and-Diffusion</td>\n",
       "      <td>8</td>\n",
       "      <td>Safetensor versions</td>\n",
       "      <td>refs/pr/8</td>\n",
       "      <td>https://huggingface.co/0xJustin/Dungeons-and-D...</td>\n",
       "      <td>comment</td>\n",
       "      <td>Could someone make safetensor versions of each...</td>\n",
       "      <td>Safetensor versions Could someone make safeten...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103801</th>\n",
       "      <td>0xJustin/Dungeons-and-Diffusion</td>\n",
       "      <td>16</td>\n",
       "      <td>Which .ckpt-files to use?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://huggingface.co/0xJustin/Dungeons-and-D...</td>\n",
       "      <td>comment</td>\n",
       "      <td>By default, use D&amp;Diffusion3.0_Protogen-fp32.s...</td>\n",
       "      <td>Which .ckpt-files to use? By default, use D&amp;Di...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104547</th>\n",
       "      <td>152334H/miqu-1-70b-sf</td>\n",
       "      <td>13</td>\n",
       "      <td>Model load fail</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://huggingface.co/152334H/miqu-1-70b-sf/d...</td>\n",
       "      <td>comment</td>\n",
       "      <td>pip install safetensors</td>\n",
       "      <td>Model load fail pip install safetensors</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104755</th>\n",
       "      <td>1bitLLM/bitnet_b1_58-3B</td>\n",
       "      <td>3</td>\n",
       "      <td>lm_head is missing in *.safetensors</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://huggingface.co/1bitLLM/bitnet_b1_58-3B...</td>\n",
       "      <td>comment</td>\n",
       "      <td>I did not notice the tie_word_embedding is True.</td>\n",
       "      <td>lm_head is missing in *.safetensors I did not ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104197</th>\n",
       "      <td>zuzhe/Ancient-Chinese-head-portrait</td>\n",
       "      <td>1</td>\n",
       "      <td>Add safetensors variant</td>\n",
       "      <td>refs/pr/1</td>\n",
       "      <td>https://huggingface.co/zuzhe/Ancient-Chinese-h...</td>\n",
       "      <td>comment</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Add safetensors variant</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104146</th>\n",
       "      <td>zuzhe/Chinese-wedding</td>\n",
       "      <td>2</td>\n",
       "      <td>Add safetensors variant</td>\n",
       "      <td>refs/pr/2</td>\n",
       "      <td>https://huggingface.co/zuzhe/Chinese-wedding/d...</td>\n",
       "      <td>comment</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Add safetensors variant</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104215</th>\n",
       "      <td>zuzhe/Mecha-model</td>\n",
       "      <td>1</td>\n",
       "      <td>Add safetensors variant</td>\n",
       "      <td>refs/pr/1</td>\n",
       "      <td>https://huggingface.co/zuzhe/Mecha-model/discu...</td>\n",
       "      <td>comment</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Add safetensors variant</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104042</th>\n",
       "      <td>zyh3826/llama2-13b-ft-openllm-leaderboard-v1</td>\n",
       "      <td>5</td>\n",
       "      <td>Upload model.safetensors.index.json</td>\n",
       "      <td>refs/pr/5</td>\n",
       "      <td>https://huggingface.co/zyh3826/llama2-13b-ft-o...</td>\n",
       "      <td>comment</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Upload model.safetensors.index.json</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104053</th>\n",
       "      <td>zyh3826/llama2-13b-ft-openllm-leaderboard-v1</td>\n",
       "      <td>5</td>\n",
       "      <td>Upload model.safetensors.index.json</td>\n",
       "      <td>refs/pr/5</td>\n",
       "      <td>https://huggingface.co/zyh3826/llama2-13b-ft-o...</td>\n",
       "      <td>comment</td>\n",
       "      <td>model.safetensors.index.json</td>\n",
       "      <td>Upload model.safetensors.index.json model.safe...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2180 rows × 10 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T04:23:47.084673Z",
     "start_time": "2025-03-11T04:19:41.153364Z"
    }
   },
   "cell_type": "code",
   "source": [
    "gh_issues_overlap = get_overlap(\n",
    "\tfolders=[\n",
    "\t\t\"all_gh_issue_secroberta_tuned_all\",\n",
    "\t\t\"all_gh_issue_distilbert_tuned_all\",\n",
    "\t\t\"all_gh_issue_bert_tuned_all\",\n",
    "\t\t\"all_gh_issue_secbert_tuned_all\",\n",
    "\t\t\"all_gh_issue_roberta_tuned_all\",\n",
    "\t\t\"all_gh_issue_securebert_tuned_all\",\n",
    "\t],\n",
    "\tcolumns=[\n",
    "\t\t\"repo_name\", \"issue_url\", \"issue_title\", \"issue_body\", \"pr_from_issue\",\n",
    "\t\t\"user_login\", \"issue_number\", \"full_comment\", \"is_security_prediction\",\n",
    "\t],\n",
    "\texport_to=\"./overlap/gh_issues_no_filtered.csv\",\n",
    ")\n",
    "\n",
    "gh_issues_overlap.head(5)\n",
    "# further processing to remove unrelated repos\n",
    "gh_issues_overlap = gh_issues_overlap[\n",
    "\t~(\n",
    "\t\t(gh_issues_overlap[\"repo_name\"] == \"microsoft/vscode\") |\n",
    "\t\t(gh_issues_overlap[\"repo_name\"] == \"MicrosoftDocs/azure-docs\")\n",
    "\t)\n",
    "]\n",
    "print(len(gh_issues_overlap))\n",
    "gh_issues_overlap.sort_values(by=[\"repo_name\", \"issue_number\"], inplace=True)\n",
    "gh_issues_overlap.to_csv(\"./overlap/gh_issues.csv\", index=False)"
   ],
   "id": "b11ae3d0a2d00975",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(df)=11968620\n",
      "Security only len(df)=3438554\n",
      "Unique value counts over columns len(df_counts)=1327938\n",
      "Filtered value counts over columns len(df_counts)=47721\n",
      "Drop duplicated over columns len(df_counts)=47721\n",
      "35782\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T04:43:53.880461Z",
     "start_time": "2025-03-11T04:43:46.604774Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with pd.ExcelWriter(\"./overlap/gh.xlsx\", engine='xlsxwriter',\n",
    "\t\t\t\t\tengine_kwargs={\"options\": {\"strings_to_urls\": False}}) as writer:\n",
    "\tgh_overlap.to_excel(writer)\n",
    "\n",
    "hf_overlap = hf_overlap.applymap(lambda x: x.encode('unicode_escape').decode('utf-8') if isinstance(x, str) else x)\n",
    "with pd.ExcelWriter(\n",
    "\t\"./overlap/hf.xlsx\",\n",
    "\tengine='xlsxwriter',\n",
    "\tengine_kwargs={\n",
    "\t\t\"options\": {\"strings_to_urls\": False, \"encoding\": \"utf-8\"},\n",
    "\t},\n",
    ") as writer:\n",
    "\thf_overlap.to_excel(writer)\n",
    "\n",
    "gh_issues_overlap = gh_issues_overlap.applymap(\n",
    "\tlambda x: x.encode('unicode_escape').decode('utf-8') if isinstance(x, str) else x)\n",
    "with pd.ExcelWriter(\n",
    "\t\"./overlap/gh_issues.xlsx\",\n",
    "\tengine='xlsxwriter',\n",
    "\tengine_kwargs={\"options\": {\"strings_to_urls\": False, \"encoding\": \"utf-8\"}}\n",
    ") as writer:\n",
    "\tgh_issues_overlap.to_excel(writer)"
   ],
   "id": "c93423971382f793",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1236567/3355115915.py:5: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  hf_overlap = hf_overlap.applymap(lambda x: x.encode('unicode_escape').decode('utf-8') if isinstance(x, str) else x)\n",
      "/tmp/ipykernel_1236567/3355115915.py:15: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  gh_issues_overlap = gh_issues_overlap.applymap(\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "f4501a58541a42a5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T01:42:38.113637Z",
     "start_time": "2025-03-17T01:42:38.107996Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_99_done = pd.read_csv(\"./test/test_99_done.csv\")\n",
    "test_99_done = test_99_done.sort_values(by=[\"id_name\", \"id_num\"])\n",
    "test_99_done.to_csv(\"./test/test_99_done.csv\", index=False)"
   ],
   "id": "f10d275fdfcadd98",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# TEST SET LOOKUP",
   "id": "6507684db3e2503"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T23:53:26.930608Z",
     "start_time": "2025-03-16T23:53:09.430303Z"
    }
   },
   "cell_type": "code",
   "source": "merged_all = pd.read_csv(\"./merged/merged_all.csv\")",
   "id": "39131a01197786fb",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T01:43:26.236092Z",
     "start_time": "2025-03-17T01:43:26.232742Z"
    }
   },
   "cell_type": "code",
   "source": "test_99_done = pd.read_csv(\"./test/test_99_done.csv\")",
   "id": "b0733c235f7d47ad",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T01:43:29.616665Z",
     "start_time": "2025-03-17T01:43:29.614487Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(len(test_99_done))\n",
    "print(len(merged_all))"
   ],
   "id": "353929b415ef3df5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99\n",
      "2708727\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T01:43:34.632777Z",
     "start_time": "2025-03-17T01:43:34.013616Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_full = merged_all[\n",
    "    merged_all.set_index([\"id_name\", \"id_num\", \"type\"]).index.isin(\n",
    "        test_99_done.set_index([\"id_name\", \"id_num\", \"type\"]).index\n",
    "    )\n",
    "]\n",
    "test_full.sort_values(by=[\"id_name\", \"id_num\"], inplace=True)\n",
    "test_full"
   ],
   "id": "126877361a43c431",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2706032/1854985838.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_full.sort_values(by=[\"id_name\", \"id_num\"], inplace=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                                                id_name  id_num  \\\n",
       "778779   AlUlkesh/stable-diffusion-webui-images-browser     245   \n",
       "796045   AlUlkesh/stable-diffusion-webui-images-browser     245   \n",
       "2505503  AlUlkesh/stable-diffusion-webui-images-browser     245   \n",
       "915716                                 AlexeyAB/darknet    5520   \n",
       "1053928                              ArrowLuo/CLIP4Clip      91   \n",
       "...                                                 ...     ...   \n",
       "2077657                                yl4579/StyleTTS2     110   \n",
       "2211049                                yl4579/StyleTTS2     110   \n",
       "2294254                                yl4579/StyleTTS2     110   \n",
       "2345942                                yl4579/StyleTTS2     110   \n",
       "2400407                                yl4579/StyleTTS2     110   \n",
       "\n",
       "                   type                                            content  \n",
       "778779   GH_DISCUSSIONS  Gradio 4 As you might be aware, a1111 is worki...  \n",
       "796045   GH_DISCUSSIONS  Gradio 4 As you might be aware, a1111 is worki...  \n",
       "2505503  GH_DISCUSSIONS  Gradio 4 As you might be aware, a1111 is worki...  \n",
       "915716        GH_ISSUES                          buffer overflow detected   \n",
       "1053928       GH_ISSUES  CVE-2007-4559 Patch # Patching CVE-2007-4559\\n...  \n",
       "...                 ...                                                ...  \n",
       "2077657  GH_DISCUSSIONS  Gradio demo HI, congrats on StyleTT2, would be...  \n",
       "2211049  GH_DISCUSSIONS  Gradio demo HI, congrats on StyleTT2, would be...  \n",
       "2294254  GH_DISCUSSIONS  Gradio demo HI, congrats on StyleTT2, would be...  \n",
       "2345942  GH_DISCUSSIONS  Gradio demo HI, congrats on StyleTT2, would be...  \n",
       "2400407  GH_DISCUSSIONS  Gradio demo HI, congrats on StyleTT2, would be...  \n",
       "\n",
       "[425 rows x 4 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_name</th>\n",
       "      <th>id_num</th>\n",
       "      <th>type</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>778779</th>\n",
       "      <td>AlUlkesh/stable-diffusion-webui-images-browser</td>\n",
       "      <td>245</td>\n",
       "      <td>GH_DISCUSSIONS</td>\n",
       "      <td>Gradio 4 As you might be aware, a1111 is worki...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>796045</th>\n",
       "      <td>AlUlkesh/stable-diffusion-webui-images-browser</td>\n",
       "      <td>245</td>\n",
       "      <td>GH_DISCUSSIONS</td>\n",
       "      <td>Gradio 4 As you might be aware, a1111 is worki...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2505503</th>\n",
       "      <td>AlUlkesh/stable-diffusion-webui-images-browser</td>\n",
       "      <td>245</td>\n",
       "      <td>GH_DISCUSSIONS</td>\n",
       "      <td>Gradio 4 As you might be aware, a1111 is worki...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>915716</th>\n",
       "      <td>AlexeyAB/darknet</td>\n",
       "      <td>5520</td>\n",
       "      <td>GH_ISSUES</td>\n",
       "      <td>buffer overflow detected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1053928</th>\n",
       "      <td>ArrowLuo/CLIP4Clip</td>\n",
       "      <td>91</td>\n",
       "      <td>GH_ISSUES</td>\n",
       "      <td>CVE-2007-4559 Patch # Patching CVE-2007-4559\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2077657</th>\n",
       "      <td>yl4579/StyleTTS2</td>\n",
       "      <td>110</td>\n",
       "      <td>GH_DISCUSSIONS</td>\n",
       "      <td>Gradio demo HI, congrats on StyleTT2, would be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2211049</th>\n",
       "      <td>yl4579/StyleTTS2</td>\n",
       "      <td>110</td>\n",
       "      <td>GH_DISCUSSIONS</td>\n",
       "      <td>Gradio demo HI, congrats on StyleTT2, would be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2294254</th>\n",
       "      <td>yl4579/StyleTTS2</td>\n",
       "      <td>110</td>\n",
       "      <td>GH_DISCUSSIONS</td>\n",
       "      <td>Gradio demo HI, congrats on StyleTT2, would be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2345942</th>\n",
       "      <td>yl4579/StyleTTS2</td>\n",
       "      <td>110</td>\n",
       "      <td>GH_DISCUSSIONS</td>\n",
       "      <td>Gradio demo HI, congrats on StyleTT2, would be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2400407</th>\n",
       "      <td>yl4579/StyleTTS2</td>\n",
       "      <td>110</td>\n",
       "      <td>GH_DISCUSSIONS</td>\n",
       "      <td>Gradio demo HI, congrats on StyleTT2, would be...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>425 rows × 4 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T01:43:39.650130Z",
     "start_time": "2025-03-17T01:43:39.643107Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(test_full[\"type\"].value_counts())\n",
    "test_full[\"is_security\"] = -1\n",
    "test_full.to_csv(\"./test/test_full.csv\", index=False)"
   ],
   "id": "1aaeb22784609753",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type\n",
      "HF_DISCUSSIONS    231\n",
      "GH_DISCUSSIONS    159\n",
      "GH_ISSUES          35\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2706032/1990146424.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_full[\"is_security\"] = -1\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T01:43:44.875987Z",
     "start_time": "2025-03-17T01:43:44.869044Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_full = pd.read_csv(\"./test/test_full.csv\")\n",
    "print(len(test_full[[\"id_num\",\"id_name\"]].drop_duplicates()))\n",
    "print(len(test_full))"
   ],
   "id": "1b4f3e1877e7b91b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99\n",
      "425\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-17T13:15:33.796201Z",
     "start_time": "2025-06-17T13:15:33.765387Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd \n",
    "test_full_done = pd.read_csv(\"./test/test_full_done.csv\")\n",
    "print(len(test_full_done[[\"id_num\",\"id_name\",\"is_security\"]].drop_duplicates()))\n",
    "print(len(test_full_done))\n",
    "\n",
    "print(test_full_done[[\"id_num\",\"id_name\",\"is_security\"]].drop_duplicates()[\"is_security\"].value_counts())\n",
    "print(test_full_done[[\"id_num\",\"id_name\",\"is_security\",\"type\"]].drop_duplicates().groupby(\"type\")[\"is_security\"].value_counts())"
   ],
   "id": "42585c11409600bf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98\n",
      "423\n",
      "is_security\n",
      "0    79\n",
      "1    19\n",
      "Name: count, dtype: int64\n",
      "type            is_security\n",
      "GH_DISCUSSIONS  0              27\n",
      "                1               3\n",
      "GH_ISSUES       0              26\n",
      "                1               9\n",
      "HF_DISCUSSIONS  0              26\n",
      "                1               7\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-31T00:17:28.460184Z",
     "start_time": "2025-03-31T00:17:27.962623Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd \n",
    "gh_distilbert_disc = pd.read_csv(\"./target/chosen/gh_distilbert_disc.csv\")\n",
    "\n",
    "gh_distilbert_disc = gh_distilbert_disc.applymap(lambda x: x.encode('unicode_escape').decode('utf-8') if isinstance(x, str) else x)\n",
    "with pd.ExcelWriter(\n",
    "\t\"./target/convert/gh_distilbert_disc.xlsx\",\n",
    "\tengine='xlsxwriter',\n",
    "\tengine_kwargs={\n",
    "\t\t\"options\": {\"strings_to_urls\": False, \"encoding\": \"utf-8\"},\n",
    "\t},\n",
    ") as writer:\n",
    "\tgh_distilbert_disc.to_excel(writer)\n"
   ],
   "id": "e9b9ae3e177aa845",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_543594/3037432682.py:4: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  gh_distilbert_disc = gh_distilbert_disc.applymap(lambda x: x.encode('unicode_escape').decode('utf-8') if isinstance(x, str) else x)\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "gh_distilbert_disc.csv",
   "id": "d8177f8e4d75aa52"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T03:27:19.626930Z",
     "start_time": "2025-04-07T03:26:28.767885Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "gh_issues_distilbert = pd.read_csv(\"./inference/all_gh_issue_distilbert_tuned_all_9_train/raw_predictions.csv\")\n",
    "gh_issue_kw = pd.read_csv(\"./manual/gh_issues_subset_3_done.csv\")\n",
    "gh_issue_kw = gh_issue_kw[gh_issue_kw[\"is_security\"] == 1]\n",
    "gh_issue_kw[\"repo_name_discussion_number\"] = gh_issue_kw[\"repo_name\"] + \"_\" + gh_issue_kw[\"issue_number\"].astype(str)\n",
    " \n",
    "gh_issues_distilbert_disc = gh_issues_distilbert.groupby([\"repo_name\", \"issue_number\", \"full_comment\",], as_index=False)[\"is_security_prediction\"].max()\n",
    "gh_issues_distilbert_disc = gh_issues_distilbert_disc[gh_issues_distilbert_disc[\"is_security_prediction\"] == 1]\n",
    "gh_issues_distilbert_disc[\"repo_name_discussion_number\"] = gh_issues_distilbert_disc[\"repo_name\"] + \"_\" + gh_issues_distilbert_disc[\"issue_number\"].astype(str)\n",
    "gh_issues_distilbert_disc[\"is_kw\"] = gh_issues_distilbert_disc[\"repo_name_discussion_number\"].isin(gh_issue_kw[\"repo_name_discussion_number\"]).astype(int)\n",
    "gh_issues_distilbert_disc[\"url\"] = \"https://github.com/\" + gh_issues_distilbert_disc[\"repo_name\"] + \"/issues/\" + gh_issues_distilbert_disc[\"issue_number\"].astype(str)\n",
    "\n",
    "gh_issues_distilbert_disc.to_csv(\"./target/gh_issues_distilbert_disc.csv\", index=False)\n",
    "gh_issues_distilbert_disc[gh_issues_distilbert_disc[\"is_kw\"] == 1].to_csv(\"./target/gh_issues_distilbert_disc_kw.csv\", index=False)\n",
    "print(len(gh_issues_distilbert_disc))\n",
    "gh_issues_distilbert_disc"
   ],
   "id": "6df5d1c59efd2626",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "605972\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                         repo_name  issue_number  \\\n",
       "3                         01-ai/Yi             2   \n",
       "4                         01-ai/Yi             3   \n",
       "74                        01-ai/Yi            78   \n",
       "79                        01-ai/Yi            83   \n",
       "82                        01-ai/Yi            86   \n",
       "...                            ...           ...   \n",
       "1994408  zzzDavid/ICDAR-2019-SROIE            20   \n",
       "1994414         zzzyuqing/DreamMat             4   \n",
       "1994422         zzzyuqing/DreamMat            12   \n",
       "1994424         zzzyuqing/DreamMat            14   \n",
       "1994440         zzzyuqing/DreamMat            30   \n",
       "\n",
       "                                              full_comment  \\\n",
       "3                                          Update license    \n",
       "4        Setup github action to build & push docker ima...   \n",
       "74       Missing pypi package in `requirements.txt` Got...   \n",
       "79                                   Update README.md typo   \n",
       "82       fix sft loss promlem according to discussion i...   \n",
       "...                                                    ...   \n",
       "1994408  building a .so files for CTPN method of task1 ...   \n",
       "1994414  No 'from_unet_inchannel' functions in ControlN...   \n",
       "1994422  Error during installing dependencies # Env\\r\\n...   \n",
       "1994424  CUDA not found in docker image I pulled the of...   \n",
       "1994440  fix validation_step && test_step bug During va...   \n",
       "\n",
       "         is_security_prediction   repo_name_discussion_number  is_kw  \\\n",
       "3                           1.0                    01-ai/Yi_2      0   \n",
       "4                           1.0                    01-ai/Yi_3      0   \n",
       "74                          1.0                   01-ai/Yi_78      0   \n",
       "79                          1.0                   01-ai/Yi_83      0   \n",
       "82                          1.0                   01-ai/Yi_86      0   \n",
       "...                         ...                           ...    ...   \n",
       "1994408                     1.0  zzzDavid/ICDAR-2019-SROIE_20      0   \n",
       "1994414                     1.0          zzzyuqing/DreamMat_4      0   \n",
       "1994422                     1.0         zzzyuqing/DreamMat_12      0   \n",
       "1994424                     1.0         zzzyuqing/DreamMat_14      0   \n",
       "1994440                     1.0         zzzyuqing/DreamMat_30      0   \n",
       "\n",
       "                                                       url  \n",
       "3                     https://github.com/01-ai/Yi/issues/2  \n",
       "4                     https://github.com/01-ai/Yi/issues/3  \n",
       "74                   https://github.com/01-ai/Yi/issues/78  \n",
       "79                   https://github.com/01-ai/Yi/issues/83  \n",
       "82                   https://github.com/01-ai/Yi/issues/86  \n",
       "...                                                    ...  \n",
       "1994408  https://github.com/zzzDavid/ICDAR-2019-SROIE/i...  \n",
       "1994414     https://github.com/zzzyuqing/DreamMat/issues/4  \n",
       "1994422    https://github.com/zzzyuqing/DreamMat/issues/12  \n",
       "1994424    https://github.com/zzzyuqing/DreamMat/issues/14  \n",
       "1994440    https://github.com/zzzyuqing/DreamMat/issues/30  \n",
       "\n",
       "[605972 rows x 7 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>repo_name</th>\n",
       "      <th>issue_number</th>\n",
       "      <th>full_comment</th>\n",
       "      <th>is_security_prediction</th>\n",
       "      <th>repo_name_discussion_number</th>\n",
       "      <th>is_kw</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01-ai/Yi</td>\n",
       "      <td>2</td>\n",
       "      <td>Update license</td>\n",
       "      <td>1.0</td>\n",
       "      <td>01-ai/Yi_2</td>\n",
       "      <td>0</td>\n",
       "      <td>https://github.com/01-ai/Yi/issues/2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01-ai/Yi</td>\n",
       "      <td>3</td>\n",
       "      <td>Setup github action to build &amp; push docker ima...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>01-ai/Yi_3</td>\n",
       "      <td>0</td>\n",
       "      <td>https://github.com/01-ai/Yi/issues/3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>01-ai/Yi</td>\n",
       "      <td>78</td>\n",
       "      <td>Missing pypi package in `requirements.txt` Got...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>01-ai/Yi_78</td>\n",
       "      <td>0</td>\n",
       "      <td>https://github.com/01-ai/Yi/issues/78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>01-ai/Yi</td>\n",
       "      <td>83</td>\n",
       "      <td>Update README.md typo</td>\n",
       "      <td>1.0</td>\n",
       "      <td>01-ai/Yi_83</td>\n",
       "      <td>0</td>\n",
       "      <td>https://github.com/01-ai/Yi/issues/83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>01-ai/Yi</td>\n",
       "      <td>86</td>\n",
       "      <td>fix sft loss promlem according to discussion i...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>01-ai/Yi_86</td>\n",
       "      <td>0</td>\n",
       "      <td>https://github.com/01-ai/Yi/issues/86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1994408</th>\n",
       "      <td>zzzDavid/ICDAR-2019-SROIE</td>\n",
       "      <td>20</td>\n",
       "      <td>building a .so files for CTPN method of task1 ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>zzzDavid/ICDAR-2019-SROIE_20</td>\n",
       "      <td>0</td>\n",
       "      <td>https://github.com/zzzDavid/ICDAR-2019-SROIE/i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1994414</th>\n",
       "      <td>zzzyuqing/DreamMat</td>\n",
       "      <td>4</td>\n",
       "      <td>No 'from_unet_inchannel' functions in ControlN...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>zzzyuqing/DreamMat_4</td>\n",
       "      <td>0</td>\n",
       "      <td>https://github.com/zzzyuqing/DreamMat/issues/4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1994422</th>\n",
       "      <td>zzzyuqing/DreamMat</td>\n",
       "      <td>12</td>\n",
       "      <td>Error during installing dependencies # Env\\r\\n...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>zzzyuqing/DreamMat_12</td>\n",
       "      <td>0</td>\n",
       "      <td>https://github.com/zzzyuqing/DreamMat/issues/12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1994424</th>\n",
       "      <td>zzzyuqing/DreamMat</td>\n",
       "      <td>14</td>\n",
       "      <td>CUDA not found in docker image I pulled the of...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>zzzyuqing/DreamMat_14</td>\n",
       "      <td>0</td>\n",
       "      <td>https://github.com/zzzyuqing/DreamMat/issues/14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1994440</th>\n",
       "      <td>zzzyuqing/DreamMat</td>\n",
       "      <td>30</td>\n",
       "      <td>fix validation_step &amp;&amp; test_step bug During va...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>zzzyuqing/DreamMat_30</td>\n",
       "      <td>0</td>\n",
       "      <td>https://github.com/zzzyuqing/DreamMat/issues/30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>605972 rows × 7 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
