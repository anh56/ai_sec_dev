{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import duckdb\n",
    "import re\n",
    "import csv\n",
    "import sys\n",
    "import ahocorasick\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "import pandas as pd\n",
    "from Levenshtein import ratio\n",
    "\n",
    "\n",
    "def generate_corasick(kws: list[str]):\n",
    "\tautomation = ahocorasick.Automaton()\n",
    "\tfor idx, key in enumerate(kws):\n",
    "\t\tautomation.add_word(key, (idx, key))\n",
    "\tautomation.make_automaton()\n",
    "\treturn automation\n",
    "\n",
    "\n",
    "def extract_keywords(text: str, corasick_auto):\n",
    "\tkw = set()\n",
    "\tfor end_index, (insert_order, original_value) in corasick_auto.iter(str(text).lower()):\n",
    "\t\tkw.add(original_value)\n",
    "\treturn list(kw) if kw else None"
   ],
   "id": "9bf645964a22b698",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "con = duckdb.connect('../mining/result/models.db')\n",
    "\n",
    "tables = [\n",
    "\t\"models\",\n",
    "\t\"hf_discussions\",\n",
    "\t\"hf_discussion_events\",\n",
    "\t\"gh_repositories\",\n",
    "\t\"gh_discussions\",\n",
    "\t\"gh_comments\",\n",
    "\t\"gh_issues\",\n",
    "\t\"gh_issues_comments\",\n",
    "]\n",
    "\n",
    "for table in tables:\n",
    "\tdf = con.execute(f\"SELECT * from {table}\").df()\n",
    "\tdf.to_csv(f\"./db_dump_csv/{table}.csv\", index=False)\n",
    "\n",
    "\t# count = con.execute(f\"SELECT COUNT(1) from {table}\")\n",
    "\t# print(f\"{table=}, {count.fetchone()=}\")\n",
    " \n",
    "\tcon.execute(f\"COPY {table} TO './db_dump_csv/{table}.csv' (HEADER, DELIMITER ',')\")\n"
   ],
   "id": "c875f5acb48aa060",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df = pd.read_csv(\"./db_dump_csv/models.csv\")\n",
    "\n",
    "df[\"github_links_set\"] = None\n",
    "df[\"github_links_score\"] = None\n",
    "df[\"highest_score_link\"] = None\n",
    "df[\"highest_score\"] = None\n",
    "\n",
    "\n",
    "print(df.info())\n",
    "print(len(df))\n",
    "\n",
    "df.drop_duplicates(subset=[\"model_id\"], inplace=True)\n",
    "print(len(df))\n",
    "\n",
    "df.dropna(subset=[\"github_links\"], inplace=True)\n",
    "print(len(df))\n",
    "\n",
    "to_remove = [\n",
    "\t\")\", \"(\", \",\",\n",
    "]\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "\tprint(index, row[\"model_id\"])\n",
    "\tlink_str = row[\"github_links\"]\n",
    "\n",
    "\tif pd.isna(link_str) or link_str == \"\" or link_str == \"[]\":\n",
    "\t\tdf.at[index, \"github_links_set\"] = None\n",
    "\t\tcontinue\n",
    "\n",
    "\tlinks = link_str.split(\",\")\n",
    "\trepo_link = None\n",
    "\tprocessed_links = set()\n",
    "\n",
    "\tfor link in links:\n",
    "\t\tif link == \"\":\n",
    "\t\t\tcontinue\n",
    "\t\tlink = link.strip()\n",
    "\t\t# remove special characters\n",
    "\t\tfor char in to_remove:\n",
    "\t\t\tlink = link.replace(char, \"\")\n",
    "\n",
    "\t\t# capture entire link\n",
    "\t\tif re.search(r\"https?:\\/\\/?github\\.com\\/[\\w-]+\\/[\\w-]+\", link):\n",
    "\t\t\trepo_link = re.search(r\"https?:\\/\\/?github\\.com\\/[\\w-]+\\/[\\w-]+\", link).group(0)\n",
    "\t\t\tprocessed_links.add(repo_link)\n",
    "\t\t\tauthor_repo = repo_link.replace(\"https://github.com/\", \"\")\n",
    "\n",
    "\t# capture each group\n",
    "\t# author, repo = re.search(r\"https?:\\/\\/?github\\.com\\/([\\w-]+)\\/([\\w-]+)\", link).groups()\n",
    "\t# authors.add(author)\n",
    "\t# repos.add(repo)\n",
    "\n",
    "\tif processed_links:\n",
    "\t\tprocessed_links = list(processed_links)\n",
    "\t\tdf.at[index, \"github_links_set\"] = processed_links\n",
    "\t\tscores = [ratio(link, row[\"model_id\"]) for link in processed_links]\n",
    "\t\tdf.at[index, \"github_links_score\"] = scores\n",
    "\t\tdf.at[index, \"highest_score_link\"] = processed_links[scores.index(max(scores))]\n",
    "\t\tdf.at[index, \"highest_score\"] = max(scores)\n",
    "\n",
    "\n",
    "df.to_csv(\"./filtered/hf_models_with_scored_link.csv\", index=False)\n",
    "\n",
    "df.dropna(subset=[\"highest_score\"], inplace=True)\n",
    "print(len(df))\n",
    "print(df.highest_score_link.nunique())\n",
    "df.to_csv(\"./filtered/hf_models.csv\", index=False)"
   ],
   "id": "32c51e12b64910af",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(df['highest_score'].dropna(), bins=10, color='skyblue', edgecolor='black')\n",
    "plt.title('Distribution of Highest Scores')\n",
    "plt.xlabel('Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ],
   "id": "3bf2898adc29b42a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# filter other files\n",
    "files = [\n",
    "\t\"gh_comments.csv\",\n",
    "\t\"gh_discussions.csv\",\n",
    "\t\"gh_issues.csv\",\n",
    "\t\"gh_issues_comments.csv\",\n",
    "\t\"gh_repositories.csv\",\n",
    "\t\"hf_discussion_events.csv\",\n",
    "\t\"hf_discussions.csv\",\n",
    "]\n",
    "\n",
    "bots = [\n",
    "\t\"allcontributors[bot]\",\n",
    "\t\"allstar-app[bot]\",\n",
    "\t\"azure-pipelines[bot]\",\n",
    "\t\"codeant-ai[bot]\",\n",
    "\t\"coderabbitai[bot]\",\n",
    "\t\"copybara-service[bot]\",\n",
    "\t\"dagshub[bot]\",\n",
    "\t\"deepsource-autofix[bot]\",\n",
    "\t\"dependabot-preview[bot]\",\n",
    "\t\"dependabot[bot]\",\n",
    "\t\"devin-ai-integration[bot]\",\n",
    "\t\"ellipsis-dev[bot]\",\n",
    "\t\"github-actions[bot]\",\n",
    "\t\"google-allstar-prod[bot]\",\n",
    "\t\"greenkeeper[bot]\",\n",
    "\t\"imgbot[bot]\",\n",
    "\t\"learn-build-service-prod[bot]\",\n",
    "\t\"lgtm-com[bot]\",\n",
    "\t\"linear[bot]\",\n",
    "\t\"lumberbot-app[bot]\",\n",
    "\t\"mend-for-github-com[bot]\",\n",
    "\t\"mergify[bot]\",\n",
    "\t\"microsoft-github-operations[bot]\",\n",
    "\t\"microsoft-github-policy-service[bot]\",\n",
    "\t\"opensearch-trigger-bot[bot]\",\n",
    "\t\"pre-commit-ci[bot]\",\n",
    "\t\"pull[bot]\",\n",
    "\t\"pytorch-bot[bot]\",\n",
    "\t\"renovate[bot]\",\n",
    "\t\"restyled-io[bot]\",\n",
    "\t\"sentry-io[bot]\",\n",
    "\t\"sourcery-ai[bot]\",\n",
    "\t\"stainless-app[bot]\",\n",
    "\t\"sweep-ai[bot]\",\n",
    "\t\"sweep-nightly[bot]\",\n",
    "\t\"sync-by-unito[bot]\",\n",
    "\t\"vs-code-engineering[bot]\",\n",
    "]\n",
    "\n",
    "for file in files:\n",
    "\t# remove bot issues\n",
    "\tdf = pd.read_csv(f\"./db_dump_csv/{file}\")\n",
    "\tprint(f\"{file} original count {len(df)}\")\n",
    "\n",
    "\tif \"author_login\" in list(df):\n",
    "\t\tdf = df[~df['author_login'].isin(bots)]\n",
    "\telif \"user_login\" in list(df):\n",
    "\t\tdf = df[~df['user_login'].isin(bots)]\n",
    "\n",
    "\tdf.drop_duplicates()\n",
    "\n",
    "\tprint(f\"{file} filtered count {len(df)}\")\n",
    "\tdf.to_csv(f\"./filtered/{file}\", index=False)"
   ],
   "id": "6f5433d05d63b3ba",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "kws1 = [\"vulnerability\", \"vulnerabilities\", \"cwe\", \"CWE\", \"cve\", \"CVE\", \"security\"]\n",
    "\n",
    "corasick = generate_corasick(kws1)\n",
    "\n",
    "# find kws in HF\n",
    "\n",
    "df = pd.read_csv(\"./filtered/hf_discussions.csv\")\n",
    "df[\"keywords\"] = None\n",
    "for index, row in df.iterrows():\n",
    "\tdf.at[index, 'keywords'] = extract_keywords(row['title'], corasick)\n",
    "print(f\"HF Discussions with KW founds: {len(df[df['keywords'].notna()])} in {len(df)}\")\n",
    "df[df['keywords'].notna()].to_csv(f\"./filtered_with_kw/hf_discussions.csv\", index=False)\n",
    "\n",
    "df = pd.read_csv(\"./filtered/hf_discussion_events.csv\")\n",
    "df[\"keywords\"] = None\n",
    "for index, row in df.iterrows():\n",
    "\tdf.at[index, 'keywords'] = extract_keywords(row['content'], corasick)\n",
    "print(f\"HF Discussions Events with KW founds: {len(df[df['keywords'].notna()])} in {len(df)}\")\n",
    "df[df['keywords'].notna()].to_csv(f\"./filtered_with_kw/hf_discussion_events.csv\", index=False)"
   ],
   "id": "ee51f9772c54bdf9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Find kws in GH\n",
    "\n",
    "df = pd.read_csv(\"./filtered/gh_discussions.csv\")\n",
    "df[\"keywords\"] = None\n",
    "for index, row in df.iterrows():\n",
    "\tdf.at[index, 'keywords'] = extract_keywords(\n",
    "\t\tstr(row['discussion_title']) + \"\\n\" + str(row['discussion_body']),\n",
    "\t\tcorasick\n",
    "\t)\n",
    "print(f\"GH Discussions KW founds: {len(df[df['keywords'].notna()])} in {len(df)}\")\n",
    "df[df['keywords'].notna()].to_csv(f\"./filtered_with_kw/gh_discussions.csv\", index=False)\n",
    "\n",
    "df = pd.read_csv(\"./filtered/gh_comments.csv\")\n",
    "df[\"keywords\"] = None\n",
    "for index, row in df.iterrows():\n",
    "\tdf.at[index, 'keywords'] = extract_keywords(row['comment_body'], corasick)\n",
    "print(f\"GH Comments KW founds: {len(df[df['keywords'].notna()])} in {len(df)}\")\n",
    "df[df['keywords'].notna()].to_csv(f\"./filtered_with_kw/gh_comments.csv\", index=False)\n",
    "\n",
    "# gh_issues.csv\n",
    "df = pd.read_csv(\"./filtered/gh_issues.csv\")\n",
    "df[\"keywords\"] = None\n",
    "for index, row in df.iterrows():\n",
    "\tdf.at[index, 'keywords'] = extract_keywords(\n",
    "\t\tstr(row['issue_title']) + \"\\n\" + str(row['issue_body']),\n",
    "\t\tcorasick\n",
    "\t)\n",
    "print(f\"GH Issues KW founds: {len(df[df['keywords'].notna()])} in {len(df)}\")\n",
    "df[df['keywords'].notna()].to_csv(f\"./filtered_with_kw/gh_issues.csv\", index=False)"
   ],
   "id": "19bc9ac27ea8fb4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# merge from filtered HF_DISCUSSION, GH_DISCUSSION and GH_ISSUES\n",
    "hf_discussions = pd.read_csv(\"./filtered/hf_discussions.csv\")\n",
    "hf_discussion_events = pd.read_csv(\"./filtered/hf_discussion_events.csv\")\n",
    "merged_hf = pd.merge(hf_discussions, hf_discussion_events, on=[\"model_id\", \"num\"], how=\"left\")\n",
    "print(\"HF merged \", len(merged_hf))\n",
    "merged_hf.drop(columns=[\"keywords_x\", \"keywords_y\"], inplace=True)\n",
    "merged_hf.to_csv(\"./merged/merged_hf_discussions.csv\", index=False)\n",
    "\n",
    "gh_discussions = pd.read_csv(\"./filtered/gh_discussions.csv\")\n",
    "gh_comments = pd.read_csv(\"./filtered/gh_comments.csv\")\n",
    "merged_gh = pd.merge(gh_discussions, gh_comments, on=[\"repo_name\", \"discussion_number\"], how=\"left\")\n",
    "print(\"GH merged\", len(merged_gh))\n",
    "merged_gh.drop(columns=[\"id_x\", \"id_y\"], inplace=True)\n",
    "merged_gh.to_csv(\"./merged/merged_gh_discussions.csv\", index=False)\n"
   ],
   "id": "4be34b370a933b90",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# merge from filtered with keywords HF_DISCUSSION, GH_DISCUSSION and GH_ISSUES\n",
    "def combine_keywords(row):\n",
    "\tkeywords = set()\n",
    "\tif not pd.isna(row[\"keywords_x\"]):\n",
    "\t\tkws = eval(row[\"keywords_x\"])\n",
    "\t\tfor kw in kws:\n",
    "\t\t\tkeywords.add(kw)\n",
    "\tif not pd.isna(row[\"keywords_y\"]):\n",
    "\t\tkws = eval(row[\"keywords_x\"])\n",
    "\t\tfor kw in kws:\n",
    "\t\t\tkeywords.add(kw)\n",
    "\tif keywords:  # Check if the list is not empty\n",
    "\t\treturn list(keywords)\n",
    "\telse:\n",
    "\t\treturn None\n",
    "\n",
    "\n",
    "hf_discussions = pd.read_csv(\"./filtered_with_kw/hf_discussions.csv\")\n",
    "hf_discussion_events = pd.read_csv(\"./filtered_with_kw/hf_discussion_events.csv\")\n",
    "merged_hf = pd.merge(hf_discussions, hf_discussion_events, on=[\"model_id\", \"num\"], how=\"left\")\n",
    "merged_hf.drop_duplicates(inplace=True)\n",
    "merged_hf[\"keywords\"] = merged_hf.apply(combine_keywords, axis=1)\n",
    "merged_hf.drop(columns=[\"keywords_x\", \"keywords_y\"], inplace=True)\n",
    "print(len(merged_hf))\n",
    "merged_hf.to_csv(\"./merged_with_kw/merged_hf_discussions.csv\", index=False)\n",
    "\n",
    "gh_discussions = pd.read_csv(\"./filtered_with_kw/gh_discussions.csv\")\n",
    "gh_comments = pd.read_csv(\"./filtered_with_kw/gh_comments.csv\")\n",
    "merged_gh = pd.merge(gh_discussions, gh_comments, on=[\"repo_name\", \"discussion_number\"], how=\"left\")\n",
    "merged_gh.drop_duplicates(inplace=True)\n",
    "merged_gh[\"keywords\"] = merged_gh.apply(combine_keywords, axis=1)\n",
    "merged_gh.drop(columns=[\"keywords_x\", \"keywords_y\", \"id_x\", \"id_y\"], inplace=True)\n",
    "print(\"GH merged\", len(merged_gh))\n",
    "merged_gh.to_csv(\"./merged_with_kw/merged_gh_discussions.csv\", index=False)"
   ],
   "id": "62505105b08a8c3d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# reconstruct the hf comments links\n",
    "# and set up label\n",
    "df = pd.read_csv(\"./filtered_with_kw/hf_discussions.csv\")\n",
    "df[\"is_security\"] = -1\n",
    "df[\"security_category\"] = None\n",
    "df[\"keyword_count\"] = df[\"keywords\"].apply(lambda x: len(eval(x)) if pd.notna(x) else 0)\n",
    "counts = {n: len(df[df[\"keyword_count\"] == n]) for n in range(0, 7)}\n",
    "df.drop_duplicates(inplace=True)\n",
    "print(\"HF Discussions with n keywords:\", counts)\n",
    "df.to_csv(\"./manual/hf_discussions_working.csv\", index=False)\n",
    "\n",
    "df = pd.read_csv(\"./filtered_with_kw/hf_discussion_events.csv\")\n",
    "df[\"url\"] = \"https://huggingface.co/\" + df[\"model_id\"] + \"/discussions/\" + df[\"num\"].astype(str) + \"#\" + df[\"event_id\"]\n",
    "df[\"is_security\"] = -1\n",
    "df[\"security_category\"] = None\n",
    "df[\"keyword_count\"] = df[\"keywords\"].apply(lambda x: len(eval(x)) if pd.notna(x) else 0)\n",
    "counts = {n: len(df[df[\"keyword_count\"] == n]) for n in range(0, 7)}\n",
    "df.drop_duplicates(inplace=True)\n",
    "print(\"HF Discussions Event with n keywords:\", counts)\n",
    "df.to_csv(\"./manual/hf_discussion_events_working.csv\", index=False)\n",
    "\n",
    "df = pd.read_csv(\"./filtered_with_kw/gh_discussions.csv\")\n",
    "df[\"url\"] = \"https://github.com/\" + df[\"repo_name\"] + \"/discussions/\" + df[\"discussion_number\"].astype(str)\n",
    "df[\"is_security\"] = -1\n",
    "df[\"security_category\"] = None\n",
    "df[\"keyword_count\"] = df[\"keywords\"].apply(lambda x: len(eval(x)) if pd.notna(x) else 0)\n",
    "counts = {n: len(df[df[\"keyword_count\"] == n]) for n in range(0, 7)}\n",
    "df = df.drop(columns=[\"id\"]).drop_duplicates(\n",
    "\tsubset=[\"repo_name\", \"discussion_number\", \"discussion_title\", \"author_login\"])\n",
    "print(\"GH Discussions with n keywords:\", counts)\n",
    "df.to_csv(\"./manual/gh_discussions_working.csv\", index=False)\n",
    "\n",
    "df = pd.read_csv(\"./filtered_with_kw/gh_comments.csv\")\n",
    "df[\"url\"] = \"https://github.com/\" + df[\"repo_name\"] + \"/discussions/\" + df[\"discussion_number\"].astype(str)\n",
    "df[\"is_security\"] = -1\n",
    "df[\"security_category\"] = None\n",
    "df[\"keyword_count\"] = df[\"keywords\"].apply(lambda x: len(eval(x)) if pd.notna(x) else 0)\n",
    "counts = {n: len(df[df[\"keyword_count\"] == n]) for n in range(0, 7)}\n",
    "df = df.drop(columns=[\"id\"]).drop_duplicates()\n",
    "print(\"GH Discussions Comment with n keywords:\", counts)\n",
    "df.to_csv(\"./manual/gh_comments_working.csv\", index=False)\n",
    "\n",
    "df = pd.read_csv(\"./filtered_with_kw/gh_issues.csv\")\n",
    "df[\"url\"] = \"https://github.com/\" + df[\"repo_name\"] + \"/issues/\" + df[\"issue_number\"].astype(str)\n",
    "df[\"is_security\"] = -1\n",
    "df[\"security_category\"] = None\n",
    "df[\"keyword_count\"] = df[\"keywords\"].apply(lambda x: len(eval(x)) if pd.notna(x) else 0)\n",
    "counts = {n: len(df[df[\"keyword_count\"] == n]) for n in [0, 1, 2, 3, 4, 5, 6]}\n",
    "df = df.drop(columns=[\"id\"]).drop_duplicates()\n",
    "print(\"GH issues with n keywords:\", counts)\n",
    "df[df[\"keyword_count\"] >= 4].to_csv(\"./manual/gh_issues_subset_working.csv\", index=False)\n",
    "df.to_csv(\"./manual/gh_issues_working.csv\", index=False)\n"
   ],
   "id": "2bbe434abbb5154d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# test get cve in gh issues\n",
    "# df = pd.read_csv(\"./filtered_with_kw/gh_issues.csv\")\n",
    "# df[\"keyword_count\"] = df[\"keywords\"].apply(lambda x: len(eval(x)) if pd.notna(x) else 0)\n",
    "# counts = {n: len(df[df[\"keyword_count\"] == n]) for n in [0, 1, 2, 3, 4, 5, 6]}\n",
    "# print(\"GH issues with n keywords:\", counts)\n",
    "# df_cve = df[df[\"keywords\"].str.contains(\"cve|CVE\", na=False)]\n",
    "# print(len(df_cve))\n",
    "# df_cve\n",
    "# NOTES: cve contains a lot of false positive\n",
    "\n",
    "df = pd.read_csv(\"./filtered_with_kw/gh_issues.csv\")\n",
    "df[\"url\"] = \"https://github.com/\" + df[\"repo_name\"] + \"/issues/\" + df[\"issue_number\"].astype(str)\n",
    "df[\"is_security\"] = -1\n",
    "df[\"security_category\"] = None\n",
    "df[\"keyword_count\"] = df[\"keywords\"].apply(lambda x: len(eval(x)) if pd.notna(x) else 0)\n",
    "counts = {n: len(df[df[\"keyword_count\"] == n]) for n in [0, 1, 2, 3, 4, 5, 6]}\n",
    "df = df.drop(columns=[\"id\"]).drop_duplicates()\n",
    "print(\"GH issues with n keywords:\", counts)\n",
    "df[df[\"keyword_count\"] >= 3].to_csv(\"./manual/gh_issues_subset_3_working.csv\", index=False)"
   ],
   "id": "3ee5f67fbf4bd40a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# HF label reverse look up\n",
    "hf_discussion = pd.read_csv(\"./manual/hf_discussions_done.csv\")\n",
    "hf_discussion_event = pd.read_csv(\"./manual/hf_discussion_events_done.csv\")\n",
    "merged_hf = pd.read_csv(\"./merged/merged_hf_discussions.csv\")\n",
    "print(f\"{len(merged_hf)=}\")\n",
    "merged_hf.drop_duplicates(inplace=True)\n",
    "# merged_hf.dropna(subset=[\"content\"], inplace=True)\n",
    "merged_hf[\"is_security\"] = 0\n",
    "\n",
    "df_non_security_disc = hf_discussion[hf_discussion[\"is_security\"] == 0][[\"model_id\", \"num\"]]\n",
    "df_non_security_event = hf_discussion_event[hf_discussion_event[\"is_security\"] == 0][[\"model_id\", \"num\"]]\n",
    "non_security_union = pd.concat([df_non_security_disc, df_non_security_event]).drop_duplicates()\n",
    "print(f\"{len(non_security_union)=}\")\n",
    "\n",
    "df_security_disc = hf_discussion[hf_discussion[\"is_security\"] == 1][[\"model_id\", \"num\"]]\n",
    "df_security_event = hf_discussion_event[hf_discussion_event[\"is_security\"] == 1][[\"model_id\", \"num\"]]\n",
    "security_union = pd.concat([df_security_disc, df_security_event]).drop_duplicates()\n",
    "print(f\"{len(security_union)=}\")\n",
    "\n",
    "# keep only the rows we labelled in manual set, either 0 or 1\n",
    "merged_hf = merged_hf.merge(\n",
    "\tpd.concat([non_security_union, security_union]).drop_duplicates(),\n",
    "\ton=[\"model_id\", \"num\"],\n",
    "\thow=\"inner\"\n",
    ")\n",
    "print(f\"{len(merged_hf)=}\")\n",
    "\n",
    "merged_hf[\"is_security\"] = merged_hf[[\"model_id\", \"num\"]].apply(tuple, axis=1).isin(\n",
    "\tpd.concat([df_security_disc, df_security_event]).apply(tuple, axis=1)\n",
    ").astype(int)\n",
    "\n",
    "merged_hf.loc[\n",
    "\tmerged_hf[[\"model_id\", \"num\"]].apply(tuple, axis=1).isin(security_union.apply(tuple, axis=1)),\n",
    "\t\"is_security\"\n",
    "] = 1\n",
    "\n",
    "merged_hf = merged_hf[merged_hf[\"event_type\"] == \"comment\"]\n",
    "\n",
    "merged_hf.to_csv(\"./merged_after_manual/merged_hf_discussions.csv\", index=False)\n",
    "\n",
    "merged_hf_sec = merged_hf[merged_hf[\"is_security\"] == 1]\n",
    "print(f\"{len(merged_hf_sec)=}\")\n",
    "merged_hf_sec.to_csv(\"./merged_after_manual/merged_hf_discussions_security.csv\", index=False)\n",
    "\n",
    "# count the distinct combination of model_id and num\n",
    "distinct_discussions = merged_hf[['model_id', 'num']].drop_duplicates().shape[0]\n",
    "print(f\"Distinct HF discussion: {distinct_discussions}\")\n",
    "\n",
    "distinct_discussions = merged_hf_sec[['model_id', 'num']].drop_duplicates().shape[0]\n",
    "print(f\"Distinct Security HF discussion: {distinct_discussions}\")\n"
   ],
   "id": "cdc82a56c3256347",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# GH label reverse look up\n",
    "gh_discussion = pd.read_csv(\"./manual/gh_discussions_done.csv\")\n",
    "gh_comments = pd.read_csv(\"./manual/gh_comments_done.csv\")\n",
    "merged_gh = pd.read_csv(\"./merged/merged_gh_discussions.csv\")\n",
    "print(f\"{len(merged_gh)=}\")\n",
    "merged_gh.drop_duplicates(inplace=True)\n",
    "merged_gh[\"is_security\"] = 0\n",
    "\n",
    "df_non_security_disc = gh_discussion[gh_discussion[\"is_security\"] == 0][[\"repo_name\", \"discussion_number\"]]\n",
    "print(f\"{len(df_non_security_disc)=}\")\n",
    "df_non_security_cmt = gh_comments[gh_comments[\"is_security\"] == 0][[\"repo_name\", \"discussion_number\"]]\n",
    "print(f\"{len(df_non_security_cmt)=}\")\n",
    "non_security_union = pd.concat([df_non_security_disc, df_non_security_cmt]).drop_duplicates()\n",
    "print(f\"{len(non_security_union)=}\")\n",
    "\n",
    "df_security_disc = gh_discussion[gh_discussion[\"is_security\"] == 1][[\"repo_name\", \"discussion_number\"]]\n",
    "print(f\"{len(df_security_disc)=}\")\n",
    "df_security_cmt = gh_comments[gh_comments[\"is_security\"] == 1][[\"repo_name\", \"discussion_number\"]]\n",
    "print(f\"{len(df_security_cmt)=}\")\n",
    "security_union = pd.concat([df_security_disc, df_security_cmt]).drop_duplicates()\n",
    "# print(security_union)\n",
    "print(f\"{len(security_union)=}\")\n",
    "\n",
    "merged_gh = merged_gh.merge(\n",
    "\tpd.concat([non_security_union, security_union]).drop_duplicates(),\n",
    "\ton=[\"repo_name\", \"discussion_number\"],\n",
    "\thow=\"inner\"\n",
    ")\n",
    "print(f\"{len(merged_gh)=}\")\n",
    "\n",
    "merged_gh.loc[\n",
    "\tmerged_gh[[\"repo_name\", \"discussion_number\"]].apply(tuple, axis=1).isin(security_union.apply(tuple, axis=1)),\n",
    "\t\"is_security\"\n",
    "] = 1\n",
    "print(f\"{len(merged_gh)=}\")\n",
    "merged_gh.to_csv(\"./merged_after_manual/merged_gh_discussions.csv\", index=False)\n",
    "\n",
    "merged_gh_sec = merged_gh[merged_gh[\"is_security\"] == 1]\n",
    "print(f\"{len(merged_gh_sec)=}\")\n",
    "merged_gh_sec.to_csv(\"./merged_after_manual/merged_gh_discussions_security.csv\", index=False)\n",
    "\n",
    "# count the distinct combination of model_id and num\n",
    "distinct_discussions = merged_gh[['repo_name', 'discussion_number']].drop_duplicates().shape[0]\n",
    "print(f\"Distinct GH discussion: {distinct_discussions}\")\n",
    "\n",
    "distinct_discussions = merged_gh_sec[['repo_name', 'discussion_number']].drop_duplicates().shape[0]\n",
    "print(f\"Distinct Security HF discussion: {distinct_discussions}\")"
   ],
   "id": "9c169a9dc7fb36c9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# manual check\n",
    "merged_gh = pd.read_csv(\"./merged_after_manual/merged_gh_discussions.csv\")\n",
    "merged_hf = pd.read_csv(\"./merged_after_manual/merged_hf_discussions.csv\")\n",
    "issues = pd.read_csv(\"./manual/gh_issues_subset_3_done.csv\")\n",
    "issues_external_sec = pd.read_csv(\"./external_issues/github_sec_issues.csv\", delimiter=\";\")\n",
    "issues_external_non_sec = pd.read_csv(\"./external_issues/github_nonsec_issues.csv\", delimiter=\";\")\n",
    "\n",
    "# repo_name,discussion_number,discussion_title,discussion_body,author_login_x,author_login_y,comment_body,is_security\n",
    "merged_columns = [\n",
    "\t\"id_name\", \"id_num\", \"type\", \"content\", \"is_security\"\n",
    "]\n",
    "\n",
    "# all_df = pd.DataFrame(columns=merged_columns)\n",
    "merged_gh[\"id_name\"] = merged_gh[\"repo_name\"]\n",
    "merged_gh[\"id_num\"] = merged_gh[\"discussion_number\"]\n",
    "merged_gh[\"content\"] = (\n",
    "\tmerged_gh[\"discussion_title\"].fillna(\"\").str.strip() + \" \" +\n",
    "\tmerged_gh[\"discussion_body\"].fillna(\"\").str.strip() + \" \" +\n",
    "\tmerged_gh[\"comment_body\"].fillna(\"\").str.strip()\n",
    ").str.strip()\n",
    "merged_gh[\"type\"] = \"GH_DISCUSSIONS\"\n",
    "\n",
    "# model_id,num,title,git_ref,url,event_id,event_type,content,is_security\n",
    "merged_hf[\"id_name\"] = merged_hf[\"model_id\"]\n",
    "merged_hf[\"id_num\"] = merged_hf[\"num\"]\n",
    "merged_hf[\"content\"] = (\n",
    "\tmerged_hf[\"title\"].fillna(\"\").str.strip() + \" \" +\n",
    "\tmerged_hf[\"content\"].fillna(\"\").str.strip()\n",
    ").str.strip()\n",
    "merged_hf[\"type\"] = \"HF_DISCUSSIONS\"\n",
    "\n",
    "# repo_name,issue_url,pr_from_issue,user_login,issue_number,keywords,url,issue_title,issue_body,is_security,security_category,keyword_count\n",
    "issues[\"id_name\"] = issues[\"repo_name\"]\n",
    "issues[\"id_num\"] = issues[\"issue_number\"]\n",
    "issues[\"content\"] = (\n",
    "\tissues[\"issue_title\"].fillna(\"\").str.strip() + \" \" +\n",
    "\tissues[\"issue_body\"].fillna(\"\").str.strip()\n",
    ")\n",
    "issues[\"type\"] = \"GH_ISSUES\"\n",
    "\n",
    "all_df = pd.concat(\n",
    "\t[\n",
    "\t\tmerged_gh[merged_columns],\n",
    "\t\tmerged_hf[merged_columns],\n",
    "\t\tissues[merged_columns]\n",
    "\t]\n",
    ")\n",
    "print(\"All manual records\", len(all_df))\n",
    "# shuffle\n",
    "all_df = all_df.sample(frac=1)"
   ],
   "id": "f603f79779599816",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "all_df",
   "id": "11e587aa1ed1c056",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# run model in classifier\n",
    "# visualize inference metrics"
   ],
   "id": "a8fe8177e5876b15",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def visualize_prob_sigmoid_distribution(file_path):\n",
    "\ttry:\n",
    "\t\tdf = pd.read_csv(file_path)\n",
    "\n",
    "\t\tif 'prob_sigmoid' not in df.columns:\n",
    "\t\t\tprint(f\"Error: 'prob_sigmoid' column not found in {file_path}\")\n",
    "\t\t\treturn\n",
    "\n",
    "\t\tif not all(0 <= x <= 1 for x in df['prob_sigmoid']):\n",
    "\t\t\tprint(\"Warning: data points found out of 0-1 range. will try to filter...\")\n",
    "\t\t\tdf = df[(df['prob_sigmoid'] >= 0) & (df['prob_sigmoid'] <= 1)]\n",
    "\t\t\tprint(f\"Filtered {sum(not (0 <= x <= 1) for x in df['prob_sigmoid'])} out of range datapoints.\")\n",
    "\n",
    "\t\tplt.figure(figsize=(10, 6))  # Adjust figure size as needed\n",
    "\t\tsns.histplot(df['prob_sigmoid'], kde=True, bins=30, color='skyblue')  #Histogram\n",
    "\t\tplt.title('Distribution of prob_sigmoid')\n",
    "\t\tplt.xlabel('prob_sigmoid')\n",
    "\t\tplt.ylabel('Frequency')\n",
    "\t\tplt.grid(axis='y', alpha=0.75)\n",
    "\n",
    "\t\tmean = df['prob_sigmoid'].mean()\n",
    "\t\tmedian = df['prob_sigmoid'].median()\n",
    "\t\tstd = df['prob_sigmoid'].std()\n",
    "\t\tplt.axvline(mean, color='red', linestyle='dashed', linewidth=1, label=f'Mean: {mean:.2f}')\n",
    "\t\tplt.axvline(median, color='green', linestyle='dashed', linewidth=1, label=f'Median: {median:.2f}')\n",
    "\t\tplt.legend()\n",
    "\n",
    "\t\tplt.tight_layout()\n",
    "\t\tplt.show()\n",
    "\n",
    "\texcept FileNotFoundError:\n",
    "\t\tprint(f\"Error: File not found at {file_path}\")\n",
    "\texcept pd.errors.EmptyDataError:\n",
    "\t\tprint(f\"Error: File {file_path} is empty.\")\n",
    "\texcept pd.errors.ParserError:\n",
    "\t\tprint(f\"Error: Could not parse {file_path}. Check file format.\")\n",
    "\texcept Exception as e:\n",
    "\t\tprint(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "\n",
    "file_path = 'inference/all_gh_bert_gh_last/raw_predictions.csv'\n",
    "visualize_prob_sigmoid_distribution(file_path)"
   ],
   "id": "e6e661cdf6c7fb7a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df = pd.read_csv(\"inference/all_hf_bert_hf_best/raw_predictions.csv\")\n",
    "df_security = df[df[\"is_security_prediction\"] == 1]\n",
    "print(len(df_security))\n",
    "\n",
    "file_path = 'inference/all_hf_bert_hf_best/raw_predictions.csv'\n",
    "visualize_prob_sigmoid_distribution(file_path)\n"
   ],
   "id": "1d607629e0f4b210",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "\n",
    "# prediction result collection\n",
    "def collect_prediction(path: str, exclude: list[str]):\n",
    "\tmodels = [\"bert_base\", \"distilbert\", \"securebert\", \"roberta_base\", \"secbert\", \"secroberta\", ]\n",
    "\tall_metrics_data = []\n",
    "\n",
    "\tsubfolders = [\n",
    "\t\tf.path\n",
    "\t\tfor f in os.scandir(path)\n",
    "\t\tif f.is_dir() and f.name not in exclude\n",
    "\t]\n",
    "\n",
    "\tfor subfolder in subfolders:\n",
    "\t\tmetrics_file_path = os.path.join(subfolder, \"metrics.csv\")\n",
    "\t\tif not os.path.exists(metrics_file_path):\n",
    "\t\t\tcontinue\n",
    "\t\tdf = pd.read_csv(metrics_file_path)\n",
    "\t\t# only get the test result\n",
    "\t\tdf = df.tail(1)\n",
    "\t\tmodel_type = None\n",
    "\t\tdata_type = None\n",
    "\n",
    "\t\tmatch = re.search(r\"_({})\".format(\"|\".join(models)), os.path.basename(subfolder))\n",
    "\t\tif match:\n",
    "\t\t\tmodel_type = str(os.path.basename(subfolder)[match.start():]).replace(\"_\", \"\", 1)\n",
    "\t\t\tdata_type = os.path.basename(subfolder)[:match.start()]\n",
    "\n",
    "\t\tdf[\"input\"] = data_type\n",
    "\t\tdf[\"model_type\"] = model_type\n",
    "\t\tdf[\"subfolder\"] = os.path.basename(subfolder)\n",
    "\t\tdf[\"folder\"] = path\n",
    "\t\tall_metrics_data.append(df)\n",
    "\n",
    "\treturn pd.concat(all_metrics_data, ignore_index=True)\n",
    "\n",
    "\n",
    "path = \"./prediction\"\n",
    "exclude = []\n",
    "metrics_predictions = collect_prediction(path, exclude)\n",
    "metrics_predictions"
   ],
   "id": "a3e5b17bfb68f25f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def collect_inference(path: str, exclude: list[str]):\n",
    "\tmodels = [\"llama\", \"deepseek\", \"phi4\", \"mistral\"]\n",
    "\tall_metrics_data = []\n",
    "\n",
    "\tsubfolders = [\n",
    "\t\tf.path\n",
    "\t\tfor f in os.scandir(path)\n",
    "\t\tif f.is_dir() and f.name not in exclude\n",
    "\t]\n",
    "\n",
    "\tfor subfolder in subfolders:\n",
    "\t\tmetrics_file_path = os.path.join(subfolder, \"metrics.json\")\n",
    "\t\tif not os.path.exists(metrics_file_path):\n",
    "\t\t\tcontinue\n",
    "\t\twith open(metrics_file_path, 'r') as f:\n",
    "\t\t\tdata = json.load(f)\n",
    "\t\tdf = pd.DataFrame([data])\n",
    "\t\t# only get the test result\n",
    "\t\tdf[\"subfolder\"] = os.path.basename(subfolder)\n",
    "\t\tmodel = None\n",
    "\t\tdata = None\n",
    "\t\tmatch = re.search(r\"_(llama\\d*|deepseekr\\d*|phi4|mistral(_small)?)\", os.path.basename(subfolder))\n",
    "\t\tif match:\n",
    "\t\t\tmodel = str(os.path.basename(subfolder)[match.start():]).replace(\"_\", \"\", 1)\n",
    "\t\t\tdata = os.path.basename(subfolder)[:match.start()]\n",
    "\t\tdf[\"input_type\"] = data\n",
    "\t\tdf[\"model\"] = model\n",
    "\t\tdf[\"path\"] = path\n",
    "\t\tall_metrics_data.append(df)\n",
    "\treturn pd.concat(all_metrics_data, ignore_index=True)\n",
    "\n",
    "\n",
    "path = \"./llm\"\n",
    "exclude = []\n",
    "metrics_infer = collect_inference(path, exclude)\n",
    "metrics_infer"
   ],
   "id": "31decfe7cda47e09",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
